<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complete Guide to Understanding LLMs - Interactive Learning</title>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y03EXMZW8F"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-Y03EXMZW8F');
    </script>

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
            line-height: 1.6;
        }

        .container {
            display: grid;
            grid-template-columns: 280px 1fr 280px;
            gap: 20px;
            max-width: 1600px;
            margin: 0 auto;
            padding: 20px;
            min-height: 100vh;
        }

        /* Left Sidebar - Category Navigation */
        .sidebar-left {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 16px;
            padding: 24px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 20px;
            height: fit-content;
            max-height: calc(100vh - 40px);
            overflow-y: auto;
        }

        .sidebar-left h1 {
            font-size: 24px;
            color: #667eea;
            margin-bottom: 8px;
            text-align: center;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 14px;
            margin-bottom: 24px;
            padding-bottom: 16px;
            border-bottom: 2px solid #f0f0f0;
        }

        .category-nav {
            margin-bottom: 24px;
        }

        .category-item {
            margin-bottom: 12px;
            border-radius: 12px;
            overflow: hidden;
            background: #fff;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
        }

        .category-item:hover {
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .category-header {
            padding: 12px 16px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: space-between;
            background: linear-gradient(135deg, var(--category-color, #2196F3) 0%, var(--category-color-dark, #1976D2) 100%);
            color: white;
            font-weight: 600;
            font-size: 14px;
            transition: all 0.3s ease;
        }

        .category-header:hover {
            filter: brightness(1.1);
        }

        .category-icon {
            font-size: 18px;
            margin-right: 8px;
        }

        .category-title {
            flex: 1;
            display: flex;
            align-items: center;
        }

        .category-toggle {
            font-size: 12px;
            transition: transform 0.3s ease;
        }

        .category-toggle.expanded {
            transform: rotate(180deg);
        }

        .category-why {
            padding: 12px 16px;
            font-size: 12px;
            color: #666;
            background: #f9f9f9;
            border-left: 3px solid var(--category-color, #2196F3);
            display: none;
        }

        .category-item.expanded .category-why {
            display: block;
        }

        .category-sections {
            display: none;
            padding: 8px;
        }

        .category-item.expanded .category-sections {
            display: block;
        }

        .section-link {
            display: block;
            padding: 8px 12px;
            margin: 4px 0;
            background: #f5f5f5;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 13px;
            color: #444;
        }

        .section-link:hover {
            background: #e0e0e0;
            transform: translateX(4px);
        }

        .section-link.active {
            background: var(--category-color, #2196F3);
            color: white;
            font-weight: 600;
        }

        .progress-tracker {
            margin-top: 24px;
            padding-top: 24px;
            border-top: 2px solid #f0f0f0;
        }

        .progress-bar {
            width: 100%;
            height: 12px;
            background: #f0f0f0;
            border-radius: 6px;
            overflow: hidden;
            margin-bottom: 8px;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.5s ease;
            border-radius: 6px;
        }

        #progressText {
            text-align: center;
            font-size: 13px;
            color: #666;
            font-weight: 600;
        }

        /* Main Content */
        .main-content {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(10px);
            border-radius: 16px;
            padding: 40px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            min-height: 600px;
        }

        .stage-badge {
            display: inline-block;
            padding: 8px 16px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 20px;
            font-size: 14px;
            font-weight: 600;
            margin-bottom: 24px;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
        }

        #content h2 {
            color: #2c3e50;
            font-size: 32px;
            margin-bottom: 16px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 12px;
        }

        #content h3 {
            color: #34495e;
            font-size: 24px;
            margin-top: 28px;
            margin-bottom: 12px;
        }

        #content p {
            margin-bottom: 16px;
            font-size: 16px;
            line-height: 1.8;
            color: #444;
        }

        #content ul, #content ol {
            margin-left: 24px;
            margin-bottom: 16px;
        }

        #content li {
            margin-bottom: 8px;
            line-height: 1.7;
        }

        #content code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            color: #e83e8c;
        }

        #content pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 16px 0;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }

        #content pre code {
            background: transparent;
            color: inherit;
            padding: 0;
        }

        .animation-container {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 12px;
            padding: 24px;
            margin: 24px 0;
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1);
            text-align: center;
        }

        .animation-container h4 {
            margin-bottom: 16px;
            color: #2c3e50;
            font-size: 18px;
        }

        .visual-demo {
            background: white;
            border-radius: 8px;
            padding: 20px;
            margin: 12px 0;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
        }


        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196F3;
            border-radius: 8px;
            padding: 16px;
            margin: 20px 0;
        }

        .info-box p {
            margin: 8px 0;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #FF9800;
            border-radius: 8px;
            padding: 16px;
            margin: 20px 0;
        }

        .warning-box p {
            margin: 8px 0;
        }

        .success-box {
            background: #e8f5e9;
            border-left: 4px solid #4CAF50;
            border-radius: 8px;
            padding: 16px;
            margin: 20px 0;
        }

        .success-box p {
            margin: 8px 0;
        }

        .lead {
            font-size: 18px;
            font-weight: 500;
            color: #555;
            margin-bottom: 24px;
            line-height: 1.6;
        }

        .flow-diagram {
            display: block;
            margin: 24px auto;
            max-width: 100%;
            height: auto;
        }

        .quiz-container {
            background: #f9f9f9;
            border-left: 4px solid #667eea;
            border-radius: 8px;
            padding: 24px;
            margin: 24px 0;
        }

        .quiz-container h4 {
            color: #667eea;
            margin-bottom: 16px;
            font-size: 20px;
        }

        .quiz-question {
            margin-bottom: 24px;
        }

        .quiz-question p {
            font-weight: 600;
            margin-bottom: 12px;
            color: #2c3e50;
        }

        .quiz-options {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .quiz-option {
            padding: 12px 16px;
            background: white;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .quiz-option:hover {
            border-color: #667eea;
            background: #f5f7ff;
        }

        .quiz-option.selected {
            border-color: #667eea;
            background: #e8eaff;
        }

        .quiz-option.correct {
            border-color: #4CAF50;
            background: #e8f5e9;
        }

        .quiz-option.incorrect {
            border-color: #f44336;
            background: #ffebee;
        }

        .quiz-feedback {
            margin-top: 12px;
            padding: 12px;
            border-radius: 8px;
            font-size: 14px;
            display: none;
        }

        .quiz-feedback.show {
            display: block;
        }

        .quiz-feedback.correct {
            background: #e8f5e9;
            color: #2e7d32;
            border-left: 4px solid #4CAF50;
        }

        .quiz-feedback.incorrect {
            background: #ffebee;
            color: #c62828;
            border-left: 4px solid #f44336;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
            padding-top: 24px;
            border-top: 2px solid #f0f0f0;
        }

        .btn {
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 16px rgba(102, 126, 234, 0.4);
        }

        .btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        /* Right Sidebar - Feed Flow */
        .sidebar-right {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 16px;
            padding: 24px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            position: sticky;
            top: 20px;
            height: fit-content;
            max-height: calc(100vh - 40px);
            overflow-y: auto;
        }

        .sidebar-right h3 {
            font-size: 18px;
            color: #667eea;
            margin-bottom: 20px;
            text-align: center;
            padding-bottom: 12px;
            border-bottom: 2px solid #f0f0f0;
        }

        .feed-flow {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .feed-step {
            position: relative;
            padding: 16px;
            background: #f9f9f9;
            border-radius: 12px;
            transition: all 0.3s ease;
            border: 2px solid transparent;
        }

        .feed-step::after {
            content: '‚Üì';
            position: absolute;
            bottom: -22px;
            left: 50%;
            transform: translateX(-50%);
            color: #ccc;
            font-size: 20px;
        }

        .feed-step:last-child::after {
            display: none;
        }

        .feed-step.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: #667eea;
            box-shadow: 0 4px 16px rgba(102, 126, 234, 0.3);
            transform: scale(1.05);
        }

        .feed-step.active::after {
            color: #667eea;
            font-weight: bold;
        }

        .feed-step.completed {
            background: #e8f5e9;
            border-color: #4CAF50;
        }

        .feed-step-icon {
            font-size: 24px;
            margin-bottom: 8px;
            text-align: center;
        }

        .feed-step-label {
            text-align: center;
            font-size: 14px;
            font-weight: 600;
        }

        .feed-step-sections {
            text-align: center;
            font-size: 11px;
            margin-top: 4px;
            opacity: 0.7;
        }

        /* Mobile Responsive */
        @media (max-width: 1200px) {
            .container {
                grid-template-columns: 1fr;
            }

            .sidebar-left, .sidebar-right {
                position: static;
                max-height: none;
            }

            .sidebar-right {
                order: -1;
            }

            .feed-flow {
                flex-direction: row;
                overflow-x: auto;
                padding-bottom: 12px;
            }

            .feed-step {
                min-width: 120px;
            }

            .feed-step::after {
                content: '‚Üí';
                bottom: 50%;
                left: auto;
                right: -18px;
                transform: translateY(50%);
            }
        }

        @media (max-width: 768px) {
            .container {
                padding: 12px;
                gap: 12px;
            }

            .main-content {
                padding: 24px 20px;
            }

            #content h2 {
                font-size: 24px;
            }

            #content h3 {
                font-size: 20px;
            }

            #content p, #content li {
                font-size: 15px;
            }

            .btn {
                padding: 10px 20px;
                font-size: 14px;
            }
        }

        /* Scrollbar Styling */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb {
            background: #667eea;
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: #764ba2;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- LEFT SIDEBAR: Category Navigation -->
        <div class="sidebar-left">
            <h1>‚ú® LLM Learning Guide</h1>
            <p class="subtitle">From Zero to Understanding AI</p>

            <div id="categoryNav" class="category-nav">
                <!-- Will be populated by JavaScript -->
            </div>

            <div class="progress-tracker">
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
                <p id="progressText">Progress: 0%</p>
            </div>
        </div>

        <!-- MAIN CONTENT -->
        <div class="main-content">
            <div id="stageBadge" class="stage-badge"></div>
            <div id="content"></div>

            <div class="nav-buttons">
                <button id="prevBtn" class="btn">‚Üê Previous</button>
                <button id="nextBtn" class="btn">Next ‚Üí</button>
            </div>
        </div>

        <!-- RIGHT SIDEBAR: Feed Flow -->
        <div class="sidebar-right">
            <h3>üîÑ Data Flow Pipeline</h3>
            <div id="feedFlow" class="feed-flow">
                <!-- Will be populated by JavaScript -->
            </div>
        </div>
    </div>

    <!-- Footer -->
    <footer style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
                   backdrop-filter: blur(10px);
                   padding: 20px;
                   text-align: center;
                   margin-top: 40px;
                   border-top: 2px solid rgba(102, 126, 234, 0.3);
                   border-radius: 12px;">
        <p style="margin: 0; color: #666; font-size: 14px;">
            ¬© 2025 <strong>Arul Selvan</strong> | LLM Learning Guide
        </p>
        <p style="margin: 5px 0 0 0; color: #999; font-size: 12px;">
            Created with ‚ú® and Claude Code |
            <a href="https://github.com/arsurvey82/AILearning" target="_blank" style="color: #667eea; text-decoration: none;">View on GitHub</a>
        </p>
    </footer>

    <script>
        // Section data structure - will be filled in phases 2-9
        const sections = [
            // SECTION 0: WELCOME & OVERVIEW
            // ============================================
            {
                id: 0,
                title: "Welcome to the LLM Learning Guide",
                category: 0,
                content: `
                    <h1 style="text-align: center; margin-bottom: 20px;">üéì Welcome to the LLM Learning Guide</h1>

                    <!-- Introduction Section -->
                    <div style="margin: 0 auto 60px;">

                        <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(16px); padding: 30px; border-radius: 16px; margin-bottom: 30px; border: 1px solid rgba(203, 213, 225, 0.5);">
                            <h2 style="margin-top: 0; color: #667eea;">üìö What is This Guide?</h2>
                            <p>
                                This is a <strong>complete, interactive guide</strong> to understanding how Large Language Models (LLMs) work - from the ground up.
                                No PhD required! We break down complex concepts into simple, visual explanations with real examples.
                            </p>
                            <p>
                                You'll follow the actual journey of text through an AI model: <code>"The cat sat"</code> ‚Üí tokens ‚Üí vectors ‚Üí attention ‚Üí transformers ‚Üí prediction!
                            </p>
                        </div>

                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 30px;">

                            <!-- Who it's for -->
                            <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(16px); padding: 25px; border-radius: 12px; border: 1px solid rgba(203, 213, 225, 0.5);">
                                <h3 style="margin-top: 0; color: #4CAF50;">üë• Who Is This For?</h3>
                                <ul style="padding-left: 20px;">
                                    <li><strong>Developers</strong> - Learn how AI models actually work</li>
                                    <li><strong>Students</strong> - Understand LLMs for courses/research</li>
                                    <li><strong>AI Enthusiasts</strong> - Deep-dive into the technology</li>
                                    <li><strong>Product Managers</strong> - Make informed AI decisions</li>
                                    <li><strong>Anyone curious</strong> - No math/coding required!</li>
                                </ul>
                            </div>

                            <!-- Time & Prerequisites -->
                            <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(16px); padding: 25px; border-radius: 12px; border: 1px solid rgba(203, 213, 225, 0.5);">
                                <h3 style="margin-top: 0; color: #FF9800;">‚è±Ô∏è Time & Prerequisites</h3>
                                <p><strong>Total Time:</strong> 2-4 hours</p>
                                <ul style="padding-left: 20px;">
                                    <li>üìñ <strong>Quick read:</strong> 1-2 hours (skip quizzes)</li>
                                    <li>üéØ <strong>Deep learning:</strong> 3-4 hours (with quizzes)</li>
                                    <li>üîñ <strong>Reference:</strong> Bookmark & revisit sections</li>
                                </ul>
                                <p style="margin-top: 15px;"><strong>Prerequisites:</strong></p>
                                <p>‚úÖ None! We start from scratch</p>
                            </div>

                        </div>

                        <!-- What you'll learn -->
                        <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(16px); padding: 25px; border-radius: 12px; border: 1px solid rgba(203, 213, 225, 0.5); margin-bottom: 30px;">
                            <h3 style="margin-top: 0; color: #E91E63;">üéØ What You'll Learn</h3>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                <div>
                                    <p><strong>üìä Foundations:</strong></p>
                                    <ul style="margin-top: 5px;">
                                        <li>How computers store text as numbers</li>
                                        <li>Tokenization & vocabulary</li>
                                        <li>Vectors and embeddings</li>
                                    </ul>
                                </div>
                                <div>
                                    <p><strong>üß† Core Concepts:</strong></p>
                                    <ul style="margin-top: 5px;">
                                        <li>Self-attention mechanism</li>
                                        <li>Transformer architecture</li>
                                        <li>How predictions are made</li>
                                    </ul>
                                </div>
                                <div>
                                    <p><strong>üíæ Practical Skills:</strong></p>
                                    <ul style="margin-top: 5px;">
                                        <li>Model compression (quantization)</li>
                                        <li>Running models locally</li>
                                        <li>Understanding file formats (GGUF)</li>
                                    </ul>
                                </div>
                                <div>
                                    <p><strong>üöÄ Deployment:</strong></p>
                                    <ul style="margin-top: 5px;">
                                        <li>LM Studio vs cloud APIs</li>
                                        <li>Context windows & memory</li>
                                        <li>Cost optimization strategies</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <!-- Learning Paths -->
                        <div style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(16px); padding: 25px; border-radius: 12px; border: 1px solid rgba(203, 213, 225, 0.5);">
                            <h3 style="margin-top: 0; color: #3F51B5;">üõ§Ô∏è Learning Paths</h3>

                            <div style="margin-bottom: 20px;">
                                <p style="font-weight: bold; color: #4CAF50; margin-bottom: 8px;">‚úÖ RECOMMENDED: Linear Path (Beginner-Friendly)</p>
                                <p style="margin-left: 20px;">
                                    üìö Start at Section 1 ‚Üí Follow in order ‚Üí Complete all 21 sections<br>
                                    <em style="color: #666;">Best for: First-time learners, comprehensive understanding</em>
                                </p>
                            </div>

                            <div style="margin-bottom: 20px;">
                                <p style="font-weight: bold; color: #FF9800; margin-bottom: 8px;">‚ö° Quick Overview Path (Experienced)</p>
                                <p style="margin-left: 20px;">
                                    üó∫Ô∏è Section 1 (Foundations) ‚Üí 11 (Attention) ‚Üí 12 (Transformers) ‚Üí 14 (Pipeline) ‚Üí 18 (GGUF)<br>
                                    <em style="color: #666;">Best for: Developers who know basics, want key concepts</em>
                                </p>
                            </div>

                            <div style="margin-bottom: 20px;">
                                <p style="font-weight: bold; color: #9C27B0; margin-bottom: 8px;">üéØ Practical Focus Path (Deployment)</p>
                                <p style="margin-left: 20px;">
                                    üìö Sections 1-5 (Basics) ‚Üí 16-21 (Efficiency & Deployment)<br>
                                    <em style="color: #666;">Best for: Running models locally, understanding formats</em>
                                </p>
                            </div>

                            <div>
                                <p style="font-weight: bold; color: #2196F3; margin-bottom: 8px;">üîç Skip-Around Path (Reference)</p>
                                <p style="margin-left: 20px;">
                                    üìç Use the navigation below ‚Üí Jump to topics you need<br>
                                    <em style="color: #666;">Best for: Quick answers, specific topics, refresher</em>
                                </p>
                            </div>

                        </div>

                    </div>

                    <div class="info-box" style="margin-top: 40px;">
                        <p style="font-weight: bold; color: #667eea; margin-bottom: 10px;">üí° How to Use This Guide:</p>
                        <ul>
                            <li><strong>Follow in order</strong> - Each section builds on the previous one</li>
                            <li><strong>Visual learner?</strong> - Every section has animated diagrams</li>
                            <li><strong>Test yourself</strong> - Complete the quiz after each section</li>
                            <li><strong>Lost?</strong> - Use the navigation to see where you are!</li>
                        </ul>
                    </div>

                    <div style="text-align: center; margin-top: 40px;">
                        <button onclick="showSection(1)" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; padding: 15px 40px; border-radius: 25px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4); transition: all 0.3s;">
                            Start Learning! ‚Üí
                        </button>
                    </div>
                `,
                quiz: []
            },

            // PART 1: DATA FOUNDATIONS

            // PART 1: DATA FOUNDATIONS
            {
                id: 1,
                title: "What is Data?",
                category: 1,
                content: `
                    <h1>What is Data?</h1>

                    <p class="lead">Before we can understand AI, we need to understand data. Everything in a computer - your photos, music, text, even AI models - is ultimately stored as data.</p>

                    <svg class="flow-diagram" width="100%" height="180" viewBox="0 0 700 180">
                        <!-- Character 'A' -->
                        <text x="50" y="90" font-size="60" fill="#2196F3" font-weight="bold">A</text>

                        <!-- Arrow 1 -->
                        <path d="M 100 90 L 160 90" stroke="#3498db" stroke-width="3" fill="none" marker-end="url(#arrowhead)">
                            <animate attributeName="stroke-dasharray" values="0 100; 100 0" dur="2s" repeatCount="indefinite"/>
                        </path>

                        <!-- ASCII Code -->
                        <rect x="180" y="60" width="100" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="230" y="95" text-anchor="middle" font-size="20" fill="#1976d2" font-weight="bold">ASCII: 65</text>

                        <!-- Arrow 2 -->
                        <path d="M 280 90 L 340 90" stroke="#3498db" stroke-width="3" fill="none" marker-end="url(#arrowhead)">
                            <animate attributeName="stroke-dasharray" values="0 100; 100 0" dur="2s" begin="0.5s" repeatCount="indefinite"/>
                        </path>

                        <!-- Binary -->
                        <rect x="360" y="60" width="140" height="60" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                        <text x="430" y="95" text-anchor="middle" font-size="16" fill="#E65100" font-family="monospace">01000001</text>

                        <!-- Arrow 3 -->
                        <path d="M 500 90 L 560 90" stroke="#3498db" stroke-width="3" fill="none" marker-end="url(#arrowhead)">
                            <animate attributeName="stroke-dasharray" values="0 100; 100 0" dur="2s" begin="1s" repeatCount="indefinite"/>
                        </path>

                        <!-- Memory -->
                        <g transform="translate(580, 50)">
                            <rect width="80" height="80" rx="8" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="40" y="35" text-anchor="middle" font-size="12" fill="#2e7d32">Memory</text>
                            <rect x="20" y="40" width="40" height="8" fill="#4CAF50" rx="2">
                                <animate attributeName="opacity" values="0.3;1;0.3" dur="1.5s" repeatCount="indefinite"/>
                            </rect>
                            <rect x="20" y="52" width="40" height="8" fill="#81C784" rx="2"/>
                            <rect x="20" y="64" width="40" height="8" fill="#A5D6A7" rx="2"/>
                        </g>

                        <defs>
                            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#3498db"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Everything is 0s and 1s</h2>
                    <p>At the lowest level, computers only understand two states: <strong>on (1)</strong> and <strong>off (0)</strong>. This is called <strong>binary</strong>.</p>

                    <div class="info-box">
                        <p><strong>Think of it like a light switch:</strong></p>
                        <ul>
                            <li>Switch OFF = 0</li>
                            <li>Switch ON = 1</li>
                        </ul>
                        <p>By combining millions of these switches, computers can represent anything - text, images, videos, and AI models!</p>
                    </div>

                    <h2>How Computers Store Text</h2>
                    <p>When you type the letter "A", here's what happens:</p>
                    <ol>
                        <li><strong>Character</strong>: You see "A" on your screen</li>
                        <li><strong>ASCII/Unicode</strong>: Computer assigns it a number (65)</li>
                        <li><strong>Binary</strong>: Converts to binary: 01000001</li>
                        <li><strong>Storage</strong>: Saves those 8 bits in memory</li>
                    </ol>

                    <h2>Real Example</h2>
                    <pre><code>Text:    "Hello"
Character: H     e     l     l     o
ASCII:    72    101   108   108   111
Binary:   01001000 01100101 01101100 01101100 01101111</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Everything in a computer, including AI models, is ultimately stored as binary numbers (0s and 1s).</p>
                    </div>

                    <h2>Why This Matters for AI</h2>
                    <p>AI models need to process text (like "The cat sat"). But computers only understand numbers. This is why we need to convert text ‚Üí numbers ‚Üí something AI can process. That's what the next sections will teach you!</p>
                `,
                quiz: [
                    {
                        question: "What are the two basic states that computers understand?",
                        options: ["Yes and No", "0 and 1", "True and False", "On and Maybe"],
                        correct: 1,
                        explanation: "Computers operate on binary: 0 (off) and 1 (on). These two states form the foundation of all computing."
                    },
                    {
                        question: "What is the ASCII code for the letter 'A'?",
                        options: ["1", "26", "65", "97"],
                        correct: 2,
                        explanation: "The letter 'A' (uppercase) has ASCII code 65, which is 01000001 in binary."
                    },
                    {
                        question: "How many bits does it take to store the letter 'A' in ASCII?",
                        options: ["4 bits", "8 bits", "16 bits", "32 bits"],
                        correct: 1,
                        explanation: "ASCII uses 8 bits (1 byte) per character. The letter 'A' is stored as 01000001."
                    }
                ]
            },
            {
                id: 2,
                title: "Text as Data",
                category: 1,
                content: `
                    <h1>Text as Data</h1>

                    <p class="lead">Now that you know computers store everything as numbers, let's see how text - words, sentences, paragraphs - becomes data.</p>

                    <svg class="flow-diagram" width="100%" height="200" viewBox="0 0 750 200">
                        <!-- Input text -->
                        <text x="40" y="100" font-size="28" fill="#2196F3" font-weight="bold">Hello</text>

                        <!-- Arrow -->
                        <path d="M 120 100 L 170 100" stroke="#3498db" stroke-width="3" marker-end="url(#arrow2)"/>

                        <!-- Character breakdown -->
                        <g id="chars">
                            <rect x="190" y="50" width="50" height="50" rx="5" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="0.5s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="215" y="82" text-anchor="middle" font-size="24" fill="#1976d2">H</text>

                            <rect x="250" y="50" width="50" height="50" rx="5" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="0.7s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="275" y="82" text-anchor="middle" font-size="24" fill="#1976d2">e</text>

                            <rect x="310" y="50" width="50" height="50" rx="5" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="0.9s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="335" y="82" text-anchor="middle" font-size="24" fill="#1976d2">l</text>

                            <rect x="370" y="50" width="50" height="50" rx="5" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="1.1s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="395" y="82" text-anchor="middle" font-size="24" fill="#1976d2">l</text>

                            <rect x="430" y="50" width="50" height="50" rx="5" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="1.3s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="455" y="82" text-anchor="middle" font-size="24" fill="#1976d2">o</text>
                        </g>

                        <!-- Character codes below -->
                        <g>
                            <text x="215" y="130" text-anchor="middle" font-size="14" fill="#666">72</text>
                            <text x="275" y="130" text-anchor="middle" font-size="14" fill="#666">101</text>
                            <text x="335" y="130" text-anchor="middle" font-size="14" fill="#666">108</text>
                            <text x="395" y="130" text-anchor="middle" font-size="14" fill="#666">108</text>
                            <text x="455" y="130" text-anchor="middle" font-size="14" fill="#666">111</text>
                        </g>

                        <!-- Arrow to result -->
                        <path d="M 490 75 L 540 75" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrow3)"/>

                        <!-- Result -->
                        <rect x="560" y="40" width="150" height="70" rx="8" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2"/>
                        <text x="635" y="65" text-anchor="middle" font-size="14" fill="#2e7d32">Array of Numbers</text>
                        <text x="635" y="90" text-anchor="middle" font-size="12" fill="#2e7d32" font-family="monospace">[72,101,108,108,111]</text>

                        <defs>
                            <marker id="arrow2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#3498db"/>
                            </marker>
                            <marker id="arrow3" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Characters, Words, and Sentences</h2>
                    <p>Text has structure at multiple levels:</p>
                    <ul>
                        <li><strong>Characters</strong>: Individual letters, numbers, symbols (A, b, 1, !, üòä)</li>
                        <li><strong>Words</strong>: Groups of characters with meaning ("cat", "hello", "AI")</li>
                        <li><strong>Sentences</strong>: Groups of words forming complete thoughts</li>
                        <li><strong>Documents</strong>: Collections of sentences</li>
                    </ul>

                    <h2>Unicode: Beyond English</h2>
                    <p>While ASCII handles English (A-Z, 0-9), <strong>Unicode (UTF-8)</strong> handles ALL human languages and symbols:</p>

                    <div class="info-box">
                        <p><strong>Examples:</strong></p>
                        <ul>
                            <li>Hello (English): 5 characters</li>
                            <li>„Åì„Çì„Å´„Å°„ÅØ (Japanese): 5 characters</li>
                            <li>ŸÖÿ±ÿ≠ÿ®ÿß (Arabic): 5 characters</li>
                            <li>üòäüëçüéâ (Emojis): 3 characters</li>
                        </ul>
                        <p>Each gets a unique number in UTF-8!</p>
                    </div>

                    <h2>Why Encoding Matters</h2>
                    <pre><code>Text:      "caf√©"
UTF-8:     [99, 97, 102, 233]  # √© has code 233
Binary:    01100011 01100001 01100110 11101001

Emoji:     "üòä"
UTF-8:     [240, 159, 152, 138]  # Takes 4 bytes!
Binary:    11110000 10011111 10011000 10001010</code></pre>

                    <div class="warning-box">
                        <p><strong>Challenge:</strong> Different characters take different amounts of space. "A" = 1 byte, but "üòä" = 4 bytes. AI models need to handle this efficiently!</p>
                    </div>

                    <h2>Text as a Sequence</h2>
                    <p>When AI processes text, it sees it as a <strong>sequence of character codes</strong>:</p>
                    <pre><code>"The cat" ‚Üí [84, 104, 101, 32, 99, 97, 116]
                   T   h    e  space c   a   t</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Text is converted to sequences of numbers (character codes). AI models process these sequences, not the letters themselves.</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What encoding system handles all human languages including emojis?",
                        options: ["ASCII", "Binary", "UTF-8", "Hexadecimal"],
                        correct: 2,
                        explanation: "UTF-8 (Unicode) can represent all characters from all human languages, plus emojis and special symbols. ASCII only handles basic English characters."
                    },
                    {
                        question: "How many bytes does the emoji 'üòä' take in UTF-8?",
                        options: ["1 byte", "2 bytes", "4 bytes", "8 bytes"],
                        correct: 2,
                        explanation: "Most emojis take 4 bytes in UTF-8, while simple English letters take only 1 byte. This is why emojis increase file sizes!"
                    },
                    {
                        question: "When AI processes the word 'cat', what does it actually see?",
                        options: ["The letters c, a, t", "A sequence of numbers", "A picture of a cat", "The meaning of 'cat'"],
                        correct: 1,
                        explanation: "AI sees 'cat' as a sequence of character codes: [99, 97, 116]. It processes numbers, not letters!"
                    }
                ]
            },
            {
                id: 3,
                title: "The Problem: Computers Need Numbers",
                category: 1,
                content: `
                    <h1>The Problem: Computers Need Numbers</h1>

                    <p class="lead">We've learned that computers store text as numbers. But there's a bigger challenge: <strong>How do we make AI understand what "cat" means?</strong></p>

                    <svg class="flow-diagram" width="100%" height="220" viewBox="0 0 750 220">
                        <!-- Input text -->
                        <text x="30" y="110" font-size="32" fill="#2196F3" font-weight="bold">"The cat"</text>

                        <!-- Arrow to problem -->
                        <path d="M 150 110 L 210 110" stroke="#FF9800" stroke-width="3" marker-end="url(#arrow4)"/>

                        <!-- Problem box -->
                        <g>
                            <rect x="230" y="60" width="140" height="100" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="3">
                                <animate attributeName="opacity" values="0.3;1;0.3" dur="2s" repeatCount="indefinite"/>
                            </rect>
                            <text x="300" y="90" text-anchor="middle" font-size="20" fill="#E65100">‚ùì</text>
                            <text x="300" y="115" text-anchor="middle" font-size="14" fill="#E65100">How to represent</text>
                            <text x="300" y="135" text-anchor="middle" font-size="14" fill="#E65100">meaning?</text>
                        </g>

                        <!-- Arrow to AI -->
                        <path d="M 370 110 L 430 110" stroke="#9C27B0" stroke-width="3" marker-end="url(#arrow5)"/>

                        <!-- AI Brain -->
                        <g>
                            <circle cx="520" cy="110" r="60" fill="#e1bee7" stroke="#9C27B0" stroke-width="3">
                                <animate attributeName="r" values="58;62;58" dur="2s" repeatCount="indefinite"/>
                            </circle>
                            <text x="520" y="100" text-anchor="middle" font-size="40">üß†</text>
                            <text x="520" y="135" text-anchor="middle" font-size="14" fill="#4A148C" font-weight="bold">AI Model</text>
                        </g>

                        <!-- Arrow to output -->
                        <path d="M 580 110 L 630 110" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrow6)"/>

                        <!-- Output -->
                        <g>
                            <rect x="650" y="85" width="80" height="50" rx="8" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="690" y="115" text-anchor="middle" font-size="24" fill="#2e7d32">‚úì</text>
                        </g>

                        <defs>
                            <marker id="arrow4" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrow5" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#9C27B0"/>
                            </marker>
                            <marker id="arrow6" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Character Codes Aren't Enough</h2>
                    <p>Remember from Section 2, "cat" becomes [99, 97, 116]. But these numbers are just <strong>labels</strong>. They don't capture <strong>meaning</strong>:</p>

                    <div class="warning-box">
                        <p><strong>The Problem:</strong></p>
                        <ul>
                            <li>"cat" ‚Üí [99, 97, 116]</li>
                            <li>"dog" ‚Üí [100, 111, 103]</li>
                            <li>"car" ‚Üí [99, 97, 114]</li>
                        </ul>
                        <p>"cat" and "car" have closer numbers than "cat" and "dog", but "cat" and "dog" are more similar in meaning (both are animals)!</p>
                        <p><strong>Character codes don't reflect semantic meaning.</strong></p>
                    </div>

                    <h2>What AI Needs</h2>
                    <p>For AI to understand language, it needs numbers that capture:</p>
                    <ol>
                        <li><strong>Meaning</strong>: Similar words have similar numbers</li>
                        <li><strong>Relationships</strong>: "king" - "man" + "woman" ‚âà "queen"</li>
                        <li><strong>Context</strong>: "bank" (river) vs "bank" (money)</li>
                        <li><strong>Grammar</strong>: Verbs, nouns, adjectives behave differently</li>
                    </ol>

                    <h2>The Journey Ahead</h2>
                    <p>To solve this, we'll learn about:</p>

                    <div class="info-box">
                        <p><strong>üî§ Tokenization</strong> (Parts 2): Breaking text into meaningful pieces</p>
                        <p><strong>üìä Vectors</strong> (Part 3): Converting words to arrays of numbers that capture meaning</p>
                        <p><strong>üîÆ Embeddings</strong> (Part 3): Learning these numbers from massive amounts of text</p>
                        <p><strong>‚öôÔ∏è Transformers</strong> (Part 5): Processing these numbers to understand relationships</p>
                    </div>

                    <h2>A Sneak Peek</h2>
                    <p>Instead of character codes, AI uses <strong>embeddings</strong> - learned representations:</p>
                    <pre><code>Word:  "cat"
Embedding:  [0.2, 0.8, -0.3, 0.5, ..., 0.1]  ‚Üê 4096 numbers!

Word:  "dog"
Embedding:  [0.3, 0.7, -0.2, 0.4, ..., 0.2]  ‚Üê Similar to "cat"!

Word:  "car"
Embedding:  [-0.5, 0.1, 0.9, -0.3, ..., 0.6]  ‚Üê Very different!</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Character codes (99, 97, 116) are just labels. AI needs rich numerical representations (embeddings) that capture meaning, relationships, and context. That's what we'll build toward!</p>
                    </div>

                    <h2>Ready for the Journey?</h2>
                    <p>You now understand:</p>
                    <ul>
                        <li>‚úì How computers store data as binary</li>
                        <li>‚úì How text becomes character codes</li>
                        <li>‚úì Why simple codes aren't enough for AI</li>
                    </ul>
                    <p><strong>Next up:</strong> Learn how tokenization breaks text into pieces AI can process!</p>

                    <div style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
                                backdrop-filter: blur(10px);
                                padding: 20px;
                                margin: 30px 0;
                                border-radius: 12px;
                                border-left: 4px solid #667eea;">
                        <h3 style="margin-top: 0; color: #667eea;">üîó Bridge to Next Section</h3>
                        <p><strong>You just learned the problem:</strong> Character codes like ASCII (99, 97, 116 for "cat") are just arbitrary labels. They don't capture that "cat" and "dog" are both animals, while "cat" and "car" are completely different concepts.</p>

                        <p><strong>So how does tokenization help?</strong> Instead of processing individual characters (<code>c</code>, <code>a</code>, <code>t</code>), tokenization creates meaningful chunks like <code>"cat"</code> as a single unit. This is the first step toward giving AI the ability to understand meaning:</p>

                        <ul style="margin-left: 20px;">
                            <li>‚úì <strong>Characters</strong> ("c", "a", "t") ‚Üí No meaning individually</li>
                            <li>‚úì <strong>Tokens</strong> ("cat") ‚Üí Meaningful word that can later be mapped to rich vectors</li>
                            <li>‚úì <strong>Result</strong> ‚Üí Fewer, more meaningful pieces for AI to process</li>
                        </ul>

                        <p>Think of it like this: Would you rather read a book letter-by-letter or word-by-word? Tokenization gives AI the "words" to work with, making the next steps (embeddings, attention) possible!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "Why aren't character codes (like ASCII) sufficient for AI to understand meaning?",
                        options: [
                            "They're too slow to process",
                            "They only label characters, not meaning",
                            "They only work in English",
                            "They take too much memory"
                        ],
                        correct: 1,
                        explanation: "Character codes like ASCII/UTF-8 are just labels for characters. 'cat'=[99,97,116] and 'dog'=[100,111,103] don't reflect that both are animals. AI needs representations that capture semantic meaning."
                    },
                    {
                        question: "What do AI embeddings provide that character codes don't?",
                        options: [
                            "Faster processing speed",
                            "Smaller file sizes",
                            "Semantic meaning and relationships",
                            "Better compression"
                        ],
                        correct: 2,
                        explanation: "Embeddings are learned representations where similar words have similar numbers, capturing meaning, relationships, and context - unlike character codes which are just arbitrary labels."
                    },
                    {
                        question: "In the example, 'cat' and 'car' have closer character codes than 'cat' and 'dog'. What does this show?",
                        options: [
                            "Character codes are perfect for AI",
                            "Cats and cars are more similar than cats and dogs",
                            "Character codes don't reflect semantic similarity",
                            "AI prefers cats over dogs"
                        ],
                        correct: 2,
                        explanation: "This shows that character codes (based on spelling) don't reflect semantic meaning. 'cat' and 'dog' (both animals) should be closer in meaning space than 'cat' and 'car', but character codes can't capture this."
                    }
                ]
            }
,

            // ============================================
            // PART 2: TOKENIZATION (Sections 4-5)
            // ============================================
            {
                id: 4,
                title: "What are Tokens?",
                category: 2,
                content: `
                    <h1>What are Tokens?</h1>

                    <p class="lead">Tokenization is the first real step in converting text into something AI can process. Instead of individual characters, we break text into meaningful chunks called <strong>tokens</strong>.</p>

                    <svg class="flow-diagram" width="100%" height="250" viewBox="0 0 800 250">
                        <!-- Input sentence -->
                        <text x="40" y="60" font-size="24" fill="#2196F3" font-weight="bold">The cat sat</text>

                        <!-- Arrow down -->
                        <path d="M 120 80 L 120 120" stroke="#FF9800" stroke-width="3" marker-end="url(#arrow7)"/>
                        <text x="140" y="105" font-size="14" fill="#FF9800" font-weight="bold">Tokenize</text>

                        <!-- Token boxes appearing one by one -->
                        <g id="tokens">
                            <rect x="30" y="140" width="80" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="0.5s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="70" y="175" text-anchor="middle" font-size="20" fill="#1976d2" font-weight="bold">The</text>

                            <rect x="130" y="140" width="80" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="0.8s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="170" y="175" text-anchor="middle" font-size="20" fill="#1976d2" font-weight="bold">cat</text>

                            <rect x="230" y="140" width="80" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2">
                                <animate attributeName="opacity" values="0;1" begin="1.1s" dur="0.3s" fill="freeze"/>
                            </rect>
                            <text x="270" y="175" text-anchor="middle" font-size="20" fill="#1976d2" font-weight="bold">sat</text>
                        </g>

                        <!-- Arrow to examples -->
                        <path d="M 330 170 L 380 170" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrow8)"/>

                        <!-- Other examples box -->
                        <g>
                            <rect x="400" y="30" width="360" height="180" rx="8" fill="#f1f8e9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="580" y="55" text-anchor="middle" font-size="16" fill="#33691E" font-weight="bold">Other Examples:</text>

                            <text x="420" y="90" font-size="14" fill="#558B2F">"running" ‚Üí</text>
                            <text x="540" y="90" font-size="14" fill="#33691E" font-weight="bold">["run", "ning"]</text>

                            <text x="420" y="120" font-size="14" fill="#558B2F">"don't" ‚Üí</text>
                            <text x="540" y="120" font-size="14" fill="#33691E" font-weight="bold">["don", "'t"]</text>

                            <text x="420" y="150" font-size="14" fill="#558B2F">"ChatGPT" ‚Üí</text>
                            <text x="540" y="150" font-size="14" fill="#33691E" font-weight="bold">["Chat", "GPT"]</text>

                            <text x="420" y="180" font-size="14" fill="#558B2F">"2024" ‚Üí</text>
                            <text x="540" y="180" font-size="14" fill="#33691E" font-weight="bold">["2024"]</text>
                        </g>

                        <defs>
                            <marker id="arrow7" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrow8" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Why Not Use Individual Characters?</h2>
                    <p>Remember from Section 2, we could process text character by character. But there are problems with this approach:</p>

                    <div class="warning-box">
                        <p><strong>Problems with Character-Level Processing:</strong></p>
                        <ul>
                            <li><strong>Too many pieces</strong>: "The cat sat" = 11 characters (including spaces). That's a lot to process!</li>
                            <li><strong>No meaning</strong>: Individual letters don't carry meaning. "c" alone tells us nothing.</li>
                            <li><strong>Inefficient</strong>: Processing millions of characters is slow and memory-intensive.</li>
                            <li><strong>Lost patterns</strong>: Can't recognize common word parts like "ing", "ed", "un".</li>
                        </ul>
                    </div>

                    <h2>What is a Token?</h2>
                    <p>A <strong>token</strong> is a piece of text - it could be a word, part of a word, or even a character. Modern AI uses <strong>subword tokenization</strong>, which is smarter than just splitting on spaces:</p>

                    <div class="info-box">
                        <p><strong>Token Examples:</strong></p>
                        <ul>
                            <li><strong>Whole words</strong>: "cat", "the", "sat" ‚Üí Common words stay whole</li>
                            <li><strong>Word parts</strong>: "running" ‚Üí ["run", "ning"] ‚Üí Breaks at meaningful boundaries</li>
                            <li><strong>Subwords</strong>: "unhappiness" ‚Üí ["un", "happi", "ness"] ‚Üí Recognizes prefixes/suffixes</li>
                            <li><strong>Special cases</strong>: "ChatGPT" ‚Üí ["Chat", "GPT"] ‚Üí Handles compound words</li>
                        </ul>
                    </div>

                    <h2>How Tokenization Works</h2>
                    <p>AI models use algorithms like <strong>Byte Pair Encoding (BPE)</strong> or <strong>WordPiece</strong> to learn the best way to split text:</p>

                    <ol>
                        <li><strong>Start with characters</strong>: Begin with individual characters</li>
                        <li><strong>Find common pairs</strong>: Look for frequently occurring character combinations</li>
                        <li><strong>Merge them</strong>: Combine common pairs into single tokens</li>
                        <li><strong>Repeat</strong>: Keep merging until you have ~50,000 tokens</li>
                    </ol>

                    <h2>Real-World Example</h2>
                    <pre><code>Input:  "I don't understand tokenization"

Tokens: ["I", " don", "'t", " understand", " token", "ization"]
        ‚Üë      ‚Üë      ‚Üë        ‚Üë              ‚Üë         ‚Üë
     whole  word   punct    whole        subword   subword
     word   part                         (common)  (suffix)</code></pre>

                    <div class="info-box">
                        <p><strong>Notice:</strong></p>
                        <ul>
                            <li>Spaces are included with words (see " don", " understand")</li>
                            <li>Contractions split logically ("don't" ‚Üí "don" + "'t")</li>
                            <li>Rare words split into parts ("tokenization" ‚Üí "token" + "ization")</li>
                            <li>Common words stay whole ("I", "understand")</li>
                        </ul>
                    </div>

                    <h2>Benefits of Tokenization</h2>
                    <ul>
                        <li><strong>Efficiency</strong>: "The cat sat" = 3 tokens vs 11 characters (63% reduction!)</li>
                        <li><strong>Meaning</strong>: Tokens often have semantic meaning ("cat" is meaningful)</li>
                        <li><strong>Flexibility</strong>: Can handle new words by breaking into known parts</li>
                        <li><strong>Consistency</strong>: Same word always tokenizes the same way</li>
                    </ul>

                    <h2>Different Tokenizers</h2>
                    <p>Different AI models use different tokenizers:</p>

                    <pre><code>GPT-4 (cl100k_base):
"Hello world!" ‚Üí ["Hello", " world", "!"]  # 3 tokens

GPT-3 (p50k_base):
"Hello world!" ‚Üí ["Hello", " world", "!"]  # 3 tokens (similar)

LLaMA:
"Hello world!" ‚Üí ["‚ñÅHello", "‚ñÅworld", "!"]  # Uses ‚ñÅ for spaces</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Tokenization breaks text into meaningful chunks (tokens) that are more efficient than characters but more flexible than whole words. This is the first step in making text understandable to AI.</p>
                    </div>

                    <h2>Token Count Matters</h2>
                    <p>When using AI APIs, you often pay per token and have token limits:</p>

                    <div class="warning-box">
                        <p><strong>Real Impact:</strong></p>
                        <ul>
                            <li>GPT-4: 8,192 token context window (older version)</li>
                            <li>GPT-4 Turbo: 128,000 token context window</li>
                            <li>1 token ‚âà 4 characters in English</li>
                            <li>1 token ‚âà ¬æ of a word on average</li>
                        </ul>
                        <p>More tokens = higher cost and potentially hitting limits!</p>
                    </div>
`,
                quiz: [
                    {
                        question: "What is the main advantage of tokenization over character-level processing?",
                        options: [
                            "It looks prettier",
                            "It's more efficient and captures meaning",
                            "It only works in English",
                            "It removes all spaces"
                        ],
                        correct: 1,
                        explanation: "Tokenization is more efficient (fewer pieces to process) and captures semantic meaning better than individual characters. 'The cat sat' becomes 3 tokens instead of 11 characters."
                    },
                    {
                        question: "How would 'running' typically be tokenized?",
                        options: [
                            "['r','u','n','n','i','n','g']",
                            "['running']",
                            "['run', 'ning']",
                            "['runn', 'ing']"
                        ],
                        correct: 2,
                        explanation: "Modern tokenizers use subword tokenization, breaking 'running' into ['run', 'ning'] - recognizing 'run' as a base word and 'ning' as a common suffix pattern."
                    },
                    {
                        question: "Approximately how many characters equal one token in English?",
                        options: ["1 character", "4 characters", "10 characters", "20 characters"],
                        correct: 1,
                        explanation: "On average, 1 token ‚âà 4 characters in English, or about ¬æ of a word. This is why 'Hello world!' (12 chars) becomes roughly 3 tokens."
                    }
                ]
            },
            {
                id: 5,
                title: "Token IDs - The Number Assignment",
                category: 2,
                content: `
                    <h1>Token IDs - The Number Assignment</h1>

                    <p class="lead">Once we have tokens, we need to convert them to numbers. Each token gets a unique ID from a <strong>vocabulary</strong> - like a giant dictionary.</p>

                    <svg class="flow-diagram" width="100%" height="280" viewBox="0 0 800 280">
                        <!-- Tokens -->
                        <text x="40" y="50" font-size="18" fill="#2196F3" font-weight="bold">Tokens:</text>

                        <rect x="30" y="60" width="90" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="75" y="92" text-anchor="middle" font-size="20" fill="#1976d2">"The"</text>

                        <rect x="140" y="60" width="90" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="185" y="92" text-anchor="middle" font-size="20" fill="#1976d2">"cat"</text>

                        <rect x="250" y="60" width="90" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="295" y="92" text-anchor="middle" font-size="20" fill="#1976d2">"sat"</text>

                        <!-- Arrows down -->
                        <path d="M 75 110 L 75 140" stroke="#FF9800" stroke-width="3" marker-end="url(#arrow9)"/>
                        <path d="M 185 110 L 185 140" stroke="#FF9800" stroke-width="3" marker-end="url(#arrow9)"/>
                        <path d="M 295 110 L 295 140" stroke="#FF9800" stroke-width="3" marker-end="url(#arrow9)"/>

                        <text x="185" y="135" text-anchor="middle" font-size="14" fill="#FF9800" font-weight="bold">Lookup in Vocabulary</text>

                        <!-- Token IDs -->
                        <rect x="30" y="150" width="90" height="50" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2">
                            <animate attributeName="opacity" values="0;1" begin="0.5s" dur="0.3s" fill="freeze"/>
                        </rect>
                        <text x="75" y="182" text-anchor="middle" font-size="24" fill="#E65100" font-weight="bold">1234</text>

                        <rect x="140" y="150" width="90" height="50" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2">
                            <animate attributeName="opacity" values="0;1" begin="0.7s" dur="0.3s" fill="freeze"/>
                        </rect>
                        <text x="185" y="182" text-anchor="middle" font-size="24" fill="#E65100" font-weight="bold">5678</text>

                        <rect x="250" y="150" width="90" height="50" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2">
                            <animate attributeName="opacity" values="0;1" begin="0.9s" dur="0.3s" fill="freeze"/>
                        </rect>
                        <text x="295" y="182" text-anchor="middle" font-size="24" fill="#E65100" font-weight="bold">9012</text>

                        <!-- Vocabulary box -->
                        <g>
                            <rect x="400" y="30" width="360" height="220" rx="8" fill="#f1f8e9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="580" y="55" text-anchor="middle" font-size="16" fill="#33691E" font-weight="bold">Vocabulary (50,000 tokens)</text>

                            <text x="420" y="85" font-size="14" fill="#558B2F" font-family="monospace">0: [START]</text>
                            <text x="420" y="110" font-size="14" fill="#558B2F" font-family="monospace">1: [END]</text>
                            <text x="420" y="135" font-size="14" fill="#558B2F" font-family="monospace">...</text>
                            <text x="420" y="160" font-size="14" fill="#33691E" font-family="monospace" font-weight="bold">1234: "The"</text>
                            <text x="420" y="185" font-size="14" fill="#33691E" font-family="monospace" font-weight="bold">5678: "cat"</text>
                            <text x="420" y="210" font-size="14" fill="#33691E" font-family="monospace" font-weight="bold">9012: "sat"</text>
                            <text x="420" y="235" font-size="14" fill="#558B2F" font-family="monospace">...</text>
                        </g>

                        <defs>
                            <marker id="arrow9" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>What is a Token ID?</h2>
                    <p>After tokenization, each token gets mapped to a unique number called a <strong>Token ID</strong>. This mapping comes from a fixed <strong>vocabulary</strong>:</p>

                    <div class="info-box">
                        <p><strong>Vocabulary = Dictionary of all possible tokens</strong></p>
                        <ul>
                            <li><strong>Size</strong>: Typically 30,000 - 100,000 tokens</li>
                            <li><strong>Fixed</strong>: Created during training, never changes</li>
                            <li><strong>Unique</strong>: Each token has exactly one ID</li>
                            <li><strong>Includes</strong>: Words, subwords, special tokens</li>
                        </ul>
                    </div>

                    <h2>Real Example: GPT-4 Tokenizer</h2>
                    <pre><code>Input:  "The cat sat"

Step 1: Tokenize
Tokens: ["The", " cat", " sat"]

Step 2: Look up Token IDs
"The"  ‚Üí 791    (common word, low ID)
" cat" ‚Üí 8415   (word with space prefix)
" sat" ‚Üí 7731   (another word with space)

Result: [791, 8415, 7731]</code></pre>

                    <h2>Special Tokens</h2>
                    <p>Vocabularies include special tokens for specific purposes:</p>

                    <div class="info-box">
                        <p><strong>Common Special Tokens:</strong></p>
                        <ul>
                            <li><strong>[START]</strong> or <code>&lt;|begin_of_text|&gt;</code>: Marks the beginning</li>
                            <li><strong>[END]</strong> or <code>&lt;|end_of_text|&gt;</code>: Marks the end</li>
                            <li><strong>[PAD]</strong>: Padding for batch processing</li>
                            <li><strong>[UNK]</strong>: Unknown tokens (rare in modern systems)</li>
                            <li><strong>[MASK]</strong>: For training (BERT-style models)</li>
                        </ul>
                    </div>

                    <h2>Why Token IDs Matter</h2>
                    <p>Token IDs are crucial because:</p>

                    <ol>
                        <li><strong>Consistent</strong>: "cat" always gets the same ID (e.g., 8415)</li>
                        <li><strong>Compact</strong>: One number per token, very efficient</li>
                        <li><strong>Indexed</strong>: Can be used to lookup embeddings</li>
                        <li><strong>Math-friendly</strong>: Numbers work in neural networks</li>
                    </ol>

                    <h2>Vocabulary Size Tradeoff</h2>
                    <pre><code># Smaller Vocabulary (30k tokens)
"unhappiness" ‚Üí ["un", "hap", "pi", "ness"]  # 4 tokens
+ Pros: Fewer parameters, smaller model
- Cons: Longer sequences, less efficiency

# Larger Vocabulary (100k tokens)
"unhappiness" ‚Üí ["unhappiness"]  # 1 token
+ Pros: Shorter sequences, more efficient
- Cons: More parameters, larger model</code></pre>

                    <div class="warning-box">
                        <p><strong>The Sweet Spot:</strong> Most modern models use 30k-50k tokens. This balances:</p>
                        <ul>
                            <li>Model size (fewer tokens = smaller embedding table)</li>
                            <li>Sequence length (more tokens = shorter sequences)</li>
                            <li>Coverage (enough to handle most text)</li>
                        </ul>
                    </div>

                    <h2>From Text to IDs: Complete Flow</h2>
                    <pre><code>Original Text:
"Hello world!"

Step 1: Tokenize
["Hello", " world", "!"]

Step 2: Lookup Token IDs
Hello  ‚Üí 9906
 world ‚Üí 1917
!      ‚Üí 0

Final: [9906, 1917, 0]</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Token IDs convert tokens into unique numbers using a fixed vocabulary. These IDs are what actually get fed into the AI model. Every "cat" becomes the same number (like 8415), making the model consistent and efficient.</p>
                    </div>

                    <h2>Try It Yourself!</h2>
                    <p>You can test tokenization with tools like:</p>
                    <ul>
                        <li><strong>OpenAI Tokenizer</strong>: platform.openai.com/tokenizer</li>
                        <li><strong>tiktoken</strong> (Python): <code>pip install tiktoken</code></li>
                        <li><strong>Hugging Face</strong>: huggingface.co/spaces/Xenova/the-tokenizer-playground</li>
                    </ul>

                    <pre><code># Python example with tiktoken
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")  # GPT-4 tokenizer
text = "The cat sat"
tokens = enc.encode(text)
print(tokens)  # [791, 8415, 7731]</code></pre>
                `,
                quiz: [
                    {
                        question: "What is a token ID?",
                        options: [
                            "A random number assigned to text",
                            "A unique number from a fixed vocabulary",
                            "The ASCII code of the first letter",
                            "A temporary identifier that changes each time"
                        ],
                        correct: 1,
                        explanation: "A token ID is a unique number assigned to each token from a fixed vocabulary. The same token always gets the same ID (e.g., 'cat' ‚Üí 8415)."
                    },
                    {
                        question: "What is the typical vocabulary size for modern LLMs?",
                        options: [
                            "256 tokens",
                            "5,000 tokens",
                            "30,000-100,000 tokens",
                            "1,000,000 tokens"
                        ],
                        correct: 2,
                        explanation: "Modern LLMs typically use vocabularies of 30,000-100,000 tokens. This balances model size, sequence length, and text coverage."
                    },
                    {
                        question: "Why do vocabularies include special tokens like [START] and [END]?",
                        options: [
                            "To make the output look pretty",
                            "To mark boundaries and special positions",
                            "To increase the token count",
                            "They don't - this is deprecated"
                        ],
                        correct: 1,
                        explanation: "Special tokens mark important boundaries (start/end of text) and positions (padding, masking). They help the model understand text structure."
                    }
                ]
            },

            // ============================================
            // üéØ CHECKPOINT 1: Data Foundations & Tokenization Complete
            // ============================================
            {
                id: 5.5,
                title: "üéØ Checkpoint 1: Data & Tokenization",
                category: 2,
                content: `
                    <div style="background: linear-gradient(135deg, #4CAF50 0%, #2E7D32 100%);
                                color: white;
                                padding: 30px;
                                margin: 0 0 30px 0;
                                border-radius: 16px;
                                box-shadow: 0 8px 32px rgba(76, 175, 80, 0.3);">
                        <h1 style="margin-top: 0; color: white; text-align: center;">üéØ Checkpoint 1: Data Foundations Complete!</h1>
                        <p style="text-align: center; font-size: 18px; margin-bottom: 0;">You've completed the first 5 sections. Let's review what you learned!</p>
                    </div>

                    <h2>üìö What You've Learned (Sections 1-5)</h2>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 30px 0;">
                        <div class="info-box">
                            <h3 style="margin-top: 0;">Section 1: What is Data?</h3>
                            <ul style="margin: 10px 0 0 20px;">
                                <li>Computers store everything as binary (0s and 1s)</li>
                                <li>Text becomes numbers (ASCII/Unicode)</li>
                                <li>Example: "A" ‚Üí 65 ‚Üí 01000001</li>
                            </ul>
                        </div>

                        <div class="info-box">
                            <h3 style="margin-top: 0;">Section 2: Text as Data</h3>
                            <ul style="margin: 10px 0 0 20px;">
                                <li>Text is a sequence of character codes</li>
                                <li>UTF-8 handles all languages + emojis</li>
                                <li>"Hello" ‚Üí [72, 101, 108, 108, 111]</li>
                            </ul>
                        </div>

                        <div class="info-box">
                            <h3 style="margin-top: 0;">Section 3: The Problem</h3>
                            <ul style="margin: 10px 0 0 20px;">
                                <li>Character codes don't capture meaning</li>
                                <li>"cat" and "car" are similar in code but different in meaning</li>
                                <li>AI needs rich representations, not simple labels</li>
                            </ul>
                        </div>

                        <div class="info-box">
                            <h3 style="margin-top: 0;">Section 4: Tokens</h3>
                            <ul style="margin: 10px 0 0 20px;">
                                <li>Break text into meaningful chunks</li>
                                <li>"running" ‚Üí ["run", "ning"]</li>
                                <li>More efficient than character-by-character</li>
                            </ul>
                        </div>

                        <div class="info-box" style="grid-column: 1 / -1;">
                            <h3 style="margin-top: 0;">Section 5: Token IDs</h3>
                            <ul style="margin: 10px 0 0 20px;">
                                <li>Each token gets a unique ID from a vocabulary (30K-100K tokens)</li>
                                <li>"cat" ‚Üí 8415 (always the same ID)</li>
                                <li>Special tokens like [START], [END], [PAD]</li>
                            </ul>
                        </div>
                    </div>

                    <h2>üîÑ The Journey So Far</h2>
                    <div style="background: #f5f5f5; padding: 20px; border-radius: 12px; margin: 20px 0;">
                        <pre style="background: transparent; color: #333; margin: 0; padding: 0; box-shadow: none;"><code>Text: "The cat sat"
   ‚Üì
Step 1: Tokenization
   ‚Üì
Tokens: ["The", " cat", " sat"]
   ‚Üì
Step 2: Token IDs (lookup in vocabulary)
   ‚Üì
Token IDs: [791, 8415, 7731]
   ‚Üì
<strong style="color: #FF9800;">‚Üê YOU ARE HERE</strong>

Next: Convert these IDs to meaningful vectors! ‚¨áÔ∏è</code></pre>
                    </div>

                    <h2>‚úÖ Self-Check Questions</h2>
                    <div class="warning-box">
                        <p><strong>Can you explain these concepts to someone else?</strong></p>
                        <ul style="margin-left: 20px;">
                            <li>Why do computers use binary?</li>
                            <li>What's the difference between characters and tokens?</li>
                            <li>Why is tokenization better than using individual characters?</li>
                            <li>What is a token ID and why is it consistent?</li>
                        </ul>
                        <p style="margin-bottom: 0;"><strong>If you can't answer these</strong>, review Sections 1-5 before continuing!</p>
                    </div>

                    <h2>üîÆ What's Next? (Sections 6-10)</h2>
                    <div class="success-box">
                        <p><strong>You're about to learn the magic of vectors and embeddings!</strong></p>
                        <ul style="margin-left: 20px;">
                            <li><strong>Section 6:</strong> What are Vectors? (Lists of numbers that capture meaning)</li>
                            <li><strong>Section 7:</strong> What are Embeddings? (Learned representations)</li>
                            <li><strong>Section 8:</strong> Dimensions (Why GPT-4 uses 4096 numbers per word)</li>
                            <li><strong>Section 9:</strong> Parameters/Weights (The AI's learned knowledge)</li>
                            <li><strong>Section 10:</strong> Embedding Tables (The giant lookup table)</li>
                        </ul>
                        <p style="margin-bottom: 0;"><strong>Goal:</strong> Transform token IDs into rich 4096-dimensional vectors that capture meaning!</p>
                    </div>

                    <h2>üìä Progress</h2>
                    <div style="background: linear-gradient(135deg, rgba(76, 175, 80, 0.1) 0%, rgba(46, 125, 50, 0.1) 100%);
                                padding: 20px;
                                border-radius: 12px;
                                text-align: center;">
                        <p style="font-size: 24px; margin: 0 0 10px 0;"><strong>23% Complete</strong></p>
                        <div style="background: #e0e0e0; height: 30px; border-radius: 15px; overflow: hidden; margin: 0 auto; max-width: 500px;">
                            <div style="background: linear-gradient(90deg, #4CAF50 0%, #2E7D32 100%);
                                        width: 23%;
                                        height: 100%;
                                        display: flex;
                                        align-items: center;
                                        justify-content: center;
                                        color: white;
                                        font-weight: bold;
                                        font-size: 14px;">
                                5/22
                            </div>
                        </div>
                        <p style="margin: 15px 0 0 0; color: #666;">Sections 1-5 ‚úÖ | Sections 6-21 ahead</p>
                    </div>

                    <div style="text-align: center; margin-top: 40px;">
                        <p style="font-size: 20px; color: #4CAF50; font-weight: bold;">Great work! Ready to dive into vectors and embeddings? üöÄ</p>
                    </div>
                `,
                quiz: []
            },

            // ============================================
            // PART 3: VECTORS & EMBEDDINGS (Sections 6-8)
            // ============================================
            {
                id: 6,
                title: "What are Vectors?",
                category: 3,
                content: `
                    <div style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
                                backdrop-filter: blur(10px);
                                padding: 20px;
                                margin: 0 0 30px 0;
                                border-radius: 12px;
                                border-left: 4px solid #667eea;">
                        <h3 style="margin-top: 0; color: #667eea;">üîó Bridge from Tokenization</h3>
                        <p><strong>Great progress!</strong> You now understand tokenization:</p>
                        <ul style="margin-left: 20px;">
                            <li>‚úì Text gets split into <strong>tokens</strong> (meaningful chunks like "cat", "running")</li>
                            <li>‚úì Each token gets a <strong>token ID</strong> from a vocabulary (e.g., "cat" ‚Üí 8415)</li>
                            <li>‚úì These IDs are consistent (same word = same ID every time)</li>
                        </ul>

                        <p><strong>But wait... we still have the same problem!</strong> Token ID 8415 is just a number, like a locker number. It doesn't tell us that "cat" and "dog" are similar (both animals), or that "cat" is very different from "car".</p>

                        <p><strong>This is where vectors come in!</strong> We need to convert these simple IDs into rich numerical representations that capture meaning, relationships, and context. Instead of one number (8415), we'll use <strong>lists of numbers</strong> (vectors) that encode semantic information.</p>

                        <p style="margin: 0;">The journey continues: Token IDs ‚Üí <strong>Vectors</strong> ‚Üí Embeddings ‚Üí AI understanding! ‚ú®</p>
                    </div>

                    <h1>What are Vectors?</h1>

                    <p class="lead">You've learned that tokens get IDs. But numbers like "8415" still don't capture meaning. Enter <strong>vectors</strong> - lists of numbers that CAN represent meaning!</p>

                    <svg class="flow-diagram" width="100%" height="280" viewBox="0 0 800 280">
                        <!-- Single number -->
                        <g>
                            <rect x="30" y="120" width="80" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="70" y="155" text-anchor="middle" font-size="28" fill="#1565C0" font-weight="bold">8415</text>
                            <text x="70" y="105" text-anchor="middle" font-size="12" fill="#666">Token ID</text>
                        </g>

                        <!-- Transform arrow -->
                        <path d="M 120 150 L 180 150" stroke="#FF9800" stroke-width="3" marker-end="url(#arrowV1)">
                            <animate attributeName="opacity" values="0.3;1;0.3" dur="1.5s" repeatCount="indefinite"/>
                        </path>
                        <text x="150" y="135" text-anchor="middle" font-size="12" fill="#FF9800" font-weight="bold">Transform</text>

                        <!-- Vector representation -->
                        <g>
                            <rect x="200" y="50" width="220" height="200" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="3"/>
                            <text x="310" y="75" text-anchor="middle" font-size="14" fill="#E65100" font-weight="bold">Vector (4 dimensions)</text>

                            <!-- Animated bars showing vector values -->
                            <rect x="220" y="95" width="60" height="20" fill="#2196F3" opacity="0.7">
                                <animate attributeName="width" values="0;60" begin="0.5s" dur="0.4s" fill="freeze"/>
                            </rect>
                            <text x="290" y="110" font-size="14" fill="#333">0.8</text>

                            <rect x="220" y="130" width="80" height="20" fill="#4CAF50" opacity="0.7">
                                <animate attributeName="width" values="0;80" begin="0.7s" dur="0.4s" fill="freeze"/>
                            </rect>
                            <text x="310" y="145" font-size="14" fill="#333">0.3</text>

                            <rect x="220" y="165" width="40" height="20" fill="#9C27B0" opacity="0.7">
                                <animate attributeName="width" values="0;40" begin="0.9s" dur="0.4s" fill="freeze"/>
                            </rect>
                            <text x="270" y="180" font-size="14" fill="#333">-0.5</text>

                            <rect x="220" y="200" width="70" height="20" fill="#F44336" opacity="0.7">
                                <animate attributeName="width" values="0;70" begin="1.1s" dur="0.4s" fill="freeze"/>
                            </rect>
                            <text x="300" y="215" font-size="14" fill="#333">0.1</text>

                            <text x="310" y="240" text-anchor="middle" font-size="12" fill="#666" font-family="monospace">[0.8, 0.3, -0.5, 0.1]</text>
                        </g>

                        <!-- Arrow to meaning -->
                        <path d="M 430 150 L 490 150" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrowV2)"/>

                        <!-- Meaning cloud -->
                        <g>
                            <ellipse cx="620" cy="150" rx="140" ry="80" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3"/>
                            <text x="620" y="125" text-anchor="middle" font-size="16" fill="#2E7D32" font-weight="bold">Captures Meaning</text>
                            <text x="620" y="150" text-anchor="middle" font-size="13" fill="#558B2F">‚úì Animal</text>
                            <text x="620" y="170" text-anchor="middle" font-size="13" fill="#558B2F">‚úì Pet</text>
                            <text x="620" y="190" text-anchor="middle" font-size="13" fill="#558B2F">‚úì Small</text>
                        </g>

                        <defs>
                            <marker id="arrowV1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrowV2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Vector = List of Numbers</h2>
                    <p>A <strong>vector</strong> is simply a list of numbers. In AI, we use vectors to represent <strong>meaning in multi-dimensional space</strong>:</p>

                    <div class="info-box">
                        <p><strong>Example: "cat" as a vector</strong></p>
                        <pre><code>Token ID:  8415 (just a label)

Vector:    [0.8, 0.3, -0.5, 0.1, 0.6, -0.2, ...]

Each number represents a different aspect of meaning:
- Position 0: "Is it an animal?" ‚Üí 0.8 (yes!)
- Position 1: "Is it large?" ‚Üí 0.3 (medium)
- Position 2: "Is it scary?" ‚Üí -0.5 (no)
- Position 3: "Does it fly?" ‚Üí 0.1 (rarely)</code></pre>
                    </div>

                    <h2>Why Vectors Work</h2>
                    <p>Vectors let us do <strong>math with meaning</strong>. Similar words have similar vectors:</p>

                    <pre><code>"cat"    ‚Üí [0.8, 0.3, -0.5, 0.1]
"dog"    ‚Üí [0.9, 0.4, -0.4, 0.0]  # Very similar!
"car"    ‚Üí [0.0, 0.8,  0.2, 0.3]  # Completely different!

Distance between cat & dog: 0.15  (close)
Distance between cat & car: 1.42  (far)</code></pre>

                    <div class="success-box">
                        <p><strong>Key Insight:</strong> Words with similar meanings have vectors that are close together in space. This is how AI "understands" that cat and dog are related!</p>
                    </div>

                    <h2>Vector Dimensions</h2>
                    <p>The <strong>dimension</strong> is how many numbers are in the vector:</p>

                    <ul>
                        <li><strong>Small models</strong>: 384 dimensions (GPT-2 small)</li>
                        <li><strong>Medium models</strong>: 768 dimensions (BERT base)</li>
                        <li><strong>Large models</strong>: 1024-4096 dimensions (GPT-3, GPT-4)</li>
                        <li><strong>Massive models</strong>: 12,288 dimensions (GPT-4)</li>
                    </ul>

                    <div class="warning-box">
                        <p><strong>Why so many?</strong> More dimensions = more nuanced meaning. Just like describing a person with more adjectives gives a clearer picture!</p>
                    </div>

                    <h2>Visualizing Vectors</h2>
                    <p>We can't visualize 768 dimensions, but here's a 2D example:</p>

                    <svg width="100%" height="300" viewBox="0 0 500 300">
                        <!-- Coordinate system -->
                        <line x1="50" y1="250" x2="450" y2="250" stroke="#999" stroke-width="2"/>
                        <line x1="50" y1="250" x2="50" y2="50" stroke="#999" stroke-width="2"/>

                        <!-- Axis labels -->
                        <text x="480" y="255" font-size="12" fill="#666">Animal ‚Üí</text>
                        <text x="20" y="40" font-size="12" fill="#666">Pet ‚Üí</text>

                        <!-- Points for words -->
                        <circle cx="350" cy="120" r="8" fill="#2196F3">
                            <animate attributeName="r" values="6;10;6" dur="2s" repeatCount="indefinite"/>
                        </circle>
                        <text x="360" y="115" font-size="14" fill="#1565C0" font-weight="bold">cat</text>

                        <circle cx="380" cy="110" r="8" fill="#4CAF50">
                            <animate attributeName="r" values="6;10;6" dur="2s" begin="0.3s" repeatCount="indefinite"/>
                        </circle>
                        <text x="390" y="105" font-size="14" fill="#2E7D32" font-weight="bold">dog</text>

                        <circle cx="150" cy="230" r="8" fill="#F44336">
                            <animate attributeName="r" values="6;10;6" dur="2s" begin="0.6s" repeatCount="indefinite"/>
                        </circle>
                        <text x="160" y="225" font-size="14" fill="#C62828" font-weight="bold">car</text>

                        <circle cx="300" cy="200" r="8" fill="#9C27B0">
                            <animate attributeName="r" values="6;10;6" dur="2s" begin="0.9s" repeatCount="indefinite"/>
                        </circle>
                        <text x="310" y="195" font-size="14" fill="#6A1B9A" font-weight="bold">fish</text>
                    </svg>

                    <h2>Real Vector Math</h2>
                    <p>You can do actual math with vectors:</p>

                    <pre><code># Vector arithmetic
king - man + woman ‚âà queen

# How it works:
king    = [0.9, 0.1, 0.8, ...]  (male, royal)
man     = [0.9, 0.0, 0.0, ...]  (male, common)
woman   = [0.1, 0.0, 0.0, ...]  (female, common)

Result  = [0.1, 0.1, 0.8, ...]  (female, royal) ‚âà queen!</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Vectors are lists of numbers that capture meaning. Similar meanings = similar vectors. This is the foundation of how AI understands language!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What is a vector in AI?",
                        options: ["A single number", "A list of numbers representing meaning", "A programming language", "A type of neural network"],
                        correct: 1,
                        explanation: "A vector is a list of numbers where each number captures some aspect of meaning. For example, 'cat' might be represented as [0.8, 0.3, -0.5, 0.1, ...]."
                    },
                    {
                        question: "Why do similar words have similar vectors?",
                        options: ["They have the same letters", "They appear together in sentences", "Their meanings are close in multi-dimensional space", "It's random"],
                        correct: 2,
                        explanation: "'cat' and 'dog' have similar vectors because their meanings are related - both are small animals and pets. The vector numbers position them close together in meaning-space."
                    },
                    {
                        question: "How many dimensions does GPT-4 use for its vectors?",
                        options: ["2 dimensions", "384 dimensions", "768 dimensions", "12,288 dimensions"],
                        correct: 3,
                        explanation: "GPT-4 uses 12,288-dimensional vectors! More dimensions allow the model to capture more nuanced aspects of meaning, though we can't visualize that many dimensions."
                    }
                ]
            },
            {
                id: 7,
                title: "What are Embeddings?",
                category: 3,
                content: `
                    <h1>What are Embeddings?</h1>

                    <p class="lead">You know vectors are lists of numbers. But where do these numbers come from? They're called <strong>embeddings</strong> - and they're learned during training!</p>

                    <svg class="flow-diagram" width="100%" height="300" viewBox="0 0 800 300">
                        <!-- Token ID -->
                        <g>
                            <rect x="40" y="130" width="100" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="90" y="150" text-anchor="middle" font-size="12" fill="#666">Token ID</text>
                            <text x="90" y="172" text-anchor="middle" font-size="28" fill="#1565C0" font-weight="bold">8415</text>
                        </g>

                        <!-- Arrow to embedding table -->
                        <path d="M 150 160 L 200 160" stroke="#FF9800" stroke-width="3" marker-end="url(#arrowE1)"/>
                        <text x="175" y="145" text-anchor="middle" font-size="11" fill="#FF9800" font-weight="bold">Lookup</text>

                        <!-- Embedding Table -->
                        <g>
                            <rect x="220" y="40" width="260" height="240" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="3"/>
                            <text x="350" y="65" text-anchor="middle" font-size="14" fill="#E65100" font-weight="bold">Embedding Table</text>
                            <text x="350" y="85" text-anchor="middle" font-size="11" fill="#666">(Learned During Training)</text>

                            <!-- Table rows with animation -->
                            <g opacity="0.3">
                                <rect x="240" y="100" width="200" height="25" fill="#e3f2fd" rx="3"/>
                                <text x="250" y="118" font-size="12" fill="#666" font-family="monospace">8414: [0.2, 0.1, ...]</text>
                            </g>

                            <g>
                                <rect x="240" y="130" width="200" height="25" fill="#c8e6c9" rx="3">
                                    <animate attributeName="opacity" values="0;1" begin="0.5s" dur="0.3s" fill="freeze"/>
                                </rect>
                                <text x="250" y="148" font-size="12" fill="#2E7D32" font-family="monospace" font-weight="bold">8415: [0.8, 0.3, ...]</text>
                                <text x="460" y="148" font-size="18" fill="#4CAF50">‚Üê</text>
                            </g>

                            <g opacity="0.3">
                                <rect x="240" y="160" width="200" height="25" fill="#e3f2fd" rx="3"/>
                                <text x="250" y="178" font-size="12" fill="#666" font-family="monospace">8416: [0.7, 0.5, ...]</text>
                            </g>

                            <text x="350" y="210" text-anchor="middle" font-size="11" fill="#666">50,000 rows (one per token)</text>
                            <text x="350" y="230" text-anchor="middle" font-size="11" fill="#666">768 columns (vector dimensions)</text>
                            <text x="350" y="250" text-anchor="middle" font-size="11" fill="#E65100" font-weight="bold">38M parameters!</text>
                        </g>

                        <!-- Arrow to vector -->
                        <path d="M 490 160 L 550 160" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrowE2)"/>

                        <!-- Output vector -->
                        <g>
                            <rect x="570" y="100" width="180" height="120" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3"/>
                            <text x="660" y="125" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">Embedding Vector</text>
                            <text x="660" y="155" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace">[0.8,</text>
                            <text x="660" y="175" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace"> 0.3,</text>
                            <text x="660" y="195" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace"> -0.5,</text>
                            <text x="660" y="215" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace"> ...]</text>
                        </g>

                        <defs>
                            <marker id="arrowE1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrowE2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Embedding = Learned Vector</h2>
                    <p>An <strong>embedding</strong> is a vector whose values are <strong>learned during training</strong>. Think of it as the AI's "dictionary of meanings":</p>

                    <div class="info-box">
                        <p><strong>How It Works:</strong></p>
                        <ol>
                            <li><strong>Start random</strong>: Initially, token 8415 might be [0.01, -0.23, 0.45, ...]</li>
                            <li><strong>Train on billions of words</strong>: AI sees "cat" in millions of contexts</li>
                            <li><strong>Adjust numbers</strong>: The vector changes to capture meaning</li>
                            <li><strong>Final embedding</strong>: [0.8, 0.3, -0.5, ...] now represents "cat"</li>
                        </ol>
                    </div>

                    <h2>Embedding vs Vector: What's the Difference?</h2>
                    <div class="warning-box">
                        <p><strong>Terminology Clarification:</strong></p>
                        <ul>
                            <li><strong>Vector</strong>: Any list of numbers [0.8, 0.3, -0.5, ...]</li>
                            <li><strong>Embedding</strong>: A vector that was <em>learned during training</em></li>
                            <li><strong>In practice</strong>: People use these terms interchangeably!</li>
                        </ul>
                    </div>

                    <h2>The Embedding Table</h2>
                    <p>All embeddings are stored in a giant table called the <strong>Embedding Table</strong> or <strong>Embedding Matrix</strong>:</p>

                    <pre><code>Embedding Table Dimensions:
- Rows:    50,000 (vocabulary size)
- Columns: 768 (vector dimensions)
- Total:   38,400,000 numbers!

Example lookup:
Token ID 8415 ‚Üí Row 8415 ‚Üí [0.8, 0.3, -0.5, 0.1, ...]</code></pre>

                    <div class="success-box">
                        <p><strong>Speed Trick:</strong> Looking up a row in a table is instant! Token ID 8415 directly accesses row 8415 - no computation needed.</p>
                    </div>

                    <h2>How Embeddings Are Learned</h2>
                    <p>During training, the AI adjusts embeddings to <strong>predict the next word</strong>:</p>

                    <pre><code># Training example
Input:  "The cat sat on the"
Target: "mat"

1. Look up embeddings for each token
2. Process through model
3. Predict next word
4. If wrong, adjust ALL embeddings slightly
5. Repeat billions of times

Result: Embeddings capture meaning!</code></pre>

                    <h2>Why Embeddings Are Powerful</h2>
                    <p>Embeddings automatically capture relationships from raw text:</p>

                    <div class="info-box">
                        <p><strong>What Gets Learned:</strong></p>
                        <ul>
                            <li><strong>Similarity</strong>: "cat" ‚âà "dog" (both pets)</li>
                            <li><strong>Opposites</strong>: "hot" ‚âà -"cold"</li>
                            <li><strong>Relationships</strong>: "Paris" - "France" ‚âà "London" - "UK"</li>
                            <li><strong>Context</strong>: "bank" (river) vs "bank" (money) get different embeddings</li>
                        </ul>
                    </div>

                    <h2>Real Model Example</h2>
                    <p>Let's look at actual numbers from GPT-2:</p>

                    <pre><code># GPT-2 Small
Vocabulary:     50,257 tokens
Embedding dims: 768
Total params:   38,597,376 (just for embeddings!)

# Full calculation:
50,257 tokens √ó 768 dimensions = 38,597,376 numbers

Each number is 32-bit float ‚Üí 154 MB just for embeddings!</code></pre>

                    <div class="warning-box">
                        <p><strong>Trade-off:</strong> Larger vocabulary = better language understanding but more memory. Smaller vocabulary = less memory but words split into more tokens.</p>
                    </div>

                    <h2>Visualizing Training</h2>
                    <svg width="100%" height="200" viewBox="0 0 600 200">
                        <!-- Before training -->
                        <g>
                            <text x="50" y="30" font-size="14" fill="#666" font-weight="bold">Before Training</text>
                            <circle cx="100" cy="100" r="8" fill="#F44336"/>
                            <text x="120" y="105" font-size="13" fill="#666">cat</text>
                            <circle cx="200" cy="80" r="8" fill="#2196F3"/>
                            <text x="220" y="85" font-size="13" fill="#666">dog</text>
                            <circle cx="150" cy="140" r="8" fill="#4CAF50"/>
                            <text x="170" y="145" font-size="13" fill="#666">car</text>
                            <text x="100" y="175" text-anchor="middle" font-size="11" fill="#999">Random positions</text>
                        </g>

                        <!-- Arrow -->
                        <text x="300" y="105" font-size="24" fill="#FF9800">‚Üí</text>
                        <text x="280" y="85" font-size="12" fill="#FF9800">Training</text>

                        <!-- After training -->
                        <g>
                            <text x="380" y="30" font-size="14" fill="#666" font-weight="bold">After Training</text>
                            <circle cx="450" cy="90" r="8" fill="#F44336">
                                <animate attributeName="r" values="6;10;6" dur="2s" repeatCount="indefinite"/>
                            </circle>
                            <text x="470" y="95" font-size="13" fill="#666">cat</text>
                            <circle cx="480" cy="100" r="8" fill="#2196F3">
                                <animate attributeName="r" values="6;10;6" dur="2s" begin="0.3s" repeatCount="indefinite"/>
                            </circle>
                            <text x="500" y="105" font-size="13" fill="#666">dog</text>
                            <circle cx="450" cy="150" r="8" fill="#4CAF50"/>
                            <text x="470" y="155" font-size="13" fill="#666">car</text>
                            <text x="465" y="175" text-anchor="middle" font-size="11" fill="#2E7D32" font-weight="bold">Meaningful clusters!</text>
                        </g>
                    </svg>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Embeddings are learned vectors that capture meaning. The AI starts with random numbers and gradually adjusts them to represent relationships between words!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What is the main difference between a vector and an embedding?",
                        options: ["Vectors are bigger than embeddings", "Embeddings are learned during training", "Embeddings are always positive numbers", "There is no difference"],
                        correct: 1,
                        explanation: "An embedding is a vector whose values are learned during training. While people often use the terms interchangeably, embeddings specifically refer to learned representations."
                    },
                    {
                        question: "How are embeddings stored in an AI model?",
                        options: ["In a database", "In an embedding table/matrix", "In the model's code", "In RAM only"],
                        correct: 1,
                        explanation: "All embeddings are stored in a giant table (matrix) where each row corresponds to a token ID. This allows instant lookup: token ID 8415 ‚Üí row 8415 in the table."
                    },
                    {
                        question: "What happens to embeddings during training?",
                        options: ["They stay random", "They are manually set by programmers", "They gradually adjust to capture meaning", "They are downloaded from the internet"],
                        correct: 2,
                        explanation: "Embeddings start as random numbers and gradually adjust during training. The AI sees billions of examples and tweaks the embeddings to better predict words, making similar words end up with similar embeddings."
                    }
                ]
            },
            {
                id: 8,
                title: "Dimensions - Vector Size",
                category: 3,
                content: `
                    <h1>Dimensions: How Big Are Vectors?</h1>

                    <p class="lead">Vectors can be small [0.8, 0.3] or huge [0.8, 0.3, ..., 0.1] with thousands of numbers. The count of numbers is called <strong>dimensions</strong> - and it's crucial!</p>

                    <svg class="flow-diagram" width="100%" height="320" viewBox="0 0 800 320">
                        <!-- Small vector -->
                        <g>
                            <text x="100" y="40" text-anchor="middle" font-size="14" fill="#666" font-weight="bold">2 Dimensions</text>
                            <rect x="40" y="60" width="120" height="80" rx="8" fill="#ffebee" stroke="#F44336" stroke-width="2"/>
                            <text x="100" y="90" text-anchor="middle" font-size="16" fill="#C62828" font-family="monospace">[0.8,</text>
                            <text x="100" y="115" text-anchor="middle" font-size="16" fill="#C62828" font-family="monospace"> 0.3]</text>
                            <text x="100" y="160" text-anchor="middle" font-size="12" fill="#999">Limited info</text>
                        </g>

                        <!-- Medium vector -->
                        <g>
                            <text x="300" y="40" text-anchor="middle" font-size="14" fill="#666" font-weight="bold">384 Dimensions</text>
                            <rect x="220" y="60" width="160" height="120" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                            <text x="300" y="85" text-anchor="middle" font-size="14" fill="#E65100" font-family="monospace">[0.8, 0.3, -0.5,</text>
                            <text x="300" y="105" text-anchor="middle" font-size="14" fill="#E65100" font-family="monospace"> 0.1, 0.6, -0.2,</text>
                            <text x="300" y="125" text-anchor="middle" font-size="14" fill="#E65100" font-family="monospace"> 0.4, 0.9, ...</text>
                            <text x="300" y="145" text-anchor="middle" font-size="14" fill="#E65100" font-family="monospace"> ..., 0.7]</text>
                            <text x="300" y="195" text-anchor="middle" font-size="12" fill="#999">Good detail</text>
                        </g>

                        <!-- Large vector -->
                        <g>
                            <text x="550" y="40" text-anchor="middle" font-size="14" fill="#666" font-weight="bold">12,288 Dimensions</text>
                            <rect x="440" y="60" width="220" height="180" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3"/>
                            <text x="550" y="85" text-anchor="middle" font-size="13" fill="#2E7D32" font-family="monospace">[0.823, 0.391, -0.512,</text>
                            <text x="550" y="105" text-anchor="middle" font-size="13" fill="#2E7D32" font-family="monospace"> 0.104, 0.678, -0.234,</text>
                            <text x="550" y="125" text-anchor="middle" font-size="13" fill="#2E7D32" font-family="monospace"> 0.445, 0.891, 0.123,</text>
                            <text x="550" y="145" text-anchor="middle" font-size="13" fill="#2E7D32" font-family="monospace"> -0.667, 0.934, ...</text>
                            <text x="550" y="175" text-anchor="middle" font-size="16" fill="#1B5E20" font-weight="bold">...</text>
                            <text x="550" y="205" text-anchor="middle" font-size="13" fill="#2E7D32" font-family="monospace"> ..., 0.712]</text>
                            <text x="550" y="255" text-anchor="middle" font-size="12" fill="#2E7D32" font-weight="bold">Extreme nuance!</text>
                        </g>

                        <!-- Comparison arrows -->
                        <path d="M 170 100 L 210 100" stroke="#666" stroke-width="2" marker-end="url(#arrowD1)"/>
                        <path d="M 390 120 L 430 120" stroke="#666" stroke-width="2" marker-end="url(#arrowD1)"/>
                        <text x="400" y="300" text-anchor="middle" font-size="14" fill="#1976D2" font-weight="bold">More dimensions ‚Üí More meaning captured</text>

                        <defs>
                            <marker id="arrowD1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>What Are Dimensions?</h2>
                    <p>The <strong>dimension</strong> or <strong>hidden size</strong> is simply how many numbers are in each vector:</p>

                    <div class="info-box">
                        <p><strong>Examples:</strong></p>
                        <ul>
                            <li><strong>2D vector</strong>: [0.8, 0.3] ‚Üí 2 dimensions</li>
                            <li><strong>768D vector</strong>: [0.8, 0.3, -0.5, ..., 0.1] ‚Üí 768 dimensions</li>
                            <li><strong>12,288D vector</strong>: GPT-4's massive vectors!</li>
                        </ul>
                    </div>

                    <h2>Why More Dimensions?</h2>
                    <p>More dimensions = more ways to capture meaning. It's like describing a person:</p>

                    <pre><code># 2 dimensions (limited)
Person: [height=5.8, weight=160]
‚Üí Can only capture physical size

# 100 dimensions (better)
Person: [height, weight, age, kindness, intelligence,
         humor, honesty, creativity, patience, ...]
‚Üí Captures personality & traits!

# 12,288 dimensions (AI)
Word embedding can capture:
- Syntax, grammar, semantics
- Context, tone, formality
- Relationships, analogies
- And thousands more subtle patterns!</code></pre>

                    <div class="success-box">
                        <p><strong>Key Insight:</strong> Each dimension captures a different aspect of meaning. More dimensions = richer, more nuanced understanding!</p>
                    </div>

                    <h2>Real Model Dimensions</h2>
                    <p>Here are actual dimensions from popular models:</p>

                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Model</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">Dimensions</th>
                            <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Use Case</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-2 Small</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">768</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Learning, experiments</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">BERT Base</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">768</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Text classification</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-2 Medium</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">1,024</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Better generation</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-2 Large</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">1,280</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Quality text</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-3</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">12,288</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Advanced reasoning</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px; border: 1px solid #ddd; font-weight: bold;">GPT-4</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace; font-weight: bold;">12,288</td>
                            <td style="padding: 10px; border: 1px solid #ddd; font-weight: bold;">State-of-the-art</td>
                        </tr>
                    </table>

                    <h2>The Trade-off: Size vs Speed</h2>
                    <div class="warning-box">
                        <p><strong>More dimensions means:</strong></p>
                        <ul>
                            <li>‚úÖ <strong>Better understanding</strong> - captures more nuance</li>
                            <li>‚úÖ <strong>Higher quality</strong> - more accurate predictions</li>
                            <li>‚ùå <strong>More memory</strong> - takes more RAM/VRAM</li>
                            <li>‚ùå <strong>Slower processing</strong> - more calculations needed</li>
                        </ul>
                    </div>

                    <h2>Memory Impact</h2>
                    <p>Let's calculate memory for embeddings:</p>

                    <pre><code># GPT-2 Small (768 dimensions)
50,257 tokens √ó 768 dims √ó 4 bytes = 154 MB

# GPT-4 (12,288 dimensions)
50,000 tokens √ó 12,288 dims √ó 4 bytes = 2.4 GB!

Just for the embedding table!</code></pre>

                    <h2>Visualizing Dimensions</h2>
                    <svg width="100%" height="250" viewBox="0 0 700 250">
                        <!-- 1D -->
                        <g>
                            <text x="80" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">1D</text>
                            <line x1="20" y1="60" x2="140" y2="60" stroke="#2196F3" stroke-width="4"/>
                            <circle cx="80" cy="60" r="6" fill="#1565C0">
                                <animate attributeName="r" values="4;8;4" dur="2s" repeatCount="indefinite"/>
                            </circle>
                            <text x="80" y="85" text-anchor="middle" font-size="11" fill="#999">A line</text>
                        </g>

                        <!-- 2D -->
                        <g>
                            <text x="280" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">2D</text>
                            <rect x="220" y="45" width="120" height="120" fill="none" stroke="#4CAF50" stroke-width="3"/>
                            <circle cx="280" cy="105" r="6" fill="#2E7D32">
                                <animate attributeName="r" values="4;8;4" dur="2s" begin="0.3s" repeatCount="indefinite"/>
                            </circle>
                            <text x="280" y="185" text-anchor="middle" font-size="11" fill="#999">A plane</text>
                        </g>

                        <!-- 3D -->
                        <g>
                            <text x="480" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">3D</text>
                            <!-- Cube representation -->
                            <path d="M 420 80 L 500 80 L 500 160 L 420 160 Z" fill="none" stroke="#FF9800" stroke-width="3"/>
                            <path d="M 450 50 L 530 50 L 530 130 L 450 130 Z" fill="none" stroke="#FF9800" stroke-width="3"/>
                            <line x1="420" y1="80" x2="450" y2="50" stroke="#FF9800" stroke-width="2"/>
                            <line x1="500" y1="80" x2="530" y2="50" stroke="#FF9800" stroke-width="2"/>
                            <line x1="500" y1="160" x2="530" y2="130" stroke="#FF9800" stroke-width="2"/>
                            <line x1="420" y1="160" x2="450" y2="130" stroke="#FF9800" stroke-width="2"/>
                            <circle cx="475" cy="105" r="6" fill="#E65100">
                                <animate attributeName="r" values="4;8;4" dur="2s" begin="0.6s" repeatCount="indefinite"/>
                            </circle>
                            <text x="480" y="185" text-anchor="middle" font-size="11" fill="#999">A space</text>
                        </g>

                        <!-- 768D -->
                        <g>
                            <text x="620" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">768D</text>
                            <text x="620" y="105" text-anchor="middle" font-size="40" fill="#9C27B0">?</text>
                            <text x="620" y="185" text-anchor="middle" font-size="11" fill="#6A1B9A" font-weight="bold">Beyond imagination!</text>
                        </g>
                    </svg>

                    <div class="info-box">
                        <p><strong>Fun Fact:</strong> We can't visualize 768 dimensions, but the math works the same way! Distance, similarity, and clustering all work in high-dimensional space just like in 2D or 3D.</p>
                    </div>

                    <h2>Choosing Dimensions</h2>
                    <p>Model designers choose dimensions based on:</p>

                    <ol>
                        <li><strong>Task complexity</strong>: Translation needs more dimensions than simple classification</li>
                        <li><strong>Data size</strong>: More training data ‚Üí can support more dimensions</li>
                        <li><strong>Hardware limits</strong>: Your GPU determines max practical size</li>
                        <li><strong>Speed requirements</strong>: Real-time apps need smaller dimensions</li>
                    </ol>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Dimensions determine how much meaning a model can capture. More dimensions = richer understanding but slower processing. It's all about finding the right balance!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What does 'dimension' mean for a vector?",
                        options: ["The size of the model file", "How many numbers are in the vector", "The training time", "The vocabulary size"],
                        correct: 1,
                        explanation: "Dimensions are simply the count of numbers in a vector. A 768-dimensional vector has 768 numbers, like [0.8, 0.3, -0.5, ..., 0.1] with 768 total values."
                    },
                    {
                        question: "Why do larger models use more dimensions?",
                        options: ["To make training faster", "To use less memory", "To capture more nuanced meaning", "To reduce file size"],
                        correct: 2,
                        explanation: "More dimensions allow the model to capture more aspects of meaning. Each dimension can represent a different feature - just like describing a person with more adjectives gives a richer picture!"
                    },
                    {
                        question: "What's the main trade-off with higher dimensions?",
                        options: ["Better quality but more memory/slower", "Faster but less accurate", "Smaller files but worse results", "No trade-off, always better"],
                        correct: 0,
                        explanation: "Higher dimensions capture more meaning and produce better results, but require more memory and processing time. A 12,288-dimensional model is more capable but needs more resources than a 768-dimensional one."
                    }
                ]
            },

            // ============================================
            // PART 4: MODEL PARAMETERS (Sections 9-10)
            // ============================================
            {
                id: 9,
                title: "What are Parameters/Weights?",
                category: 4,
                content: `
                    <div style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
                                backdrop-filter: blur(10px);
                                padding: 20px;
                                margin: 0 0 30px 0;
                                border-radius: 12px;
                                border-left: 4px solid #667eea;">
                        <h3 style="margin-top: 0; color: #667eea;">üîó From Vectors to Parameters</h3>
                        <p><strong>Quick recap of where we are:</strong></p>
                        <ul style="margin-left: 20px;">
                            <li>‚úì <strong>Tokens</strong> ‚Üí Words broken into chunks ("cat")</li>
                            <li>‚úì <strong>Token IDs</strong> ‚Üí Unique numbers for each token (8415)</li>
                            <li>‚úì <strong>Vectors</strong> ‚Üí Lists of numbers that CAN capture meaning [0.8, 0.3, -0.5, ...]</li>
                            <li>‚úì <strong>Dimensions</strong> ‚Üí Size of vectors (GPT-4 uses 4096 numbers per word!)</li>
                        </ul>

                        <p><strong>But where do those 4096 numbers come from?</strong> How does the AI "know" that "cat" should be represented as [0.8, 0.3, -0.5, 0.1, ...]?</p>

                        <p><strong>Answer: Parameters!</strong> Parameters (also called weights) are the <strong>learned values</strong> stored in the model. They're like the AI's "memory" or "knowledge" - billions of numbers learned from massive amounts of text data.</p>

                        <p>Think of it this way: When you learned math, you memorized multiplication tables (3√ó4=12). Similarly, AI models "memorize" that "cat" should map to specific vector values through training on billions of examples.</p>

                        <p style="margin: 0;"><strong>Next up:</strong> Understand what parameters are and why "GPT-4 has 1.76 trillion parameters" actually means! üß†</p>
                    </div>

                    <h1>What are Parameters/Weights?</h1>

                    <p class="lead">You've heard "GPT-4 has 1.76 trillion parameters!" But what ARE parameters? They're the <strong>learned numbers</strong> that make AI work - and they're everywhere!</p>

                    <svg class="flow-diagram" width="100%" height="280" viewBox="0 0 800 280">
                        <!-- Input -->
                        <g>
                            <rect x="30" y="110" width="100" height="60" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="80" y="135" text-anchor="middle" font-size="12" fill="#666">Input Vector</text>
                            <text x="80" y="155" text-anchor="middle" font-size="14" fill="#1565C0" font-family="monospace">[0.8, 0.3]</text>
                        </g>

                        <!-- Multiply arrow -->
                        <path d="M 140 140 L 190 140" stroke="#FF9800" stroke-width="3" marker-end="url(#arrowP1)"/>
                        <text x="165" y="125" text-anchor="middle" font-size="12" fill="#FF9800" font-weight="bold">√ó</text>

                        <!-- Parameters/Weights Matrix -->
                        <g>
                            <rect x="210" y="60" width="180" height="160" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="3"/>
                            <text x="300" y="85" text-anchor="middle" font-size="14" fill="#E65100" font-weight="bold">Parameters (Weights)</text>

                            <!-- Matrix visualization -->
                            <g>
                                <rect x="230" y="100" width="60" height="25" fill="#e3f2fd" rx="3"/>
                                <text x="260" y="118" text-anchor="middle" font-size="12" fill="#666" font-family="monospace">0.5</text>
                                <rect x="300" y="100" width="60" height="25" fill="#e3f2fd" rx="3"/>
                                <text x="330" y="118" text-anchor="middle" font-size="12" fill="#666" font-family="monospace">0.2</text>
                            </g>
                            <g>
                                <rect x="230" y="135" width="60" height="25" fill="#e3f2fd" rx="3"/>
                                <text x="260" y="153" text-anchor="middle" font-size="12" fill="#666" font-family="monospace">0.1</text>
                                <rect x="300" y="135" width="60" height="25" fill="#e3f2fd" rx="3"/>
                                <text x="330" y="153" text-anchor="middle" font-size="12" fill="#666" font-family="monospace">0.9</text>
                            </g>

                            <text x="300" y="190" text-anchor="middle" font-size="11" fill="#E65100" font-weight="bold">Learned during training!</text>
                            <text x="300" y="210" text-anchor="middle" font-size="10" fill="#666">These numbers change to fit the data</text>
                        </g>

                        <!-- Equals arrow -->
                        <path d="M 400 140 L 450 140" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrowP2)"/>
                        <text x="425" y="125" text-anchor="middle" font-size="12" fill="#4CAF50" font-weight="bold">=</text>

                        <!-- Output -->
                        <g>
                            <rect x="470" y="95" width="150" height="90" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3"/>
                            <text x="545" y="120" text-anchor="middle" font-size="12" fill="#2E7D32" font-weight="bold">Output Vector</text>
                            <text x="545" y="150" text-anchor="middle" font-size="14" fill="#2E7D32" font-family="monospace">[0.46,</text>
                            <text x="545" y="170" text-anchor="middle" font-size="14" fill="#2E7D32" font-family="monospace"> 0.35]</text>
                        </g>

                        <!-- Animation -->
                        <circle cx="300" cy="140" r="50" fill="none" stroke="#FF9800" stroke-width="2" opacity="0.3">
                            <animate attributeName="r" values="40;60;40" dur="2s" repeatCount="indefinite"/>
                            <animate attributeName="opacity" values="0.3;0.1;0.3" dur="2s" repeatCount="indefinite"/>
                        </circle>

                        <defs>
                            <marker id="arrowP1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrowP2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Parameters = Learned Numbers</h2>
                    <p><strong>Parameters</strong> (also called <strong>weights</strong>) are the numbers the AI learns during training. Every calculation in the model uses these numbers:</p>

                    <div class="info-box">
                        <p><strong>What Parameters Do:</strong></p>
                        <ul>
                            <li><strong>Transform vectors</strong>: Multiply input by parameters to get output</li>
                            <li><strong>Capture patterns</strong>: Store learned knowledge from training data</li>
                            <li><strong>Change during training</strong>: Start random, adjust to fit data</li>
                            <li><strong>Stay fixed after training</strong>: Frozen when you use the model</li>
                        </ul>
                    </div>

                    <h2>Simple Example: Matrix Multiplication</h2>
                    <p>Every layer in a neural network does this:</p>

                    <pre><code>Input:  [0.8, 0.3]
Weights: [[0.5, 0.2],
          [0.1, 0.9]]

Calculation:
Output[0] = 0.8 √ó 0.5 + 0.3 √ó 0.1 = 0.43
Output[1] = 0.8 √ó 0.2 + 0.3 √ó 0.9 = 0.43

Result: [0.43, 0.43]</code></pre>

                    <div class="success-box">
                        <p><strong>Key Insight:</strong> These weight values (0.5, 0.2, 0.1, 0.9) are the PARAMETERS! They determine how input transforms to output.</p>
                    </div>

                    <h2>Where Are All The Parameters?</h2>
                    <p>Parameters are EVERYWHERE in AI models:</p>

                    <div class="warning-box">
                        <p><strong>Parameter Locations:</strong></p>
                        <ol>
                            <li><strong>Embedding Table</strong>: Token ID ‚Üí Vector (millions of params)</li>
                            <li><strong>Attention Weights</strong>: Q, K, V matrices (billions of params)</li>
                            <li><strong>Feed-Forward Networks</strong>: Layer connections (billions of params)</li>
                            <li><strong>Layer Norms</strong>: Normalization parameters (thousands)</li>
                            <li><strong>Output Layer</strong>: Final predictions (millions)</li>
                        </ol>
                    </div>

                    <h2>Counting Parameters: GPT-2 Example</h2>
                    <pre><code># GPT-2 Small (124M parameters)

Embedding:          50,257 √ó 768 = 38,597,376
Position Embedding:  1,024 √ó 768 =    786,432
12 Transformer Blocks:
  - Attention:     4 √ó (768 √ó 768) = 2,359,296 per block
  - FFN:           2 √ó (768 √ó 3072) = 4,718,592 per block
  - Total per block: 7,077,888
  - 12 blocks: 84,934,656

Output Layer:       768 √ó 50,257 = 38,597,376

TOTAL: ~124,000,000 parameters!</code></pre>

                    <h2>Why "Weights" and "Parameters"?</h2>
                    <div class="info-box">
                        <p><strong>Terminology:</strong></p>
                        <ul>
                            <li><strong>Weights</strong>: Used in traditional neural networks (comes from "weighted sum")</li>
                            <li><strong>Parameters</strong>: Modern term, includes weights AND biases</li>
                            <li><strong>In practice</strong>: People use them interchangeably</li>
                        </ul>
                        <p>When someone says "GPT-4 has 1.76 trillion parameters," they mean ALL the learned numbers in the model!</p>
                    </div>

                    <h2>How Parameters Are Learned</h2>
                    <p>During training, parameters gradually adjust to minimize errors:</p>

                    <svg width="100%" height="200" viewBox="0 0 600 200">
                        <!-- Initial random -->
                        <g>
                            <text x="80" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">Start: Random</text>
                            <circle cx="80" cy="100" r="40" fill="#ffebee" stroke="#F44336" stroke-width="3"/>
                            <text x="80" y="105" text-anchor="middle" font-size="12" fill="#C62828" font-family="monospace">W=0.12</text>
                            <text x="80" y="160" text-anchor="middle" font-size="11" fill="#999">Wrong predictions</text>
                        </g>

                        <!-- Training -->
                        <text x="200" y="105" font-size="24" fill="#FF9800">‚Üí</text>
                        <text x="185" y="85" font-size="12" fill="#FF9800">Training</text>
                        <text x="170" y="130" font-size="10" fill="#666">Adjust weights</text>

                        <!-- Middle -->
                        <g>
                            <text x="320" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">Training...</text>
                            <circle cx="320" cy="100" r="40" fill="#fff3e0" stroke="#FF9800" stroke-width="3">
                                <animate attributeName="r" values="38;42;38" dur="1.5s" repeatCount="indefinite"/>
                            </circle>
                            <text x="320" y="105" text-anchor="middle" font-size="12" fill="#E65100" font-family="monospace">W=0.47</text>
                            <text x="320" y="160" text-anchor="middle" font-size="11" fill="#999">Getting better...</text>
                        </g>

                        <!-- Final -->
                        <text x="440" y="105" font-size="24" fill="#4CAF50">‚Üí</text>

                        <g>
                            <text x="520" y="30" text-anchor="middle" font-size="13" fill="#666" font-weight="bold">Trained!</text>
                            <circle cx="520" cy="100" r="40" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3"/>
                            <text x="520" y="105" text-anchor="middle" font-size="12" fill="#2E7D32" font-family="monospace">W=0.83</text>
                            <text x="520" y="160" text-anchor="middle" font-size="11" fill="#2E7D32" font-weight="bold">Good predictions!</text>
                        </g>
                    </svg>

                    <h2>Model Size = Parameter Count</h2>
                    <p>When you hear "7B model" or "70B model," the B stands for BILLION parameters:</p>

                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Model</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">Parameters</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">File Size (FP16)</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-2 Small</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">124M</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd;">~240 MB</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">LLaMA-2 7B</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">7B</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd;">~13 GB</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">LLaMA-2 13B</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">13B</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd;">~25 GB</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">LLaMA-2 70B</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">70B</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd;">~135 GB</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px; border: 1px solid #ddd; font-weight: bold;">GPT-4</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace; font-weight: bold;">~1.76T</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-weight: bold;">~3.4 TB!</td>
                        </tr>
                    </table>

                    <div class="warning-box">
                        <p><strong>File Size Formula:</strong> Parameters √ó Bytes per Parameter = File Size</p>
                        <p>Example: 7B params √ó 2 bytes (FP16) = 14 GB (roughly)</p>
                    </div>

                    <h2>Why More Parameters = Better?</h2>
                    <p>More parameters allow the model to:</p>

                    <ul>
                        <li><strong>Memorize more patterns</strong> from training data</li>
                        <li><strong>Capture subtle relationships</strong> between concepts</li>
                        <li><strong>Perform complex reasoning</strong> across multiple steps</li>
                        <li><strong>Generalize better</strong> to new situations</li>
                    </ul>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Parameters are the learned numbers that make AI work. When training, these numbers adjust to fit the data. When using the model, they stay fixed. Model size = parameter count!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What are parameters in an AI model?",
                        options: ["The input data", "Learned numbers that transform inputs to outputs", "The training algorithm", "The model architecture"],
                        correct: 1,
                        explanation: "Parameters (weights) are the learned numbers in the model. They multiply inputs to produce outputs, and they're adjusted during training to fit the data."
                    },
                    {
                        question: "When do parameters change?",
                        options: ["Every time you use the model", "Only during training", "When you load the model", "Never, they're fixed"],
                        correct: 1,
                        explanation: "Parameters change during training as the model learns from data. Once training is complete, they're frozen and stay the same when you use the model."
                    },
                    {
                        question: "What does '7B model' mean?",
                        options: ["7 billion bytes", "7 billion bits", "7 billion parameters", "7 billion tokens"],
                        correct: 2,
                        explanation: "The 'B' stands for billion parameters. A 7B model has 7 billion learned numbers (weights) that were adjusted during training."
                    }
                ]
            },
            {
                id: 10,
                title: "Embedding Tables - The Dictionary",
                category: 4,
                content: `
                    <h1>Embedding Tables: The AI's Dictionary</h1>

                    <p class="lead">The <strong>Embedding Table</strong> is where ALL the magic begins. It's a giant lookup table that converts token IDs into meaningful vectors - instantly!</p>

                    <svg class="flow-diagram" width="100%" height="320" viewBox="0 0 850 320">
                        <!-- Token IDs flowing in -->
                        <g>
                            <text x="70" y="30" text-anchor="middle" font-size="14" fill="#666" font-weight="bold">Token IDs</text>

                            <rect x="30" y="50" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="70" y="82" text-anchor="middle" font-size="20" fill="#1565C0" font-weight="bold">8415</text>

                            <rect x="30" y="120" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="70" y="152" text-anchor="middle" font-size="20" fill="#1565C0" font-weight="bold">1234</text>

                            <rect x="30" y="190" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="70" y="222" text-anchor="middle" font-size="20" fill="#1565C0" font-weight="bold">9876</text>
                        </g>

                        <!-- Arrows to table -->
                        <path d="M 120 75 L 170 75" stroke="#FF9800" stroke-width="3" marker-end="url(#arrowT1)"/>
                        <path d="M 120 145 L 170 145" stroke="#FF9800" stroke-width="3" marker-end="url(#arrowT1)"/>
                        <path d="M 120 215 L 170 215" stroke="#FF9800" stroke-width="3" marker-end="url(#arrowT1)"/>

                        <text x="145" y="55" text-anchor="middle" font-size="11" fill="#FF9800" font-weight="bold">Lookup</text>

                        <!-- Embedding Table -->
                        <g>
                            <rect x="190" y="30" width="300" height="270" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="4"/>
                            <text x="340" y="60" text-anchor="middle" font-size="16" fill="#E65100" font-weight="bold">Embedding Table</text>
                            <text x="340" y="80" text-anchor="middle" font-size="12" fill="#666">(50,000 rows √ó 768 columns)</text>

                            <!-- Table visualization -->
                            <g opacity="0.4">
                                <text x="210" y="110" font-size="12" fill="#666" font-family="monospace">0: [0.1, 0.2, 0.3, ...]</text>
                                <text x="210" y="130" font-size="12" fill="#666" font-family="monospace">1: [0.4, 0.5, 0.6, ...]</text>
                                <text x="210" y="150" font-size="12" fill="#666" font-family="monospace">...</text>
                            </g>

                            <!-- Highlighted row 8415 -->
                            <rect x="200" y="160" width="270" height="25" fill="#c8e6c9" rx="4">
                                <animate attributeName="opacity" values="0.5;1;0.5" dur="2s" repeatCount="indefinite"/>
                            </rect>
                            <text x="210" y="178" font-size="12" fill="#1B5E20" font-family="monospace" font-weight="bold">8415: [0.8, 0.3, -0.5, 0.1, ...]</text>

                            <g opacity="0.4">
                                <text x="210" y="200" font-size="12" fill="#666" font-family="monospace">8416: [0.2, 0.7, 0.1, ...]</text>
                                <text x="210" y="220" font-size="12" fill="#666" font-family="monospace">...</text>
                                <text x="210" y="240" font-size="12" fill="#666" font-family="monospace">49999: [0.9, 0.4, ...]</text>
                            </g>

                            <text x="340" y="275" text-anchor="middle" font-size="11" fill="#E65100" font-weight="bold">38 Million Parameters!</text>
                        </g>

                        <!-- Arrows to vectors -->
                        <path d="M 500 75 L 550 75" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrowT2)"/>
                        <path d="M 500 173 L 550 115" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrowT2)" opacity="0.6"/>
                        <path d="M 500 215 L 550 155" stroke="#4CAF50" stroke-width="3" marker-end="url(#arrowT2)" opacity="0.4"/>

                        <!-- Output vectors -->
                        <g>
                            <rect x="570" y="50" width="240" height="130" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3"/>
                            <text x="690" y="75" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">Embedding Vectors</text>
                            <text x="690" y="100" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace">[0.8, 0.3, -0.5, ..., 0.1]</text>
                            <text x="690" y="125" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace">[0.2, 0.9, 0.4, ..., 0.3]</text>
                            <text x="690" y="150" text-anchor="middle" font-size="12" fill="#558B2F" font-family="monospace">[0.6, 0.1, 0.7, ..., 0.5]</text>
                            <text x="690" y="170" text-anchor="middle" font-size="11" fill="#2E7D32">Ready for processing!</text>
                        </g>

                        <defs>
                            <marker id="arrowT1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrowT2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>What Is an Embedding Table?</h2>
                    <p>An <strong>Embedding Table</strong> (also called Embedding Matrix) is a 2D grid of numbers:</p>

                    <div class="info-box">
                        <p><strong>Structure:</strong></p>
                        <ul>
                            <li><strong>Rows</strong>: One for each token in vocabulary (e.g., 50,000)</li>
                            <li><strong>Columns</strong>: Embedding dimensions (e.g., 768)</li>
                            <li><strong>Each cell</strong>: A single learned number (parameter)</li>
                            <li><strong>Total size</strong>: Rows √ó Columns = millions of parameters!</li>
                        </ul>
                    </div>

                    <h2>How It Works: Instant Lookup</h2>
                    <p>The beauty of the embedding table is its <strong>simplicity and speed</strong>:</p>

                    <pre><code># Vocabulary size: 50,000
# Embedding dimensions: 768

Embedding_Table = [
    [0.1, 0.2, 0.3, ..., 0.5],  # Row 0 (Token ID 0)
    [0.4, 0.5, 0.6, ..., 0.2],  # Row 1 (Token ID 1)
    ...
    [0.8, 0.3, -0.5, ..., 0.1], # Row 8415 (Token ID 8415 = "cat")
    ...
    [0.9, 0.4, 0.7, ..., 0.3]   # Row 49,999
]

# Lookup is instant!
Token_ID = 8415
Embedding_Vector = Embedding_Table[8415]
# Result: [0.8, 0.3, -0.5, ..., 0.1]</code></pre>

                    <div class="success-box">
                        <p><strong>Speed Hack:</strong> Array indexing is O(1) - instant! No computation needed, just direct memory access.</p>
                    </div>

                    <h2>Real Example: GPT-2</h2>
                    <p>Let's look at actual numbers from GPT-2:</p>

                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Property</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">GPT-2 Small</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">GPT-4</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">Vocabulary Size (rows)</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">50,257</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">~100,000</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">Embedding Dims (columns)</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">768</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">12,288</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">Total Parameters</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">38,597,376</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">~1.2 Billion</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px; border: 1px solid #ddd; font-weight: bold;">Memory (FP32)</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace; font-weight: bold;">~147 MB</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace; font-weight: bold;">~4.8 GB</td>
                        </tr>
                    </table>

                    <h2>Why Embedding Tables Are Special</h2>
                    <p>The embedding table is the <strong>first and most important parameter group</strong> in any LLM:</p>

                    <div class="warning-box">
                        <p><strong>Key Properties:</strong></p>
                        <ul>
                            <li>üéØ <strong>First layer</strong>: Converts discrete tokens to continuous vectors</li>
                            <li>‚ö° <strong>Instant lookup</strong>: No computation, just memory access</li>
                            <li>üìä <strong>Huge parameter count</strong>: Often 30-50% of all model parameters</li>
                            <li>üß† <strong>Stores meaning</strong>: Captures word relationships and semantics</li>
                            <li>üîí <strong>Shared with output</strong>: Often same table used for predictions (tied weights)</li>
                        </ul>
                    </div>

                    <h2>Tied Weights: A Common Trick</h2>
                    <p>Many models use the SAME embedding table for both input and output:</p>

                    <pre><code># Input: Token ID ‚Üí Vector
embedding = Embedding_Table[token_id]

# Output: Vector ‚Üí Token probabilities
# Same table, but transposed!
logits = output_vector @ Embedding_Table.T

# This saves MILLIONS of parameters!</code></pre>

                    <h2>Position Embeddings: The Partner Table</h2>
                    <p>There's often a SECOND embedding table for positions:</p>

                    <div class="info-box">
                        <p><strong>Two Tables Working Together:</strong></p>
                        <ul>
                            <li><strong>Token Embedding</strong>: Maps token ID ‚Üí meaning vector</li>
                            <li><strong>Position Embedding</strong>: Maps position ‚Üí position vector</li>
                            <li><strong>Final embedding</strong>: Token embedding + Position embedding</li>
                        </ul>
                        <pre><code>Token "cat" at position 5:
Token_Emb = Embedding_Table[8415]      # [0.8, 0.3, ...]
Pos_Emb = Position_Table[5]            # [0.1, -0.2, ...]
Final = Token_Emb + Pos_Emb            # [0.9, 0.1, ...]</code></pre>
                    </div>

                    <h2>Visualizing the Table</h2>
                    <svg width="100%" height="280" viewBox="0 0 700 280">
                        <!-- Table structure -->
                        <g>
                            <rect x="50" y="40" width="600" height="200" rx="8" fill="#f5f5f5" stroke="#666" stroke-width="2"/>

                            <!-- Row labels -->
                            <text x="30" y="80" text-anchor="end" font-size="11" fill="#666">Token 0</text>
                            <text x="30" y="120" text-anchor="end" font-size="11" fill="#666">Token 1</text>
                            <text x="30" y="160" text-anchor="end" font-size="11" fill="#666">...</text>
                            <text x="30" y="200" text-anchor="end" font-size="11" fill="#666">Token 8415</text>
                            <text x="30" y="240" text-anchor="end" font-size="11" fill="#666">...</text>

                            <!-- Column labels -->
                            <text x="100" y="25" text-anchor="middle" font-size="11" fill="#666">Dim 0</text>
                            <text x="200" y="25" text-anchor="middle" font-size="11" fill="#666">Dim 1</text>
                            <text x="300" y="25" text-anchor="middle" font-size="11" fill="#666">Dim 2</text>
                            <text x="400" y="25" text-anchor="middle" font-size="11" fill="#666">...</text>
                            <text x="550" y="25" text-anchor="middle" font-size="11" fill="#666">Dim 767</text>

                            <!-- Sample cells -->
                            <rect x="60" y="70" width="70" height="20" fill="#e3f2fd" stroke="#2196F3"/>
                            <text x="95" y="84" text-anchor="middle" font-size="10" fill="#1565C0">0.12</text>

                            <rect x="160" y="70" width="70" height="20" fill="#e3f2fd" stroke="#2196F3"/>
                            <text x="195" y="84" text-anchor="middle" font-size="10" fill="#1565C0">0.45</text>

                            <!-- Highlighted row for token 8415 -->
                            <rect x="60" y="190" width="570" height="20" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2">
                                <animate attributeName="opacity" values="0.6;1;0.6" dur="2s" repeatCount="indefinite"/>
                            </rect>
                            <text x="95" y="204" text-anchor="middle" font-size="10" fill="#1B5E20" font-weight="bold">0.8</text>
                            <text x="195" y="204" text-anchor="middle" font-size="10" fill="#1B5E20" font-weight="bold">0.3</text>
                            <text x="295" y="204" text-anchor="middle" font-size="10" fill="#1B5E20" font-weight="bold">-0.5</text>
                            <text x="400" y="204" text-anchor="middle" font-size="10" fill="#1B5E20" font-weight="bold">...</text>
                            <text x="550" y="204" text-anchor="middle" font-size="10" fill="#1B5E20" font-weight="bold">0.1</text>

                            <!-- Dimension indicator -->
                            <text x="350" y="270" text-anchor="middle" font-size="12" fill="#2E7D32" font-weight="bold">This row = embedding for token "cat"!</text>
                        </g>
                    </svg>

                    <h2>Training the Embedding Table</h2>
                    <p>How does the table learn meaningful embeddings?</p>

                    <ol>
                        <li><strong>Initialize randomly</strong>: Start with random numbers</li>
                        <li><strong>Forward pass</strong>: Use embeddings to make predictions</li>
                        <li><strong>Calculate error</strong>: How wrong was the prediction?</li>
                        <li><strong>Backpropagate</strong>: Adjust embeddings to reduce error</li>
                        <li><strong>Repeat billions of times</strong>: Gradually learn meaning!</li>
                    </ol>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> The Embedding Table is the AI's learned dictionary - it instantly converts token IDs to meaningful vectors. It's often the largest single parameter group, storing the model's knowledge of language!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What is an Embedding Table?",
                        options: ["A list of token IDs", "A 2D matrix where each row is a token's embedding", "The model's output layer", "A database of words"],
                        correct: 1,
                        explanation: "An Embedding Table is a 2D matrix (table) where each row corresponds to a token ID, and that row contains the embedding vector (e.g., 768 numbers) for that token."
                    },
                    {
                        question: "Why is embedding lookup so fast?",
                        options: ["It uses GPU acceleration", "It's just array indexing - no computation", "It uses caching", "It compresses the data"],
                        correct: 1,
                        explanation: "Embedding lookup is instant because it's simple array indexing: Embedding_Table[8415] directly accesses row 8415. No calculations needed - just memory access!"
                    },
                    {
                        question: "What percentage of model parameters are often in the embedding table?",
                        options: ["Less than 5%", "About 10-20%", "30-50%", "Over 80%"],
                        correct: 2,
                        explanation: "The embedding table typically contains 30-50% of all model parameters! For GPT-2, the embedding table has ~39M params out of ~124M total (31%). It's huge!"
                    }
                ]
            },

            // ============================================
            // üéØ CHECKPOINT 2: Vectors & Embeddings Complete
            // ============================================
            {
                id: 10.5,
                title: "üéØ Checkpoint 2: Vectors & Embeddings",
                category: 4,
                content: `
                    <div style="background: linear-gradient(135deg, #4CAF50 0%, #2E7D32 100%);
                                color: white;
                                padding: 30px;
                                margin: 0 0 30px 0;
                                border-radius: 16px;
                                box-shadow: 0 8px 32px rgba(76, 175, 80, 0.3);">
                        <h1 style="margin-top: 0; color: white; text-align: center; font-size: 32px;">
                            üéØ Checkpoint 2: Vectors & Embeddings Complete!
                        </h1>
                        <p style="text-align: center; font-size: 18px; margin-bottom: 0; opacity: 0.95;">
                            Halfway there! You've mastered how AI represents meaning with numbers.
                        </p>
                    </div>

                    <h2>üìö What You've Learned (Sections 6-10)</h2>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; margin: 30px 0;">
                        <div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #2196F3;">
                            <h3 style="margin-top: 0; color: #1565C0;">üìä Section 6: Vectors</h3>
                            <p style="margin-bottom: 0;">Numbers that capture meaning! Vectors represent concepts in multi-dimensional space where similar meanings are close together.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #9C27B0;">
                            <h3 style="margin-top: 0; color: #6A1B9A;">üéØ Section 7: Embeddings</h3>
                            <p style="margin-bottom: 0;">Learned vector representations! Embeddings are vectors specifically trained to capture semantic meaning from data.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #FF9800;">
                            <h3 style="margin-top: 0; color: #E65100;">üìê Section 8: Dimensions</h3>
                            <p style="margin-bottom: 0;">Why 4096 numbers? Each dimension captures a different feature of meaning - more dimensions = richer representation!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #4CAF50;">
                            <h3 style="margin-top: 0; color: #2E7D32;">üß† Section 9: Parameters</h3>
                            <p style="margin-bottom: 0;">7 billion learned numbers! Parameters are the AI's "memory" - weights that were adjusted during training on massive datasets.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fce4ec 0%, #f8bbd0 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #E91E63;">
                            <h3 style="margin-top: 0; color: #AD1457;">üìñ Section 10: Embedding Tables</h3>
                            <p style="margin-bottom: 0;">The AI's dictionary! A massive 50K√ó4096 lookup table that instantly converts token IDs to meaningful vectors.</p>
                        </div>
                    </div>

                    <h2>üîÑ The Journey So Far</h2>

                    <div style="background: linear-gradient(135deg, #f5f5f5 0%, #e0e0e0 100%);
                                padding: 25px;
                                border-radius: 12px;
                                margin: 30px 0;
                                border: 2px solid #9E9E9E;">
                        <p style="font-size: 16px; line-height: 1.8; margin: 0;">
                            <strong style="color: #1976D2;">Text input:</strong> "The cat sat"<br>
                            ‚Üì <em style="color: #666;">(Tokenization)</em><br>
                            <strong style="color: #7B1FA2;">Tokens:</strong> ["The", " cat", " sat"]<br>
                            ‚Üì <em style="color: #666;">(Token IDs)</em><br>
                            <strong style="color: #D84315;">Token IDs:</strong> [791, 8415, 7731]<br>
                            ‚Üì <em style="color: #666;">(Embedding Table Lookup)</em><br>
                            <strong style="color: #388E3C;">Embedding Vectors:</strong><br>
                            &nbsp;&nbsp;‚Ä¢ Token 791 ("The") ‚Üí <code>[0.1, 0.3, -0.2, 0.5, ..., 0.2]</code> (4096 numbers)<br>
                            &nbsp;&nbsp;‚Ä¢ Token 8415 (" cat") ‚Üí <code>[0.8, 0.3, -0.5, 0.1, ..., 0.4]</code> (4096 numbers)<br>
                            &nbsp;&nbsp;‚Ä¢ Token 7731 (" sat") ‚Üí <code>[0.2, 0.7, 0.1, -0.3, ..., 0.6]</code> (4096 numbers)
                        </p>
                    </div>

                    <div class="info-box" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 4px solid #2196F3;">
                        <p><strong>üéì Key Insight:</strong> You now understand how AI converts raw text into rich numerical representations that capture meaning! Each word becomes a point in a 4096-dimensional space where semantically similar words are close together.</p>
                    </div>

                    <h2>‚úÖ Self-Check: Can You Explain These?</h2>

                    <div style="background: #fff9c4;
                                padding: 25px;
                                border-radius: 12px;
                                border-left: 5px solid #F57F17;
                                margin: 30px 0;">
                        <p style="margin-top: 0;"><strong>Before moving forward, make sure you can answer:</strong></p>
                        <ul style="line-height: 2; margin-bottom: 0;">
                            <li>‚ùì What's the difference between a vector and an embedding?</li>
                            <li>‚ùì Why do we need 4096 dimensions instead of just 10?</li>
                            <li>‚ùì What are parameters and how do they relate to model size?</li>
                            <li>‚ùì How does the embedding table convert token IDs to vectors?</li>
                            <li>‚ùì Why is embedding lookup instant (no computation)?</li>
                        </ul>
                    </div>

                    <div style="background: linear-gradient(135deg, rgba(76, 175, 80, 0.1) 0%, rgba(46, 125, 50, 0.1) 100%);
                                padding: 20px;
                                border-radius: 10px;
                                border: 2px dashed #4CAF50;
                                margin: 30px 0;">
                        <p style="margin: 0; text-align: center; font-size: 15px;">
                            <strong style="color: #2E7D32;">üí° Quick Answers:</strong><br>
                            <em style="color: #558B2F;">Vector = any list of numbers; Embedding = learned vector | More dimensions = capture more subtle patterns | Parameters = AI's learned knowledge | Embedding table = simple array lookup (Table[8415]) | Lookup is indexing, not calculation!</em>
                        </p>
                    </div>

                    <h2>üîÆ What's Next? (Sections 11-15: Transformer Magic!)</h2>

                    <p>Now that you understand how text becomes meaningful vectors, you're ready for the <strong>most exciting part</strong> - how transformers actually <em>process</em> these vectors!</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0;">
                        <div style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #FF9800;
                                    box-shadow: 0 4px 12px rgba(255, 152, 0, 0.2);">
                            <h3 style="margin-top: 0; color: #E65100;">üéØ Section 11: Self-Attention</h3>
                            <p>The <strong>breakthrough mechanism</strong> that lets AI understand context! Learn how "bank" knows if it's financial or riverside.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e1f5fe 0%, #b3e5fc 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #03A9F4;">
                            <h3 style="margin-top: 0; color: #01579B;">‚öôÔ∏è Section 12: Transformer Layers</h3>
                            <p>Discover how 96 identical layers stack together to create deep understanding - each layer refines meaning!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #9C27B0;">
                            <h3 style="margin-top: 0; color: #6A1B9A;">üîÄ Section 13: Multi-Head Attention</h3>
                            <p>Why use 32 attention heads instead of one? Learn how parallel attention captures different relationships!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #4CAF50;">
                            <h3 style="margin-top: 0; color: #2E7D32;">üîÑ Section 14: Forward Pass</h3>
                            <p>See the complete data flow from input text through all 96 layers to final prediction!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fff9c4 0%, #fff59d 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #FBC02D;">
                            <h3 style="margin-top: 0; color: #F57F17;">üé≤ Section 15: Token Prediction</h3>
                            <p>How does AI choose the next word? Understand probability distributions and sampling strategies!</p>
                        </div>
                    </div>

                    <h2>üìä Your Progress</h2>

                    <div style="margin: 30px 0;">
                        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                            <span style="font-weight: bold; color: #4CAF50; font-size: 18px;">50% Complete!</span>
                            <span style="color: #666; font-size: 14px;">10 of 22 sections (excluding checkpoints)</span>
                        </div>
                        <div style="background: #e0e0e0;
                                    height: 30px;
                                    border-radius: 15px;
                                    overflow: hidden;
                                    box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);">
                            <div style="background: linear-gradient(90deg, #4CAF50 0%, #66BB6A 100%);
                                        width: 50%;
                                        height: 100%;
                                        border-radius: 15px;
                                        display: flex;
                                        align-items: center;
                                        justify-content: center;
                                        color: white;
                                        font-weight: bold;
                                        font-size: 14px;
                                        box-shadow: 0 2px 8px rgba(76, 175, 80, 0.4);
                                        animation: progressGrow 1s ease-out;">
                                50%
                            </div>
                        </div>
                    </div>

                    <style>
                        @keyframes progressGrow {
                            from { width: 0%; }
                            to { width: 50%; }
                        }
                    </style>

                    <div class="success-box" style="background: linear-gradient(135deg, #c8e6c9 0%, #a5d6a7 100%); border-left: 5px solid #2E7D32; margin-top: 30px;">
                        <p style="margin: 0; font-size: 16px;">
                            <strong>üéâ Congratulations!</strong> You're halfway through the guide! You've built a solid foundation in how AI represents language. The next sections reveal the <em>magic</em> of how transformers actually think and reason. Take a break if needed, then dive into the most exciting part! üöÄ
                        </p>
                    </div>
                `,
                quiz: []
            },

            // ============================================
            // PART 5: TRANSFORMER BLOCKS (Sections 11-13)
            // ============================================
            {
                id: 11,
                title: "Self-Attention Mechanism",
                category: 5,
                content: `
                    <div style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
                                backdrop-filter: blur(10px);
                                padding: 20px;
                                margin: 0 0 30px 0;
                                border-radius: 12px;
                                border-left: 4px solid #667eea;">
                        <h3 style="margin-top: 0; color: #667eea;">üîó From Static Embeddings to Dynamic Understanding</h3>
                        <p><strong>Amazing progress!</strong> You've learned how AI represents text:</p>
                        <ul style="margin-left: 20px;">
                            <li>‚úì <strong>Tokenization</strong> ‚Üí Break text into meaningful chunks</li>
                            <li>‚úì <strong>Token IDs</strong> ‚Üí Each token gets a unique number</li>
                            <li>‚úì <strong>Embeddings</strong> ‚Üí IDs lookup rich 4096-dimensional vectors</li>
                            <li>‚úì <strong>Embedding Table</strong> ‚Üí Massive 50K√ó4096 lookup table storing all embeddings</li>
                        </ul>

                        <p><strong>But there's a critical limitation!</strong> These embeddings are <strong>static</strong> - the word "bank" always gets the same vector, whether you mean:</p>
                        <ul style="margin-left: 20px;">
                            <li>üí∞ "I went to the <strong>bank</strong> to deposit money" (financial institution)</li>
                            <li>üèûÔ∏è "The river <strong>bank</strong> was muddy" (riverside)</li>
                        </ul>

                        <p><strong>How does AI distinguish between these meanings?</strong> Enter <strong>self-attention</strong> - the mechanism that makes embeddings <strong>context-aware</strong>!</p>

                        <p><strong>Attention lets each word "look at" surrounding words:</strong> When processing "bank" in "deposit money", attention makes it focus on "deposit" and "money", updating its meaning to be financial. In "river bank", it focuses on "river", updating to mean riverside.</p>

                        <p style="margin: 0;"><strong>This is the breakthrough</strong> that made ChatGPT, GPT-4, and all modern LLMs possible! Let's see how it works... üöÄ</p>
                    </div>

                    <h1>Self-Attention: How AI Sees Connections</h1>

                    <p class="lead">Self-attention is the BREAKTHROUGH that made modern AI possible. It lets the model figure out which words are related to which other words - automatically!</p>

                    <svg class="flow-diagram" width="100%" height="340" viewBox="0 0 850 340">
                        <!-- Input sentence -->
                        <g>
                            <text x="425" y="30" text-anchor="middle" font-size="14" fill="#666" font-weight="bold">Input: "The cat sat on the mat"</text>
                        </g>

                        <!-- Word embeddings -->
                        <g>
                            <rect x="100" y="60" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="140" y="90" text-anchor="middle" font-size="14" fill="#1565C0">The</text>

                            <rect x="220" y="60" width="80" height="50" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                            <text x="260" y="90" text-anchor="middle" font-size="14" fill="#E65100" font-weight="bold">cat</text>

                            <rect x="340" y="60" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="380" y="90" text-anchor="middle" font-size="14" fill="#1565C0">sat</text>

                            <rect x="460" y="60" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="500" y="90" text-anchor="middle" font-size="14" fill="#1565C0">on</text>

                            <rect x="580" y="60" width="80" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="620" y="90" text-anchor="middle" font-size="14" fill="#1565C0">the</text>

                            <rect x="700" y="60" width="80" height="50" rx="8" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="740" y="90" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">mat</text>
                        </g>

                        <!-- Attention mechanism -->
                        <text x="425" y="145" text-anchor="middle" font-size="14" fill="#9C27B0" font-weight="bold">Attention: Which words relate to "cat"?</text>

                        <!-- Attention arrows from cat -->
                        <g>
                            <!-- Weak attention to "The" -->
                            <path d="M 260 110 L 140 130" stroke="#999" stroke-width="1" opacity="0.3" marker-end="url(#arrowA1)"/>
                            <text x="200" y="115" font-size="11" fill="#999">0.1</text>

                            <!-- Strong attention to "sat" -->
                            <path d="M 270 110 L 370 130" stroke="#FF9800" stroke-width="4" opacity="0.8" marker-end="url(#arrowA2)">
                                <animate attributeName="opacity" values="0.5;1;0.5" dur="2s" repeatCount="indefinite"/>
                            </path>
                            <text x="320" y="115" font-size="12" fill="#FF9800" font-weight="bold">0.8</text>

                            <!-- Medium attention to "on" -->
                            <path d="M 280 110 L 490 130" stroke="#2196F3" stroke-width="2" opacity="0.6" marker-end="url(#arrowA3)"/>
                            <text x="385" y="115" font-size="11" fill="#2196F3">0.4</text>

                            <!-- Weak attention to "the" -->
                            <path d="M 290 110 L 610 130" stroke="#999" stroke-width="1" opacity="0.3" marker-end="url(#arrowA1)"/>
                            <text x="450" y="115" font-size="11" fill="#999">0.1</text>

                            <!-- Strong attention to "mat" -->
                            <path d="M 300 110 L 730 130" stroke="#4CAF50" stroke-width="4" opacity="0.8" marker-end="url(#arrowA4)">
                                <animate attributeName="opacity" values="0.5;1;0.5" dur="2s" begin="0.5s" repeatCount="indefinite"/>
                            </path>
                            <text x="515" y="115" font-size="12" fill="#4CAF50" font-weight="bold">0.7</text>
                        </g>

                        <!-- Output embeddings -->
                        <g>
                            <text x="425" y="180" text-anchor="middle" font-size="13" fill="#666">Updated embedding for "cat" (context-aware!)</text>

                            <rect x="200" y="200" width="450" height="100" rx="8" fill="#f3e5f5" stroke="#9C27B0" stroke-width="3"/>
                            <text x="425" y="230" text-anchor="middle" font-size="14" fill="#6A1B9A" font-weight="bold">New "cat" vector combines:</text>
                            <text x="425" y="255" text-anchor="middle" font-size="12" fill="#7B1FA2">‚Ä¢ Original "cat" meaning</text>
                            <text x="425" y="275" text-anchor="middle" font-size="12" fill="#7B1FA2">‚Ä¢ Strong connection to "sat" (action)</text>
                            <text x="425" y="295" text-anchor="middle" font-size="12" fill="#7B1FA2">‚Ä¢ Strong connection to "mat" (location)</text>
                        </g>

                        <defs>
                            <marker id="arrowA1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#999"/>
                            </marker>
                            <marker id="arrowA2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FF9800"/>
                            </marker>
                            <marker id="arrowA3" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#2196F3"/>
                            </marker>
                            <marker id="arrowA4" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>What is Self-Attention?</h2>
                    <p><strong>Self-attention</strong> lets each word look at all other words in the sentence and decide which ones are important for understanding it:</p>

                    <div class="info-box">
                        <p><strong>The Problem It Solves:</strong></p>
                        <p>Without attention, each word's embedding is static - "bank" always has the same vector. But "bank" means different things in different contexts:</p>
                        <ul>
                            <li>"I went to the <strong>bank</strong>" (financial institution)</li>
                            <li>"River <strong>bank</strong>" (side of river)</li>
                        </ul>
                        <p>Attention lets the model adjust each word's representation based on context!</p>
                    </div>

                    <h2>How Self-Attention Works</h2>
                    <p>For each word, the model computes three vectors: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>:</p>

                    <pre><code># For the word "cat" with embedding [0.8, 0.3, -0.5, ...]

Q = Query  = W_Q @ embedding  # "What am I looking for?"
K = Key    = W_K @ embedding  # "What do I offer?"
V = Value  = W_V @ embedding  # "What information do I contain?"

# W_Q, W_K, W_V are learned weight matrices</code></pre>

                    <h2>The Attention Calculation</h2>
                    <p>Three simple steps:</p>

                    <div class="warning-box">
                        <p><strong>Step 1: Calculate Similarity Scores</strong></p>
                        <pre><code># How similar is "cat" to each word?
score_with_sat = Q_cat ‚Ä¢ K_sat  # Dot product
score_with_mat = Q_cat ‚Ä¢ K_mat
# ... for all words</code></pre>

                        <p><strong>Step 2: Normalize with Softmax</strong></p>
                        <pre><code># Convert to probabilities (sum = 1.0)
attention_weights = softmax([scores...])
# Example: [0.1, 0.8, 0.4, 0.1, 0.7]</code></pre>

                        <p><strong>Step 3: Weighted Sum of Values</strong></p>
                        <pre><code># Combine information from all words
output = 0.1*V_The + 0.8*V_sat + 0.4*V_on + 0.1*V_the + 0.7*V_mat
# Result: context-aware embedding for "cat"!</code></pre>
                    </div>

                    <h2>Real Example with Numbers</h2>
                    <pre><code>Input: "cat sat mat"
Embeddings: Each word ‚Üí 4D vector

cat = [1, 0, 0, 0]
sat = [0, 1, 0, 0]
mat = [0, 0, 1, 0]

# For "cat", calculate attention to all words:
Scores:  [cat¬∑cat, cat¬∑sat, cat¬∑mat] = [1.0, 0.3, 0.8]
Softmax: [0.35, 0.15, 0.50]  # Normalized weights

# New context-aware "cat" embedding:
output = 0.35*[1,0,0,0] + 0.15*[0,1,0,0] + 0.50*[0,0,1,0]
       = [0.35, 0.15, 0.50, 0]

# Now "cat" embedding includes context from "sat" and "mat"!</code></pre>

                    <h2>Why It's Called "Self"-Attention</h2>
                    <div class="success-box">
                        <p><strong>"Self" means the sentence attends to itself!</strong></p>
                        <ul>
                            <li>Each word looks at <strong>every other word</strong> in the SAME sentence</li>
                            <li>No external information needed</li>
                            <li>The model learns which words relate to which automatically</li>
                        </ul>
                    </div>

                    <h2>Attention Weights Visualization</h2>
                    <p>Researchers often visualize attention as a heatmap:</p>

                    <svg width="100%" height="280" viewBox="0 0 600 280">
                        <!-- Heatmap grid -->
                        <text x="300" y="25" text-anchor="middle" font-size="14" fill="#666" font-weight="bold">Attention Heatmap: What "cat" attends to</text>

                        <!-- Column headers -->
                        <text x="180" y="65" text-anchor="middle" font-size="12" fill="#666">The</text>
                        <text x="250" y="65" text-anchor="middle" font-size="12" fill="#666">cat</text>
                        <text x="320" y="65" text-anchor="middle" font-size="12" fill="#666">sat</text>
                        <text x="390" y="65" text-anchor="middle" font-size="12" fill="#666">on</text>
                        <text x="460" y="65" text-anchor="middle" font-size="12" fill="#666">the</text>
                        <text x="530" y="65" text-anchor="middle" font-size="12" fill="#666">mat</text>

                        <!-- Row: cat -->
                        <text x="110" y="105" text-anchor="end" font-size="12" fill="#666" font-weight="bold">cat ‚Üí</text>

                        <rect x="150" y="80" width="60" height="40" fill="#e0e0e0"/>
                        <text x="180" y="105" text-anchor="middle" font-size="11" fill="#333">0.1</text>

                        <rect x="220" y="80" width="60" height="40" fill="#FFEB3B"/>
                        <text x="250" y="105" text-anchor="middle" font-size="11" fill="#333" font-weight="bold">0.5</text>

                        <rect x="290" y="80" width="60" height="40" fill="#FF9800">
                            <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" repeatCount="indefinite"/>
                        </rect>
                        <text x="320" y="105" text-anchor="middle" font-size="11" fill="#fff" font-weight="bold">0.8</text>

                        <rect x="360" y="80" width="60" height="40" fill="#FFC107"/>
                        <text x="390" y="105" text-anchor="middle" font-size="11" fill="#333">0.4</text>

                        <rect x="430" y="80" width="60" height="40" fill="#e0e0e0"/>
                        <text x="460" y="105" text-anchor="middle" font-size="11" fill="#333">0.1</text>

                        <rect x="500" y="80" width="60" height="40" fill="#4CAF50">
                            <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" begin="0.5s" repeatCount="indefinite"/>
                        </rect>
                        <text x="530" y="105" text-anchor="middle" font-size="11" fill="#fff" font-weight="bold">0.7</text>

                        <!-- Legend -->
                        <text x="300" y="155" text-anchor="middle" font-size="12" fill="#666">Attention Strength</text>
                        <rect x="150" y="165" width="80" height="20" fill="#e0e0e0"/>
                        <text x="190" y="180" text-anchor="middle" font-size="10" fill="#333">Low (0.1)</text>

                        <rect x="260" y="165" width="80" height="20" fill="#FF9800"/>
                        <text x="300" y="180" text-anchor="middle" font-size="10" fill="#fff">High (0.8)</text>

                        <text x="300" y="215" text-anchor="middle" font-size="12" fill="#9C27B0" font-weight="bold">Interpretation:</text>
                        <text x="300" y="235" text-anchor="middle" font-size="11" fill="#666">"cat" pays most attention to "sat" (the action)</text>
                        <text x="300" y="255" text-anchor="middle" font-size="11" fill="#666">and "mat" (the location where sitting happens)</text>
                    </svg>

                    <h2>Why Self-Attention is Powerful</h2>
                    <ul>
                        <li><strong>Parallel processing</strong>: All words computed simultaneously (unlike RNNs)</li>
                        <li><strong>Long-range dependencies</strong>: Word 1 can directly attend to word 1000</li>
                        <li><strong>Learned relationships</strong>: Model learns which words relate without rules</li>
                        <li><strong>Context-aware</strong>: Same word gets different representations in different contexts</li>
                    </ul>

                    <h2>Computational Cost</h2>
                    <div class="warning-box">
                        <p><strong>Trade-off:</strong> Self-attention is powerful but expensive!</p>
                        <pre><code># For sequence of length N:
Memory:  O(N¬≤)  # Store all attention scores
Time:    O(N¬≤)  # Compare every word to every other word

Example:
100 words   ‚Üí 10,000 comparisons
1,000 words ‚Üí 1,000,000 comparisons!

This is why context windows are limited!</code></pre>
                    </div>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Self-attention lets each word look at all other words and figure out which ones matter. It creates context-aware embeddings that change based on surrounding words - this is the magic that makes modern AI work!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What problem does self-attention solve?",
                        options: ["Making models faster", "Creating context-aware word representations", "Reducing model size", "Improving tokenization"],
                        correct: 1,
                        explanation: "Self-attention allows each word to look at all other words in the sentence and adjust its representation based on context. This makes 'bank' have different meanings in 'river bank' vs 'financial bank'."
                    },
                    {
                        question: "What are the three vectors computed in self-attention?",
                        options: ["Input, Output, Hidden", "Query, Key, Value", "Embedding, Position, Context", "Word, Token, Vector"],
                        correct: 1,
                        explanation: "Self-attention computes three vectors for each word: Query (what am I looking for?), Key (what do I offer?), and Value (what information do I contain?). These are combined to create context-aware representations."
                    },
                    {
                        question: "Why is self-attention computationally expensive?",
                        options: ["It uses too many parameters", "It requires O(N¬≤) comparisons for N words", "It needs special hardware", "It processes words sequentially"],
                        correct: 1,
                        explanation: "Self-attention has O(N¬≤) complexity because every word must attend to every other word. For 1000 words, that's 1 million comparisons! This is why context windows are limited."
                    }
                ]
            },
            {
                id: 12,
                title: "Transformer Layers",
                category: 5,
                content: `
                    <h1>Transformer Layers: Putting It All Together</h1>

                    <p class="lead">A transformer layer combines self-attention with other components to create a powerful processing block. GPT-4 has 120 of these stacked together!</p>

                    <svg class="flow-diagram" width="100%" height="400" viewBox="0 0 700 400">
                        <!-- Input -->
                        <g>
                            <rect x="250" y="20" width="200" height="40" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                            <text x="350" y="45" text-anchor="middle" font-size="14" fill="#1565C0" font-weight="bold">Input Embeddings</text>
                        </g>

                        <!-- Arrow -->
                        <path d="M 350 60 L 350 90" stroke="#666" stroke-width="2" marker-end="url(#arrowTL1)"/>

                        <!-- Layer Norm 1 -->
                        <g>
                            <rect x="250" y="95" width="200" height="35" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                            <text x="350" y="118" text-anchor="middle" font-size="13" fill="#E65100">Layer Normalization</text>
                        </g>

                        <path d="M 350 130 L 350 155" stroke="#666" stroke-width="2" marker-end="url(#arrowTL1)"/>

                        <!-- Self-Attention -->
                        <g>
                            <rect x="230" y="160" width="240" height="50" rx="8" fill="#f3e5f5" stroke="#9C27B0" stroke-width="3">
                                <animate attributeName="opacity" values="0.8;1;0.8" dur="2s" repeatCount="indefinite"/>
                            </rect>
                            <text x="350" y="185" text-anchor="middle" font-size="14" fill="#6A1B9A" font-weight="bold">Multi-Head Self-Attention</text>
                            <text x="350" y="202" text-anchor="middle" font-size="11" fill="#7B1FA2">Q, K, V transformations</text>
                        </g>

                        <!-- Add & Norm (Residual Connection) -->
                        <path d="M 470 187 Q 550 187 550 240" stroke="#4CAF50" stroke-width="3" stroke-dasharray="5,5" marker-end="url(#arrowTL2)"/>
                        <text x="520" y="210" font-size="11" fill="#4CAF50" font-weight="bold">Skip Connection</text>

                        <path d="M 350 210 L 350 235" stroke="#666" stroke-width="2" marker-end="url(#arrowTL1)"/>

                        <!-- Add box -->
                        <circle cx="350" cy="245" r="20" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2"/>
                        <text x="350" y="252" text-anchor="middle" font-size="18" fill="#2E7D32" font-weight="bold">+</text>

                        <path d="M 350 265 L 350 290" stroke="#666" stroke-width="2" marker-end="url(#arrowTL1)"/>

                        <!-- Layer Norm 2 -->
                        <g>
                            <rect x="250" y="295" width="200" height="35" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                            <text x="350" y="318" text-anchor="middle" font-size="13" fill="#E65100">Layer Normalization</text>
                        </g>

                        <path d="M 350 330 L 350 355" stroke="#666" stroke-width="2" marker-end="url(#arrowTL1)"/>

                        <!-- Feed-Forward Network -->
                        <g>
                            <rect x="200" y="360" width="300" height="50" rx="8" fill="#e1f5fe" stroke="#03A9F4" stroke-width="3">
                                <animate attributeName="opacity" values="0.8;1;0.8" dur="2s" begin="0.5s" repeatCount="indefinite"/>
                            </rect>
                            <text x="350" y="382" text-anchor="middle" font-size="14" fill="#0277BD" font-weight="bold">Feed-Forward Network (FFN)</text>
                            <text x="350" y="400" text-anchor="middle" font-size="11" fill="#0288D1">Linear ‚Üí ReLU ‚Üí Linear</text>
                        </g>

                        <!-- Second skip connection -->
                        <path d="M 500 387 Q 580 387 580 450" stroke="#4CAF50" stroke-width="3" stroke-dasharray="5,5"/>

                        <path d="M 350 410 L 350 435" stroke="#666" stroke-width="2"/>

                        <!-- Second Add box -->
                        <circle cx="350" cy="445" r="20" fill="#c8e6c9" stroke="#4CAF50" stroke-width="2"/>
                        <text x="350" y="452" text-anchor="middle" font-size="18" fill="#2E7D32" font-weight="bold">+</text>

                        <path d="M 350 465 L 350 485" stroke="#666" stroke-width="2" marker-end="url(#arrowTL1)"/>

                        <!-- Output -->
                        <g>
                            <rect x="250" y="490" width="200" height="40" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="350" y="515" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">Output ‚Üí Next Layer</text>
                        </g>

                        <defs>
                            <marker id="arrowTL1" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
                            </marker>
                            <marker id="arrowTL2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4CAF50"/>
                            </marker>
                        </defs>
                    </svg>

                    <h2>Components of a Transformer Layer</h2>
                    <p>Every transformer layer has the same structure:</p>

                    <div class="info-box">
                        <p><strong>The Recipe:</strong></p>
                        <ol>
                            <li><strong>Layer Normalization</strong>: Stabilize values</li>
                            <li><strong>Multi-Head Self-Attention</strong>: Let words talk to each other</li>
                            <li><strong>Residual Connection (Add & Norm)</strong>: Preserve original information</li>
                            <li><strong>Feed-Forward Network</strong>: Process each position independently</li>
                            <li><strong>Another Residual Connection</strong>: Preserve info again</li>
                        </ol>
                    </div>

                    <h2>Layer Normalization</h2>
                    <p>Normalizes values to have mean=0, std=1. Prevents values from exploding or vanishing:</p>

                    <pre><code># Before: values all over the place
x = [100, 0.01, -50, 200]

# After normalization:
x_norm = (x - mean(x)) / std(x)
# Result: [-0.3, -0.8, -1.2, 2.3]  # More stable!</code></pre>

                    <h2>Residual Connections (Skip Connections)</h2>
                    <p>The <strong>add</strong> operation bypasses the layer - this is crucial for deep networks:</p>

                    <div class="warning-box">
                        <p><strong>Why Skip Connections Matter:</strong></p>
                        <pre><code>output = input + SelfAttention(LayerNorm(input))

# Without skip connection:
# Information gets lost after 120 layers!

# With skip connection:
# Original information flows through +
# Attention adds new context
# = Best of both worlds!</code></pre>
                    </div>

                    <h2>Feed-Forward Network (FFN)</h2>
                    <p>Two linear layers with ReLU activation in between:</p>

                    <pre><code># For each position independently:
FFN(x) = Linear2(ReLU(Linear1(x)))

# Example dimensions (GPT-2):
Input:  768 dims
Linear1: 768 ‚Üí 3072 (expand 4x!)
ReLU:   Keep positive, zero negative
Linear2: 3072 ‚Üí 768 (compress back)
Output: 768 dims

# This gives the model extra "thinking space"</code></pre>

                    <h2>Complete Transformer Layer Formula</h2>
                    <pre><code># Pseudocode for one transformer layer:

def TransformerLayer(x):
    # Sub-layer 1: Self-Attention with residual
    x_norm = LayerNorm(x)
    attn_out = MultiHeadAttention(x_norm, x_norm, x_norm)
    x = x + attn_out  # Residual connection

    # Sub-layer 2: FFN with residual
    x_norm = LayerNorm(x)
    ffn_out = FeedForward(x_norm)
    x = x + ffn_out   # Residual connection

    return x

# Stack 12, 24, 96, or even 120 of these!</code></pre>

                    <h2>Stacking Layers</h2>
                    <p>Models stack many identical transformer layers:</p>

                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Model</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">Layers</th>
                            <th style="padding: 12px; text-align: right; border: 1px solid #ddd;">Hidden Size</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-2 Small</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">12</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">768</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-2 Large</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">36</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">1280</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px; border: 1px solid #ddd;">GPT-3</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">96</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace;">12,288</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px; border: 1px solid #ddd; font-weight: bold;">GPT-4</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace; font-weight: bold;">~120</td>
                            <td style="padding: 10px; text-align: right; border: 1px solid #ddd; font-family: monospace; font-weight: bold;">12,288</td>
                        </tr>
                    </table>

                    <h2>Why This Architecture Works</h2>
                    <ul>
                        <li><strong>Self-Attention</strong>: Captures relationships between words</li>
                        <li><strong>FFN</strong>: Processes individual positions with extra capacity</li>
                        <li><strong>Layer Norm</strong>: Keeps values stable</li>
                        <li><strong>Residual Connections</strong>: Preserves information across 100+ layers</li>
                        <li><strong>Stacking</strong>: Each layer extracts progressively abstract features</li>
                    </ul>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> A transformer layer = Layer Norm + Self-Attention + Residual + Layer Norm + FFN + Residual. Stack 12-120 of these, and you get a powerful language model that can understand and generate human-like text!</p>
                    </div>
                `,
                quiz: [
                    {
                        question: "What are the two main sub-components in a transformer layer?",
                        options: ["Embedding and Output", "Self-Attention and Feed-Forward Network", "Input and Hidden State", "Query and Key"],
                        correct: 1,
                        explanation: "Each transformer layer has two main parts: Multi-Head Self-Attention (which lets words interact) and a Feed-Forward Network (which processes each position independently). Both have residual connections and layer normalization."
                    },
                    {
                        question: "Why are residual connections (skip connections) important?",
                        options: ["They make models faster", "They preserve information across deep networks", "They reduce memory usage", "They improve tokenization"],
                        correct: 1,
                        explanation: "Residual connections add the original input to the layer's output (x = x + layer(x)). This preserves information across 100+ layers and prevents vanishing gradients. Without them, deep networks wouldn't work!"
                    },
                    {
                        question: "How many transformer layers does GPT-4 have?",
                        options: ["12 layers", "36 layers", "96 layers", "~120 layers"],
                        correct: 3,
                        explanation: "GPT-4 has approximately 120 transformer layers stacked together! Each layer processes the output from the previous layer, allowing the model to extract increasingly abstract patterns."
                    }
                ]
            },
            {
                id: 13,
                title: "Multi-Head Attention",
                category: 5,
                content: `
                    <h1>Multi-Head Attention: Parallel Processing Power</h1>
                    <p class="lead">Instead of one attention mechanism, use many in parallel! Each "head" learns different patterns - grammar, facts, relationships, etc.</p>

                    <svg class="flow-diagram" width="100%" height="300" viewBox="0 0 800 300">
                        <rect x="300" y="30" width="200" height="50" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="400" y="60" text-anchor="middle" font-size="14" fill="#1565C0" font-weight="bold">Input Embedding</text>

                        <!-- 8 parallel heads -->
                        <g>
                            <rect x="100" y="120" width="80" height="60" rx="5" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2">
                                <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" repeatCount="indefinite"/>
                            </rect>
                            <text x="140" y="155" text-anchor="middle" font-size="12" fill="#6A1B9A">Head 1</text>

                            <rect x="200" y="120" width="80" height="60" rx="5" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2">
                                <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" begin="0.25s" repeatCount="indefinite"/>
                            </rect>
                            <text x="240" y="155" text-anchor="middle" font-size="12" fill="#6A1B9A">Head 2</text>

                            <rect x="300" y="120" width="80" height="60" rx="5" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2">
                                <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" begin="0.5s" repeatCount="indefinite"/>
                            </rect>
                            <text x="340" y="155" text-anchor="middle" font-size="12" fill="#6A1B9A">Head 3</text>

                            <rect x="520" y="120" width="80" height="60" rx="5" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2">
                                <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" begin="0.75s" repeatCount="indefinite"/>
                            </rect>
                            <text x="560" y="155" text-anchor="middle" font-size="12" fill="#6A1B9A">Head 8</text>

                            <text x="450" y="155" text-anchor="middle" font-size="20" fill="#999">...</text>
                        </g>

                        <rect x="300" y="220" width="200" height="50" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
                        <text x="400" y="250" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">Concatenate & Project</text>
                    </svg>

                    <h2>Why Multiple Heads?</h2>
                    <div class="info-box">
                        <p><strong>Each head learns different patterns:</strong></p>
                        <ul>
                            <li><strong>Head 1</strong>: Subject-verb relationships</li>
                            <li><strong>Head 2</strong>: Adjective-noun pairs</li>
                            <li><strong>Head 3</strong>: Long-distance dependencies</li>
                            <li><strong>Head 4-8</strong>: Other linguistic patterns</li>
                        </ul>
                    </div>

                    <pre><code># GPT-2 Configuration:
Hidden size: 768
Number of heads: 12
Head dimension: 768 / 12 = 64 dimensions per head

# Each head operates on 64-dim space
# All heads process in parallel
# Results concatenated: 12 √ó 64 = 768 dims back!</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Multi-head attention runs 8-16 attention mechanisms in parallel, each learning different patterns. This dramatically increases the model's ability to understand complex language!</p>
                    </div>
                `,
                quiz: [
                    {question: "Why use multiple attention heads?", options: ["Faster processing", "Each head learns different patterns", "Reduces memory", "Simpler architecture"], correct: 1, explanation: "Multiple heads let the model attend to different types of patterns simultaneously - one head might focus on grammar, another on semantics, another on long-range dependencies."},
                    {question: "How many attention heads does GPT-2 have?", options: ["4 heads", "8 heads", "12 heads", "16 heads"], correct: 2, explanation: "GPT-2 uses 12 attention heads. Each head operates on 64 dimensions (768 total / 12 heads = 64 per head)."},
                    {question: "If a model has 768 hidden dimensions and 12 heads, what's each head's dimension?", options: ["32 dims", "64 dims", "128 dims", "768 dims"], correct: 1, explanation: "768 / 12 = 64 dimensions per head. Each head processes a smaller subspace, then results are concatenated back to 768 dimensions."}
                ]
            },
            {
                id: 14,
                title: "Forward Pass - End to End",
                category: 6,
                content: `
                    <h1>Forward Pass: The Complete Journey</h1>
                    <p class="lead">Let's follow "The cat" through an entire model, from text input to predicted next word!</p>

                    <svg class="flow-diagram" width="100%" height="500" viewBox="0 0 600 500">
                        <rect x="200" y="20" width="200" height="40" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="300" y="45" text-anchor="middle" font-size="14" fill="#1565C0">"The cat"</text>

                        <path d="M 300 60 L 300 85" stroke="#666" stroke-width="2"/>

                        <rect x="200" y="90" width="200" height="35" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                        <text x="300" y="112" text-anchor="middle" font-size="13" fill="#E65100">Tokenize ‚Üí [1234, 5678]</text>

                        <path d="M 300 125 L 300 150" stroke="#666" stroke-width="2"/>

                        <rect x="180" y="155" width="240" height="35" rx="8" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2"/>
                        <text x="300" y="177" text-anchor="middle" font-size="13" fill="#6A1B9A">Embedding Table Lookup</text>

                        <path d="M 300 190 L 300 215" stroke="#666" stroke-width="2"/>

                        <rect x="150" y="220" width="300" height="120" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="3">
                            <animate attributeName="opacity" values="0.8;1;0.8" dur="2s" repeatCount="indefinite"/>
                        </rect>
                        <text x="300" y="245" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">12 Transformer Layers</text>
                        <text x="300" y="265" text-anchor="middle" font-size="12" fill="#388E3C">Layer 1: Self-Attention + FFN</text>
                        <text x="300" y="285" text-anchor="middle" font-size="12" fill="#388E3C">Layer 2: Self-Attention + FFN</text>
                        <text x="300" y="305" text-anchor="middle" font-size="12" fill="#388E3C">...</text>
                        <text x="300" y="325" text-anchor="middle" font-size="12" fill="#388E3C">Layer 12: Self-Attention + FFN</text>

                        <path d="M 300 340 L 300 365" stroke="#666" stroke-width="2"/>

                        <rect x="200" y="370" width="200" height="35" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                        <text x="300" y="392" text-anchor="middle" font-size="13" fill="#E65100">Final Layer Norm</text>

                        <path d="M 300 405 L 300 430" stroke="#666" stroke-width="2"/>

                        <rect x="150" y="435" width="300" height="50" rx="8" fill="#e1f5fe" stroke="#03A9F4" stroke-width="2"/>
                        <text x="300" y="455" text-anchor="middle" font-size="13" fill="#0277BD">Output Projection</text>
                        <text x="300" y="475" text-anchor="middle" font-size="12" fill="#0288D1">50,000 logits (one per token)</text>
                    </svg>

                    <h2>Step-by-Step Breakdown</h2>
                    <pre><code>Input: "The cat"

1. Tokenization: [1234, 5678]

2. Embedding Lookup:
   Token 1234 ‚Üí [0.8, 0.3, ..., 0.1]  (768 dims)
   Token 5678 ‚Üí [0.2, 0.9, ..., 0.5]  (768 dims)

3. Add Position Embeddings:
   Position 0 + Token embedding
   Position 1 + Token embedding

4. Process through 12 Transformer Layers:
   Layer 1 output ‚Üí Layer 2 input ‚Üí ... ‚Üí Layer 12 output

5. Final vector for last token: [0.4, 0.7, ..., 0.2]

6. Project to vocabulary size:
   768 dims ‚Üí 50,000 logits

7. Get probabilities:
   softmax([logit_0, logit_1, ..., logit_49999])

Result: Probability for each possible next word!</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> The forward pass transforms text ‚Üí tokens ‚Üí embeddings ‚Üí 12 layers of processing ‚Üí output probabilities. This happens for EVERY word the model generates!</p>
                    </div>
                `,
                quiz: [
                    {question: "What happens first in the forward pass?", options: ["Embedding lookup", "Tokenization", "Self-attention", "Output projection"], correct: 1, explanation: "Tokenization is always first! Text must be split into tokens before we can look up embeddings or process through transformer layers."},
                    {question: "How many values does the output layer produce for GPT-2?", options: ["768 values", "12,000 values", "50,257 values", "1 value"], correct: 2, explanation: "The output layer produces one logit for EVERY token in the vocabulary. GPT-2 has 50,257 tokens, so it outputs 50,257 values (one probability per possible next word)."},
                    {question: "What is a forward pass?", options: ["Training the model", "One complete journey from input to output", "Updating parameters", "Calculating loss"], correct: 1, explanation: "A forward pass is the complete journey from input text through all model layers to the final output. During inference, this is how we generate predictions. During training, we also do a backward pass to update weights."}
                ]
            },
            {
                id: 15,
                title: "Next Token Prediction",
                category: 6,
                content: `
                    <h1>Next Token Prediction: How AI Generates Text</h1>
                    <p class="lead">Models predict one word at a time. Understanding sampling strategies (temperature, top-k, top-p) is key to controlling AI output!</p>

                    <svg class="flow-diagram" width="100%" height="300" viewBox="0 0 700 300">
                        <rect x="200" y="20" width="300" height="40" rx="8" fill="#e3f2fd" stroke="#2196F3" stroke-width="2"/>
                        <text x="350" y="45" text-anchor="middle" font-size="14" fill="#1565C0">Logits from model</text>
                        <text x="350" y="53" text-anchor="middle" font-size="10" fill="#666">[2.1, 3.5, -1.2, 4.8, ...]</text>

                        <path d="M 350 60 L 350 90" stroke="#666" stroke-width="2"/>

                        <rect x="250" y="95" width="200" height="40" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                        <text x="350" y="120" text-anchor="middle" font-size="13" fill="#E65100">Softmax ‚Üí Probabilities</text>

                        <path d="M 350 135 L 350 165" stroke="#666" stroke-width="2"/>

                        <g>
                            <rect x="50" y="170" width="150" height="100" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="125" y="195" text-anchor="middle" font-size="13" fill="#2E7D32" font-weight="bold">Top Predictions</text>
                            <text x="125" y="215" text-anchor="middle" font-size="11" fill="#388E3C">"sat": 45%</text>
                            <text x="125" y="235" text-anchor="middle" font-size="11" fill="#388E3C">"jumped": 23%</text>
                            <text x="125" y="255" text-anchor="middle" font-size="11" fill="#388E3C">"ran": 15%</text>
                        </g>

                        <g>
                            <rect x="250" y="170" width="200" height="100" rx="8" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2">
                                <animate attributeName="opacity" values="0.8;1;0.8" dur="2s" repeatCount="indefinite"/>
                            </rect>
                            <text x="350" y="195" text-anchor="middle" font-size="13" fill="#6A1B9A" font-weight="bold">Sampling Strategy</text>
                            <text x="350" y="215" text-anchor="middle" font-size="11" fill="#7B1FA2">Temperature = 0.7</text>
                            <text x="350" y="235" text-anchor="middle" font-size="11" fill="#7B1FA2">Top-k = 50</text>
                            <text x="350" y="255" text-anchor="middle" font-size="11" fill="#7B1FA2">Top-p = 0.9</text>
                        </g>

                        <g>
                            <rect x="500" y="170" width="150" height="100" rx="8" fill="#e1f5fe" stroke="#03A9F4" stroke-width="2"/>
                            <text x="575" y="195" text-anchor="middle" font-size="13" fill="#0277BD" font-weight="bold">Selected Token</text>
                            <text x="575" y="230" text-anchor="middle" font-size="20" fill="#0288D1" font-weight="bold">"sat"</text>
                        </g>
                    </svg>

                    <h2>From Logits to Text</h2>
                    <pre><code># Model output (logits):
the:     2.1
cat:     1.8
sat:     4.8  ‚Üê Highest!
jumped:  3.2
...

# Apply softmax to get probabilities:
the:     8%
cat:     5%
sat:     45%  ‚Üê Highest probability
jumped:  23%
...</code></pre>

                    <h2>Sampling Strategies</h2>
                    <div class="warning-box">
                        <p><strong>Temperature (0.0 - 2.0)</strong></p>
                        <ul>
                            <li><strong>Low (0.1)</strong>: Deterministic, always picks most likely ‚Üí boring but safe</li>
                            <li><strong>Medium (0.7)</strong>: Balanced creativity</li>
                            <li><strong>High (1.5)</strong>: Very random ‚Üí creative but risky</li>
                        </ul>
                        <pre><code># Temperature = 0.1 (deterministic):
"The cat sat"  ‚Üí always predicts "sat"

# Temperature = 1.5 (creative):
"The cat sat" OR "jumped" OR "meowed" ‚Üí varied!</code></pre>
                    </div>

                    <h2>Top-k and Top-p Sampling</h2>
                    <div class="info-box">
                        <p><strong>Top-k</strong>: Only consider top k most likely tokens</p>
                        <p><strong>Top-p</strong>: Consider tokens until cumulative probability reaches p</p>
                        <pre><code># Top-k = 3:
Consider only: sat (45%), jumped (23%), ran (15%)
Ignore everything else

# Top-p = 0.9 (90%):
sat: 45%
jumped: 23%
ran: 15%
walked: 10%  ‚Üí Total = 93%, stop here!
Ignore all tokens after 90% threshold</code></pre>
                    </div>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Models predict probabilities for every possible next word. Temperature controls randomness, top-k/top-p filter unlikely options. This is how you control AI creativity vs accuracy!</p>
                    </div>
                `,
                quiz: [
                    {question: "What does temperature control?", options: ["Model size", "Randomness of predictions", "Training speed", "Context window"], correct: 1, explanation: "Temperature controls randomness: Low temperature (0.1) makes output deterministic and boring. High temperature (1.5) makes it creative but risky. Medium (0.7) balances both."},
                    {question: "What is greedy decoding?", options: ["Always picking the highest probability token", "Random sampling", "Using temperature=1.0", "Training faster"], correct: 0, explanation: "Greedy decoding always picks the most likely token (temperature=0). It's deterministic but can be repetitive. Most chat models use temperature ~0.7 for better variety."},
                    {question: "If top-p = 0.9, which tokens are considered?", options: ["Only the top token", "Top 90 tokens", "Tokens until cumulative probability reaches 90%", "90% of vocabulary"], correct: 2, explanation: "Top-p (nucleus sampling) considers tokens until their cumulative probability reaches the threshold (e.g., 90%). This dynamically adjusts how many tokens are considered based on the probability distribution."}
                ]
            },

            // ============================================
            // üéØ CHECKPOINT 3: Complete Pipeline Understood
            // ============================================
            {
                id: 15.5,
                title: "üéØ Checkpoint 3: Complete Pipeline",
                category: 6,
                content: `
                    <div style="background: linear-gradient(135deg, #4CAF50 0%, #2E7D32 100%);
                                color: white;
                                padding: 30px;
                                margin: 0 0 30px 0;
                                border-radius: 16px;
                                box-shadow: 0 8px 32px rgba(76, 175, 80, 0.3);">
                        <h1 style="margin-top: 0; color: white; text-align: center; font-size: 32px;">
                            üéØ Checkpoint 3: You Understand How AI Works!
                        </h1>
                        <p style="text-align: center; font-size: 18px; margin-bottom: 0; opacity: 0.95;">
                            Amazing! You've conquered the core transformer architecture and complete pipeline.
                        </p>
                    </div>

                    <h2>üìö What You've Learned (Sections 11-15)</h2>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; margin: 30px 0;">
                        <div style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #FF9800;">
                            <h3 style="margin-top: 0; color: #E65100;">üéØ Section 11: Self-Attention</h3>
                            <p style="margin-bottom: 0;">The breakthrough! Self-attention lets AI understand context - how "bank" means different things based on surrounding words.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e1f5fe 0%, #b3e5fc 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #03A9F4;">
                            <h3 style="margin-top: 0; color: #01579B;">‚öôÔ∏è Section 12: Transformer Layers</h3>
                            <p style="margin-bottom: 0;">96 identical layers stacked together! Each layer refines understanding through attention + feed-forward networks.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #9C27B0;">
                            <h3 style="margin-top: 0; color: #6A1B9A;">üîÄ Section 13: Multi-Head Attention</h3>
                            <p style="margin-bottom: 0;">32 attention heads in parallel! Each head specializes in different relationships (syntax, semantics, position).</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #4CAF50;">
                            <h3 style="margin-top: 0; color: #2E7D32;">üîÑ Section 14: Forward Pass</h3>
                            <p style="margin-bottom: 0;">The complete journey! Data flows from tokenization ‚Üí embeddings ‚Üí 96 transformer layers ‚Üí output probabilities.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fff9c4 0%, #fff59d 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border-left: 4px solid #FBC02D;">
                            <h3 style="margin-top: 0; color: #F57F17;">üé≤ Section 15: Token Prediction</h3>
                            <p style="margin-bottom: 0;">How AI generates text! Temperature, top-k, and top-p control the balance between creativity and accuracy.</p>
                        </div>
                    </div>

                    <h2>üîÑ Complete Pipeline: "The cat sat" ‚Üí "on"</h2>

                    <div style="background: linear-gradient(135deg, #f5f5f5 0%, #e0e0e0 100%);
                                padding: 25px;
                                border-radius: 12px;
                                margin: 30px 0;
                                border: 2px solid #9E9E9E;">
                        <p style="font-size: 16px; line-height: 2; margin: 0;">
                            <strong style="color: #1976D2;">Input:</strong> "The cat sat"<br>
                            ‚Üì <em style="color: #666;">(Tokenization)</em><br>
                            <strong style="color: #7B1FA2;">Tokens:</strong> ["The", " cat", " sat"]<br>
                            ‚Üì <em style="color: #666;">(Token IDs)</em><br>
                            <strong style="color: #D84315;">IDs:</strong> [791, 8415, 7731]<br>
                            ‚Üì <em style="color: #666;">(Embedding Lookup)</em><br>
                            <strong style="color: #388E3C;">Vectors:</strong> [0.1, 0.3, ..., 0.2], [0.8, 0.3, ..., 0.4], [0.2, 0.7, ..., 0.6]<br>
                            ‚Üì <em style="color: #666;">(Layer 1 Self-Attention)</em><br>
                            <strong style="color: #FF9800;">Context-aware vectors:</strong> Each word now knows about surrounding words!<br>
                            ‚Üì <em style="color: #666;">(Layers 2-96 refine understanding)</em><br>
                            <strong style="color: #5E35B1;">Deep representations:</strong> Rich, nuanced meaning captured<br>
                            ‚Üì <em style="color: #666;">(Output Layer)</em><br>
                            <strong style="color: #C62828;">Logits:</strong> [2.1, 3.5, -1.2, 4.8, ...] (50,257 values)<br>
                            ‚Üì <em style="color: #666;">(Softmax)</em><br>
                            <strong style="color: #0277BD;">Probabilities:</strong> "on" (45%), "down" (23%), "still" (15%)<br>
                            ‚Üì <em style="color: #666;">(Sampling)</em><br>
                            <strong style="color: #2E7D32; font-size: 18px;">‚ú® Output:</strong> "on" ‚Üí Complete sentence: "The cat sat on"
                        </p>
                    </div>

                    <div class="info-box" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 4px solid #2196F3;">
                        <p><strong>üéì Incredible Achievement:</strong> You now understand the complete architecture of ChatGPT, GPT-4, Llama, and all modern LLMs! From raw text to AI predictions, you can explain every single step.</p>
                    </div>

                    <h2>‚úÖ Self-Check: Can You Explain These?</h2>

                    <div style="background: #fff9c4;
                                padding: 25px;
                                border-radius: 12px;
                                border-left: 5px solid #F57F17;
                                margin: 30px 0;">
                        <p style="margin-top: 0;"><strong>Before moving forward, make sure you can answer:</strong></p>
                        <ul style="line-height: 2; margin-bottom: 0;">
                            <li>‚ùì How does self-attention make embeddings context-aware?</li>
                            <li>‚ùì Why do we need 96 layers instead of just 1?</li>
                            <li>‚ùì What's the difference between single-head and multi-head attention?</li>
                            <li>‚ùì How does text flow through the forward pass from input to output?</li>
                            <li>‚ùì What's the difference between temperature, top-k, and top-p?</li>
                        </ul>
                    </div>

                    <div style="background: linear-gradient(135deg, rgba(76, 175, 80, 0.1) 0%, rgba(46, 125, 50, 0.1) 100%);
                                padding: 20px;
                                border-radius: 10px;
                                border: 2px dashed #4CAF50;
                                margin: 30px 0;">
                        <p style="margin: 0; text-align: center; font-size: 15px;">
                            <strong style="color: #2E7D32;">üí° Quick Answers:</strong><br>
                            <em style="color: #558B2F;">Attention compares words using Query/Key/Value | More layers = deeper understanding (each refines meaning) | Multi-head captures different relationships in parallel | Tokenize ‚Üí Embed ‚Üí 96 Layers ‚Üí Predict | Temperature=randomness, top-k=filter to k tokens, top-p=cumulative probability threshold!</em>
                        </p>
                    </div>

                    <h2>üîÆ What's Next? (Sections 16-21: Practical Deployment!)</h2>

                    <p>You understand <strong>how AI works internally</strong>! Now learn the practical side - how to actually run these massive models efficiently on your own computer.</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; margin: 30px 0;">
                        <div style="background: linear-gradient(135deg, #ffebee 0%, #ffcdd2 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #F44336;">
                            <h3 style="margin-top: 0; color: #C62828;">üìä Section 16: Floating Point</h3>
                            <p>Why FP32 vs FP16 vs BF16 matters. How precision affects model size and performance.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #FF9800;">
                            <h3 style="margin-top: 0; color: #E65100;">üîΩ Section 17: Quantization</h3>
                            <p>Shrink 7B models from 28GB to 4GB! Learn 8-bit, 4-bit, and Q4_K_M quantization.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #4CAF50;">
                            <h3 style="margin-top: 0; color: #2E7D32;">üì¶ Section 18: GGUF Format</h3>
                            <p>The standard for local AI! Understand .gguf files and naming conventions.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e1f5fe 0%, #b3e5fc 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #03A9F4;">
                            <h3 style="margin-top: 0; color: #01579B;">üíª Section 19: Local Models</h3>
                            <p>Hardware requirements and setup for running AI on your own computer privately!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #9C27B0;">
                            <h3 style="margin-top: 0; color: #6A1B9A;">üöÄ Section 20: LM Studio</h3>
                            <p>The easiest way to run AI locally! Step-by-step guide to downloading and using models.</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fff9c4 0%, #fff59d 100%);
                                    padding: 20px;
                                    border-radius: 12px;
                                    border: 2px solid #FBC02D;">
                            <h3 style="margin-top: 0; color: #F57F17;">üìè Section 21: Context Windows</h3>
                            <p>Why 4K vs 32K vs 128K context matters. Memory management and cost implications.</p>
                        </div>
                    </div>

                    <h2>üìä Your Progress</h2>

                    <div style="margin: 30px 0;">
                        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                            <span style="font-weight: bold; color: #4CAF50; font-size: 18px;">68% Complete!</span>
                            <span style="color: #666; font-size: 14px;">15 of 22 sections (excluding checkpoints)</span>
                        </div>
                        <div style="background: #e0e0e0;
                                    height: 30px;
                                    border-radius: 15px;
                                    overflow: hidden;
                                    box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);">
                            <div style="background: linear-gradient(90deg, #4CAF50 0%, #66BB6A 100%);
                                        width: 68%;
                                        height: 100%;
                                        border-radius: 15px;
                                        display: flex;
                                        align-items: center;
                                        justify-content: center;
                                        color: white;
                                        font-weight: bold;
                                        font-size: 14px;
                                        box-shadow: 0 2px 8px rgba(76, 175, 80, 0.4);
                                        animation: progressGrow3 1s ease-out;">
                                68%
                            </div>
                        </div>
                    </div>

                    <style>
                        @keyframes progressGrow3 {
                            from { width: 0%; }
                            to { width: 68%; }
                        }
                    </style>

                    <div class="success-box" style="background: linear-gradient(135deg, #c8e6c9 0%, #a5d6a7 100%); border-left: 5px solid #2E7D32; margin-top: 30px;">
                        <p style="margin: 0; font-size: 16px;">
                            <strong>üéâ Outstanding Progress!</strong> You've mastered the theory - the <em>hard part is done</em>! The remaining sections are practical guides to actually using what you've learned. You're almost there! üöÄ
                        </p>
                    </div>
                `,
                quiz: []
            },

            {
                id: 16,
                title: "Floating Point Numbers",
                category: 7,
                content: `
                    <h1>Floating Point: How Models Store Numbers</h1>
                    <p class="lead">Every parameter is a number. The format you choose (FP32, FP16, BF16) dramatically affects model size and speed!</p>

                    <svg class="flow-diagram" width="100%" height="250" viewBox="0 0 700 250">
                        <g>
                            <rect x="50" y="50" width="180" height="150" rx="8" fill="#ffebee" stroke="#F44336" stroke-width="2"/>
                            <text x="140" y="80" text-anchor="middle" font-size="14" fill="#C62828" font-weight="bold">FP32 (Full)</text>
                            <text x="140" y="110" text-anchor="middle" font-size="12" fill="#666">32 bits per number</text>
                            <text x="140" y="135" text-anchor="middle" font-size="12" fill="#666">Best precision</text>
                            <text x="140" y="160" text-anchor="middle" font-size="13" fill="#E65100" font-weight="bold">7B model = 28 GB</text>
                        </g>

                        <g>
                            <rect x="260" y="50" width="180" height="150" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                            <text x="350" y="80" text-anchor="middle" font-size="14" fill="#E65100" font-weight="bold">FP16 (Half)</text>
                            <text x="350" y="110" text-anchor="middle" font-size="12" fill="#666">16 bits per number</text>
                            <text x="350" y="135" text-anchor="middle" font-size="12" fill="#666">Good precision</text>
                            <text x="350" y="160" text-anchor="middle" font-size="13" fill="#F57C00" font-weight="bold">7B model = 14 GB</text>
                        </g>

                        <g>
                            <rect x="470" y="50" width="180" height="150" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
                            <text x="560" y="80" text-anchor="middle" font-size="14" fill="#2E7D32" font-weight="bold">BF16 (Brain)</text>
                            <text x="560" y="110" text-anchor="middle" font-size="12" fill="#666">16 bits per number</text>
                            <text x="560" y="135" text-anchor="middle" font-size="12" fill="#666">Better range</text>
                            <text x="560" y="160" text-anchor="middle" font-size="13" fill="#388E3C" font-weight="bold">7B model = 14 GB</text>
                        </g>
                    </svg>

                    <h2>Understanding Precision</h2>
                    <pre><code># FP32 (32 bits):
- 1 bit: sign (+/-)
- 8 bits: exponent (range)
- 23 bits: mantissa (precision)
= Can represent: 0.0000000001 to 3.4√ó10¬≥‚Å∏

# FP16 (16 bits):
- 1 bit: sign
- 5 bits: exponent
- 10 bits: mantissa
= Smaller range, less precision

# BF16 (16 bits):
- 1 bit: sign
- 8 bits: exponent (same as FP32!)
- 7 bits: mantissa
= Same range as FP32, less precision</code></pre>

                    <h2>Why This Matters</h2>
                    <div class="info-box">
                        <p><strong>Model Size Calculation:</strong></p>
                        <pre><code>7B parameters √ó 4 bytes (FP32) = 28 GB
7B parameters √ó 2 bytes (FP16)  = 14 GB
7B parameters √ó 2 bytes (BF16)  = 14 GB

# Going from FP32 to FP16/BF16:
‚úÖ 50% smaller file size
‚úÖ 2x faster on modern GPUs
‚úÖ Uses 50% less VRAM
‚ùå Slightly less accurate (usually fine!)</code></pre>
                    </div>

                    <h2>Which to Choose?</h2>
                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px; text-align: left;">Format</th>
                            <th style="padding: 12px; text-align: left;">Best For</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">FP32</td>
                            <td style="padding: 10px;">Training, maximum accuracy</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">FP16</td>
                            <td style="padding: 10px;">GPU inference (NVIDIA/AMD)</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">BF16</td>
                            <td style="padding: 10px;">Training on modern GPUs, better stability</td>
                        </tr>
                    </table>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> FP32 is most accurate but huge. FP16/BF16 cut size in half with minimal quality loss. Most local models use FP16 or quantized formats for efficiency!</p>
                    </div>
                `,
                quiz: [
                    {question: "How much memory does a 7B parameter model take in FP16?", options: ["7 GB", "14 GB", "28 GB", "56 GB"], correct: 1, explanation: "7 billion parameters √ó 2 bytes (FP16) = 14 GB. FP32 would be 28 GB, and FP16 cuts it in half while maintaining good quality."},
                    {question: "What's the main advantage of BF16 over FP16?", options: ["Smaller file size", "Better precision", "Same range as FP32", "Faster processing"], correct: 2, explanation: "BF16 (Brain Float 16) has the same exponent range as FP32 (8 bits), making it more stable during training. FP16 has smaller range (5-bit exponent) which can cause overflow issues."},
                    {question: "Why don't we always use FP32?", options: ["It's less accurate", "It's 2x larger and slower", "It's not supported", "It's more expensive"], correct: 1, explanation: "FP32 takes 2x more memory than FP16/BF16 and is slower on modern hardware. The quality difference is minimal for inference, so most people use FP16 or even quantized models."}
                ]
            },
            {
                id: 17,
                title: "Quantization",
                category: 7,
                content: `
                    <h1>Quantization: Extreme Compression</h1>
                    <p class="lead">Go beyond FP16! INT8, INT4, even INT2 quantization can shrink 70B models from 140GB to just 18GB with surprisingly good quality.</p>

                    <svg class="flow-diagram" width="100%" height="280" viewBox="0 0 750 280">
                        <rect x="50" y="50" width="150" height="80" rx="8" fill="#ffebee" stroke="#F44336" stroke-width="2"/>
                        <text x="125" y="75" text-anchor="middle" font-size="13" fill="#C62828" font-weight="bold">FP16</text>
                        <text x="125" y="100" text-anchor="middle" font-size="12" fill="#666">2 bytes</text>
                        <text x="125" y="120" text-anchor="middle" font-size="11" fill="#E65100">70B = 140GB</text>

                        <path d="M 200 90 L 230 90" stroke="#FF9800" stroke-width="3"/>
                        <text x="215" y="80" text-anchor="middle" font-size="11" fill="#FF9800" font-weight="bold">Quantize</text>

                        <rect x="250" y="50" width="150" height="80" rx="8" fill="#fff3e0" stroke="#FF9800" stroke-width="2"/>
                        <text x="325" y="75" text-anchor="middle" font-size="13" fill="#E65100" font-weight="bold">INT8</text>
                        <text x="325" y="100" text-anchor="middle" font-size="12" fill="#666">1 byte</text>
                        <text x="325" y="120" text-anchor="middle" font-size="11" fill="#F57C00">70B = 70GB</text>

                        <path d="M 400 90 L 430 90" stroke="#4CAF50" stroke-width="3"/>

                        <rect x="450" y="50" width="150" height="80" rx="8" fill="#e1f5fe" stroke="#03A9F4" stroke-width="2"/>
                        <text x="525" y="75" text-anchor="middle" font-size="13" fill="#0277BD" font-weight="bold">INT4</text>
                        <text x="525" y="100" text-anchor="middle" font-size="12" fill="#666">0.5 bytes</text>
                        <text x="525" y="120" text-anchor="middle" font-size="11" fill="#0288D1">70B = 35GB</text>

                        <rect x="625" y="50" width="100" height="80" rx="8" fill="#e8f5e9" stroke="#4CAF50" stroke-width="2"/>
                        <text x="675" y="75" text-anchor="middle" font-size="13" fill="#2E7D32" font-weight="bold">INT2</text>
                        <text x="675" y="100" text-anchor="middle" font-size="12" fill="#666">0.25 bytes</text>
                        <text x="675" y="120" text-anchor="middle" font-size="11" fill="#388E3C">70B = 18GB!</text>

                        <g>
                            <rect x="200" y="170" width="350" height="90" rx="8" fill="#f3e5f5" stroke="#9C27B0" stroke-width="2"/>
                            <text x="375" y="195" text-anchor="middle" font-size="14" fill="#6A1B9A" font-weight="bold">Quality vs Size Trade-off</text>
                            <text x="375" y="220" text-anchor="middle" font-size="11" fill="#7B1FA2">INT8: ~99% quality, 50% size</text>
                            <text x="375" y="240" text-anchor="middle" font-size="11" fill="#7B1FA2">INT4: ~95% quality, 25% size</text>
                        </g>
                    </svg>

                    <h2>How Quantization Works</h2>
                    <pre><code># Original FP16 values:
weights = [0.1234, -0.5678, 0.9012, ...]

# Quantize to INT8 (-128 to 127):
1. Find min/max: -0.5678 to 0.9012
2. Map to -128 to 127:
   0.1234  ‚Üí 25
   -0.5678 ‚Üí -128
   0.9012  ‚Üí 127

# Store: 8-bit integers + scale factor
# Dequantize when needed:
   25 √ó scale ‚Üí 0.1234 (approximately)</code></pre>

                    <h2>Quantization Levels</h2>
                    <div class="info-box">
                        <p><strong>Popular Formats:</strong></p>
                        <ul>
                            <li><strong>Q8_0</strong>: 8-bit, highest quality quantization</li>
                            <li><strong>Q4_K_M</strong>: 4-bit, best quality/size balance (most popular!)</li>
                            <li><strong>Q4_K_S</strong>: 4-bit, smaller variant</li>
                            <li><strong>Q3_K_M</strong>: 3-bit, very aggressive</li>
                            <li><strong>Q2_K</strong>: 2-bit, extreme compression</li>
                        </ul>
                    </div>

                    <h2>Real Example: LLaMA 2 70B</h2>
                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px;">Format</th>
                            <th style="padding: 12px;">Size</th>
                            <th style="padding: 12px;">Quality</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">FP16</td>
                            <td style="padding: 10px;">140 GB</td>
                            <td style="padding: 10px;">100% ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Q8_0</td>
                            <td style="padding: 10px;">70 GB</td>
                            <td style="padding: 10px;">99% ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;"><strong>Q4_K_M</strong></td>
                            <td style="padding: 10px;"><strong>39 GB</strong></td>
                            <td style="padding: 10px;"><strong>95% ‚≠ê‚≠ê‚≠ê‚≠ê</strong></td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Q2_K</td>
                            <td style="padding: 10px;">18 GB</td>
                            <td style="padding: 10px;">85% ‚≠ê‚≠ê‚≠ê</td>
                        </tr>
                    </table>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Quantization reduces model size by 4-8x with minimal quality loss. Q4_K_M is the sweet spot for most users - 1/4 the size, 95% quality!</p>
                    </div>
                `,
                quiz: [
                    {question: "What does Q4_K_M mean?", options: ["4 KB file size", "4-bit quantization, K-quant method, Medium quality", "4 transformer layers", "4 attention heads"], correct: 1, explanation: "Q4_K_M means 4-bit quantization using K-quant method (advanced technique) with Medium quality settings. It's one of the most popular formats for local models."},
                    {question: "How much can Q4 quantization reduce model size?", options: ["10%", "25%", "50%", "75%"], correct: 3, explanation: "Q4 (4-bit) quantization reduces size to 25% of original (75% reduction). FP16 uses 16 bits, Q4 uses 4 bits ‚Üí 4/16 = 25% of original size!"},
                    {question: "What's the trade-off with aggressive quantization?", options: ["Slower inference", "Lower quality outputs", "More memory usage", "Harder to load"], correct: 1, explanation: "Aggressive quantization (Q2, Q3) significantly reduces quality. Q4_K_M offers the best balance - 1/4 size with ~95% quality. Q8 is nearly identical to FP16 but half the size."}
                ]
            },
            {
                id: 18,
                title: "GGUF Format",
                category: 7,
                content: `
                    <h1>GGUF: The Universal Model Format</h1>
                    <p class="lead">GGUF (GPT-Generated Unified Format) is the standard for running models locally. It's optimized for CPU/GPU inference and supports all quantization levels!</p>

                    <div class="info-box" style="margin: 20px 0;">
                        <p><strong>Why GGUF?</strong></p>
                        <ul>
                            <li>‚úÖ <strong>CPU-friendly</strong>: Can run on laptops without GPUs</li>
                            <li>‚úÖ <strong>Memory efficient</strong>: Supports all quantization formats</li>
                            <li>‚úÖ <strong>Fast loading</strong>: Optimized file structure</li>
                            <li>‚úÖ <strong>Wide support</strong>: Works with llama.cpp, LM Studio, Ollama</li>
                            <li>‚úÖ <strong>Cross-platform</strong>: Windows, Mac, Linux</li>
                        </ul>
                    </div>

                    <h2>GGUF File Naming Convention</h2>
                    <pre><code>llama-2-7b-chat.Q4_K_M.gguf
‚îÇ           ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ           ‚îÇ     ‚îÇ      ‚îî‚îÄ File format
‚îÇ           ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Quantization
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Model variant
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Base model

Common patterns:
- llama-2-7b.Q4_K_M.gguf       (Base model, 4-bit)
- mistral-7b-instruct.Q5_K_S.gguf  (5-bit, small variant)
- llama-2-70b.Q2_K.gguf        (Huge model, extreme compression)</code></pre>

                    <h2>Popular Quantization Formats</h2>
                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px;">Format</th>
                            <th style="padding: 12px;">Bits</th>
                            <th style="padding: 12px;">Use Case</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">Q2_K</td>
                            <td style="padding: 10px;">2-bit</td>
                            <td style="padding: 10px;">Maximum compression, lowest quality</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Q3_K_M</td>
                            <td style="padding: 10px;">3-bit</td>
                            <td style="padding: 10px;">Good compression, acceptable quality</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px;"><strong>Q4_K_M</strong></td>
                            <td style="padding: 10px;"><strong>4-bit</strong></td>
                            <td style="padding: 10px;"><strong>Best balance (recommended!)</strong></td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Q5_K_M</td>
                            <td style="padding: 10px;">5-bit</td>
                            <td style="padding: 10px;">Higher quality, larger size</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">Q6_K</td>
                            <td style="padding: 10px;">6-bit</td>
                            <td style="padding: 10px;">Near-original quality</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Q8_0</td>
                            <td style="padding: 10px;">8-bit</td>
                            <td style="padding: 10px;">Maximum quality, if you have space</td>
                        </tr>
                    </table>

                    <h2>GGUF vs Other Formats</h2>
                    <div class="warning-box">
                        <p><strong>Other formats you might see:</strong></p>
                        <ul>
                            <li><strong>PyTorch (.pth, .bin)</strong>: Training format, large, needs GPU</li>
                            <li><strong>SafeTensors</strong>: Secure format, used by Hugging Face</li>
                            <li><strong>GGML</strong>: Old format (replaced by GGUF)</li>
                            <li><strong>GGUF</strong>: Current standard for local inference ‚ú®</li>
                        </ul>
                    </div>

                    <h2>Choosing the Right Quantization</h2>
                    <pre><code># 16GB RAM laptop:
llama-2-7b.Q4_K_M.gguf         (4 GB)   ‚Üê Perfect fit!

# 32GB RAM laptop:
llama-2-13b.Q4_K_M.gguf        (7 GB)   ‚Üê Good performance
mistral-7b.Q5_K_M.gguf         (5 GB)   ‚Üê Higher quality

# 64GB RAM workstation:
llama-2-70b.Q4_K_M.gguf        (39 GB)  ‚Üê Run big models!

# 8GB RAM (tight):
llama-2-7b.Q2_K.gguf           (2.5 GB) ‚Üê Works but lower quality</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> GGUF is THE format for local AI. Q4_K_M is the sweet spot. Download from HuggingFace, load in LM Studio/Ollama, and you're running AI locally!</p>
                    </div>
                `,
                quiz: [
                    {question: "What does GGUF stand for?", options: ["GPU Generated Format", "GPT-Generated Unified Format", "General Graph Unified Format", "Quantized Format"], correct: 1, explanation: "GGUF stands for GPT-Generated Unified Format. It's the successor to GGML and is now the standard format for running LLMs locally with tools like llama.cpp and LM Studio."},
                    {question: "Which quantization is recommended for most users?", options: ["Q2_K", "Q4_K_M", "Q8_0", "FP16"], correct: 1, explanation: "Q4_K_M offers the best balance between size and quality - about 1/4 the size of FP16 with ~95% quality retention. It's the most popular choice for local inference."},
                    {question: "What's the main advantage of GGUF over PyTorch models?", options: ["Better training", "CPU-friendly and quantized", "More accurate", "Faster training"], correct: 1, explanation: "GGUF is optimized for CPU/GPU inference and supports quantization. PyTorch models are great for training but are large and GPU-dependent. GGUF lets you run models on regular laptops!"}
                ]
            },
            {
                id: 19,
                title: "Running Models Locally",
                category: 8,
                content: `
                    <h1>Running AI Models on Your Computer</h1>
                    <p class="lead">You don't need expensive cloud APIs! With the right hardware and tools, you can run powerful AI models completely locally and privately.</p>

                    <h2>Hardware Requirements</h2>
                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px;">Model Size</th>
                            <th style="padding: 12px;">RAM Needed</th>
                            <th style="padding: 12px;">VRAM (GPU)</th>
                            <th style="padding: 12px;">Speed</th>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px;"><strong>7B (Q4)</strong></td>
                            <td style="padding: 10px;"><strong>8 GB</strong></td>
                            <td style="padding: 10px;"><strong>6 GB</strong></td>
                            <td style="padding: 10px;"><strong>Fast ‚úÖ</strong></td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">13B (Q4)</td>
                            <td style="padding: 10px;">16 GB</td>
                            <td style="padding: 10px;">10 GB</td>
                            <td style="padding: 10px;">Medium</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">34B (Q4)</td>
                            <td style="padding: 10px;">32 GB</td>
                            <td style="padding: 10px;">20 GB</td>
                            <td style="padding: 10px;">Slow</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">70B (Q4)</td>
                            <td style="padding: 10px;">64 GB</td>
                            <td style="padding: 10px;">40 GB</td>
                            <td style="padding: 10px;">Very slow (CPU)</td>
                        </tr>
                    </table>

                    <h2>CPU vs GPU Inference</h2>
                    <div class="info-box">
                        <p><strong>CPU Inference (Any laptop)</strong></p>
                        <ul>
                            <li>‚úÖ Works on any computer</li>
                            <li>‚úÖ No special GPU needed</li>
                            <li>‚ùå Slow (5-20 tokens/sec)</li>
                            <li>üîÑ Good for: 7B models, occasional use</li>
                        </ul>

                        <p><strong>GPU Inference (NVIDIA/AMD)</strong></p>
                        <ul>
                            <li>‚úÖ Very fast (50-150 tokens/sec)</li>
                            <li>‚úÖ Can run larger models</li>
                            <li>‚ùå Requires compatible GPU</li>
                            <li>üîÑ Good for: 13B-70B models, frequent use</li>
                        </ul>
                    </div>

                    <h2>Popular Tools</h2>
                    <div class="warning-box">
                        <p><strong>LM Studio</strong> (Easiest - GUI)</p>
                        <ul>
                            <li>Download any model from HuggingFace</li>
                            <li>One-click install</li>
                            <li>ChatGPT-like interface</li>
                            <li>Windows, Mac, Linux</li>
                        </ul>

                        <p><strong>Ollama</strong> (Developer-friendly - CLI)</p>
                        <ul>
                            <li>Simple commands: <code>ollama run llama2</code></li>
                            <li>Built-in model library</li>
                            <li>API compatible with OpenAI</li>
                            <li>Great for coding</li>
                        </ul>

                        <p><strong>llama.cpp</strong> (Advanced - Most control)</p>
                        <ul>
                            <li>Pure C++ implementation</li>
                            <li>Maximum performance</li>
                            <li>Flexible configuration</li>
                            <li>Requires technical knowledge</li>
                        </ul>
                    </div>

                    <h2>Recommended Setup by Budget</h2>
                    <pre><code># Budget laptop (8GB RAM):
Model:  llama-2-7b-chat.Q4_K_M.gguf (4 GB)
Tool:   LM Studio
Speed:  ~10 tokens/sec (CPU)
Use:    Casual chat, coding help

# Mid-range (16GB RAM + RTX 3060):
Model:  mistral-7b-instruct.Q5_K_M.gguf (5 GB)
Tool:   Ollama
Speed:  ~60 tokens/sec (GPU)
Use:    Daily coding, writing

# High-end (32GB RAM + RTX 4090):
Model:  llama-2-70b-chat.Q4_K_M.gguf (39 GB)
Tool:   LM Studio with GPU offloading
Speed:  ~30 tokens/sec (partial GPU)
Use:    Professional work, complex tasks</code></pre>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Start with 7B Q4 models on any laptop. Use LM Studio for simplicity or Ollama for coding. GPU dramatically improves speed but isn't required!</p>
                    </div>
                `,
                quiz: [
                    {question: "What's the minimum RAM needed to run a 7B Q4 model?", options: ["4 GB", "8 GB", "16 GB", "32 GB"], correct: 1, explanation: "A 7B model in Q4 format is about 4 GB, so 8 GB RAM minimum is recommended (leaving room for OS and other apps). 16 GB is more comfortable."},
                    {question: "What's the main advantage of GPU inference over CPU?", options: ["Better quality", "Smaller models", "Much faster speed", "Lower cost"], correct: 2, explanation: "GPU inference is 5-10x faster than CPU - getting 50-150 tokens/sec vs 5-20 on CPU. Quality is the same, but speed makes the experience much better!"},
                    {question: "Which tool is best for beginners?", options: ["llama.cpp", "Python scripts", "LM Studio", "Command line"], correct: 2, explanation: "LM Studio has a GUI, one-click downloads, and is extremely beginner-friendly. Ollama is great for developers, llama.cpp is for advanced users who want maximum control."}
                ]
            },
            {
                id: 20,
                title: "LM Studio vs Enterprise APIs",
                category: 8,
                content: `
                    <h1>Local vs Cloud: Choosing the Right Approach</h1>
                    <p class="lead">Should you run AI locally or use cloud APIs? Each has pros and cons. Understanding both helps you make the right choice for your needs!</p>

                    <h2>Local Tools (LM Studio, Ollama)</h2>
                    <div class="info-box">
                        <p><strong>Advantages</strong></p>
                        <ul>
                            <li>‚úÖ <strong>Privacy</strong>: Your data never leaves your computer</li>
                            <li>‚úÖ <strong>Cost</strong>: No API fees, free after initial download</li>
                            <li>‚úÖ <strong>Offline</strong>: Works without internet</li>
                            <li>‚úÖ <strong>Control</strong>: Choose any model, any settings</li>
                            <li>‚úÖ <strong>No limits</strong>: Unlimited usage</li>
                        </ul>

                        <p><strong>Disadvantages</strong></p>
                        <ul>
                            <li>‚ùå <strong>Hardware needed</strong>: Requires decent RAM/GPU</li>
                            <li>‚ùå <strong>Slower</strong>: Especially on CPU</li>
                            <li>‚ùå <strong>Lower quality</strong>: 7B/13B < GPT-4</li>
                            <li>‚ùå <strong>Setup required</strong>: Not just "type and go"</li>
                        </ul>
                    </div>

                    <h2>Cloud APIs (OpenAI, Anthropic, Google)</h2>
                    <div class="warning-box">
                        <p><strong>Advantages</strong></p>
                        <ul>
                            <li>‚úÖ <strong>Best quality</strong>: GPT-4, Claude 3.5 are state-of-the-art</li>
                            <li>‚úÖ <strong>Fast</strong>: Optimized infrastructure</li>
                            <li>‚úÖ <strong>No hardware</strong>: Works on any device</li>
                            <li>‚úÖ <strong>Easy</strong>: Just API key and go</li>
                            <li>‚úÖ <strong>Latest models</strong>: Always updated</li>
                        </ul>

                        <p><strong>Disadvantages</strong></p>
                        <ul>
                            <li>‚ùå <strong>Cost</strong>: $0.01-0.06 per 1K tokens</li>
                            <li>‚ùå <strong>Privacy</strong>: Data sent to cloud</li>
                            <li>‚ùå <strong>Rate limits</strong>: API quotas apply</li>
                            <li>‚ùå <strong>Internet required</strong>: No offline use</li>
                            <li>‚ùå <strong>Vendor lock-in</strong>: Depends on their service</li>
                        </ul>
                    </div>

                    <h2>Cost Comparison</h2>
                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px;">Usage</th>
                            <th style="padding: 12px;">Local (LM Studio)</th>
                            <th style="padding: 12px;">Cloud (GPT-4)</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">Light (10K tokens/day)</td>
                            <td style="padding: 10px;">$0/month</td>
                            <td style="padding: 10px;">~$3/month</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Medium (100K tokens/day)</td>
                            <td style="padding: 10px;">$0/month</td>
                            <td style="padding: 10px;">~$30/month</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">Heavy (1M tokens/day)</td>
                            <td style="padding: 10px;">$0/month</td>
                            <td style="padding: 10px;">~$300/month</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px;"><strong>Initial cost</strong></td>
                            <td style="padding: 10px;"><strong>$0 (free)</strong></td>
                            <td style="padding: 10px;"><strong>$0</strong></td>
                        </tr>
                    </table>

                    <h2>Hybrid Approach (Best of Both)</h2>
                    <pre><code># Use Local for:
- Coding assistance (private code)
- Personal notes and brainstorming
- Learning and experimentation
- High-volume, simple tasks

# Use Cloud for:
- Complex reasoning (GPT-4 level)
- Latest features (vision, voice)
- Critical accuracy needs
- When you need the absolute best</code></pre>

                    <h2>Recommendations by Use Case</h2>
                    <div class="success-box">
                        <p><strong>Student / Learner</strong></p>
                        <p>‚Üí Start with LM Studio (free, unlimited)</p>

                        <p><strong>Developer</strong></p>
                        <p>‚Üí Ollama for local coding + GPT-4 for complex problems</p>

                        <p><strong>Business / Enterprise</strong></p>
                        <p>‚Üí Cloud APIs for production + local for sensitive data</p>

                        <p><strong>Privacy-Conscious</strong></p>
                        <p>‚Üí 100% local with LM Studio + larger models (13B/34B)</p>
                    </div>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Local is free, private, and unlimited but requires hardware. Cloud is easy and highest quality but costs money. Most people benefit from using both!</p>
                    </div>
                `,
                quiz: [
                    {question: "What's the main advantage of running models locally?", options: ["Better quality", "Privacy and no API costs", "Faster speed", "Easier setup"], correct: 1, explanation: "Local models are completely private (data never leaves your computer) and free after download (no API fees). This makes them perfect for sensitive data or high-volume use."},
                    {question: "When should you use cloud APIs instead of local models?", options: ["For simple tasks", "When you need maximum quality", "To save money", "For privacy"], correct: 1, explanation: "Cloud APIs like GPT-4 offer the highest quality and latest features. Use them when accuracy is critical or for complex reasoning tasks. For simple tasks, local models work great and are free."},
                    {question: "What's a hybrid approach?", options: ["Using only local models", "Using only cloud APIs", "Using both local and cloud depending on the task", "Mixing different quantizations"], correct: 2, explanation: "A hybrid approach uses local models for private/simple tasks and cloud APIs for complex/critical tasks. This gives you the best of both worlds - privacy + cost savings + high quality when needed."}
                ]
            },
            {
                id: 21,
                title: "Context Windows & Memory",
                category: 8,
                content: `
                    <h1>Context Windows: How Much Can AI Remember?</h1>
                    <p class="lead">The context window determines how much text an AI can "see" at once. Understanding this helps you work more effectively with AI models!</p>

                    <h2>What is a Context Window?</h2>
                    <div class="info-box">
                        <p><strong>Context window = Maximum input + output tokens</strong></p>
                        <p>If a model has a 4K context window:</p>
                        <ul>
                            <li>You can send up to ~3K tokens of input</li>
                            <li>Model can generate ~1K tokens of output</li>
                            <li><strong>Total = 4,000 tokens max</strong></li>
                        </ul>
                        <p>Anything beyond this is forgotten!</p>
                    </div>

                    <h2>Context Window Sizes</h2>
                    <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px;">Model</th>
                            <th style="padding: 12px;">Context</th>
                            <th style="padding: 12px;">Use Case</th>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">GPT-3.5</td>
                            <td style="padding: 10px;">4K tokens</td>
                            <td style="padding: 10px;">Short conversations</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">LLaMA 2</td>
                            <td style="padding: 10px;">4K tokens</td>
                            <td style="padding: 10px;">Standard chat</td>
                        </tr>
                        <tr style="background: #f5f5f5;">
                            <td style="padding: 10px;">GPT-4</td>
                            <td style="padding: 10px;">8K - 32K</td>
                            <td style="padding: 10px;">Long documents</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Claude 3</td>
                            <td style="padding: 10px;">200K</td>
                            <td style="padding: 10px;">Entire books!</td>
                        </tr>
                        <tr style="background: #e8f5e9;">
                            <td style="padding: 10px;"><strong>Gemini 1.5</strong></td>
                            <td style="padding: 10px;"><strong>1M tokens</strong></td>
                            <td style="padding: 10px;"><strong>Multiple books</strong></td>
                        </tr>
                    </table>

                    <h2>Why Are Context Windows Limited?</h2>
                    <div class="warning-box">
                        <p><strong>Remember: Self-attention is O(N¬≤)</strong></p>
                        <pre><code>1K tokens   ‚Üí 1M comparisons
4K tokens   ‚Üí 16M comparisons
128K tokens ‚Üí 16 BILLION comparisons!

This requires massive memory and computation!</code></pre>
                    </div>

                    <h2>Token Estimation</h2>
                    <pre><code># Rule of thumb:
~750 words = 1,000 tokens
~3 pages = 1,000 tokens

Examples:
Short email:     ~100-200 tokens
This webpage:    ~2,000 tokens
Blog post:       ~1,000-2,000 tokens
Research paper:  ~8,000-15,000 tokens
Novel chapter:   ~5,000-10,000 tokens
Entire book:     ~100,000+ tokens</code></pre>

                    <h2>Working with Long Documents</h2>
                    <div class="info-box">
                        <p><strong>Strategies when context is too small:</strong></p>
                        <ul>
                            <li><strong>Chunking</strong>: Break document into sections, process separately</li>
                            <li><strong>Summarization</strong>: Summarize early parts to fit more</li>
                            <li><strong>Retrieval</strong>: Use vector DB to fetch relevant sections only</li>
                            <li><strong>Sliding window</strong>: Process overlapping chunks</li>
                            <li><strong>Use longer context models</strong>: Claude 3 (200K), Gemini (1M)</li>
                        </ul>
                    </div>

                    <h2>Context Window vs Response Length</h2>
                    <pre><code># 4K context window:
Input:  3,000 tokens (your prompt + conversation)
Output: 1,000 tokens max (model's response)
Total:  4,000 tokens

# If you want longer outputs:
- Use model with larger context (8K, 32K)
- Or split into multiple requests
- Set max_tokens parameter appropriately</code></pre>

                    <h2>Practical Tips</h2>
                    <div class="success-box">
                        <ul>
                            <li>‚úÖ <strong>Track tokens</strong>: Use token counters (tiktoken library)</li>
                            <li>‚úÖ <strong>Summarize</strong>: Compress conversation history when near limit</li>
                            <li>‚úÖ <strong>Be concise</strong>: Every word counts toward limit</li>
                            <li>‚úÖ <strong>Choose right model</strong>: Match context needs to model capability</li>
                            <li>‚ùå <strong>Don't assume</strong>: Model forgets everything beyond window!</li>
                        </ul>
                    </div>

                    <div class="success-box">
                        <p><strong>Key Takeaway:</strong> Context window = how much the model can "see". Small windows (4K) for chat, large windows (200K) for documents. Remember: attention is O(N¬≤), so longer context = much more computation!</p>
                    </div>
                `,
                quiz: [
                    {question: "What happens when you exceed the context window?", options: ["Model gets slower", "Older messages are forgotten", "Error message appears", "Quality decreases"], correct: 1, explanation: "When you exceed the context window, the oldest messages are dropped (forgotten). The model only sees the most recent tokens that fit in the window. This is why long conversations sometimes lose context."},
                    {question: "Why can't models have unlimited context windows?", options: ["Cost too much", "Self-attention is O(N¬≤) - grows quadratically", "Storage limitations", "API restrictions"], correct: 1, explanation: "Self-attention requires every token to attend to every other token, giving O(N¬≤) complexity. Doubling context length quadruples the memory and computation needed! This is why 1M token context is a huge engineering achievement."},
                    {question: "How many tokens is a typical book?", options: ["1,000 tokens", "10,000 tokens", "100,000 tokens", "1,000,000 tokens"], correct: 2, explanation: "A typical book is ~100,000 tokens (about 75,000 words). This means you need a 100K+ context window to fit an entire book, which only recent models like Claude 3 (200K) and Gemini 1.5 (1M) can handle."}
                ]
            },

            // ============================================
            // üéØ FINAL CHECKPOINT: Complete Guide Mastered!
            // ============================================
            {
                id: 21.5,
                title: "üéØ Final Checkpoint: Complete!",
                category: 8,
                content: `
                    <div style="background: linear-gradient(135deg, #FFD700 0%, #FFA500 50%, #FF6347 100%);
                                color: white;
                                padding: 40px;
                                margin: 0 0 40px 0;
                                border-radius: 20px;
                                box-shadow: 0 12px 40px rgba(255, 215, 0, 0.4);
                                text-align: center;">
                        <h1 style="margin-top: 0; color: white; font-size: 42px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);">
                            üéâüéì CONGRATULATIONS! üéìüéâ
                        </h1>
                        <h2 style="margin: 20px 0; color: white; font-size: 28px; font-weight: normal; opacity: 0.95;">
                            You've Completed the Entire LLM Learning Guide!
                        </h2>
                        <p style="font-size: 18px; margin-bottom: 0; opacity: 0.9;">
                            You now understand Large Language Models better than 99% of people!
                        </p>
                    </div>

                    <h2>üåü What You've Achieved</h2>

                    <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                padding: 30px;
                                border-radius: 16px;
                                border-left: 6px solid #4CAF50;
                                margin: 30px 0;">
                        <p style="font-size: 18px; line-height: 1.8; margin: 0;">
                            <strong style="color: #2E7D32; font-size: 20px;">You can now explain:</strong><br><br>
                            ‚úÖ How text becomes tokens (tokenization)<br>
                            ‚úÖ How tokens become vectors (embeddings)<br>
                            ‚úÖ How vectors capture meaning (dimensions & parameters)<br>
                            ‚úÖ How attention makes AI context-aware (self-attention)<br>
                            ‚úÖ How 96 layers refine understanding (transformers)<br>
                            ‚úÖ How AI predicts the next word (forward pass)<br>
                            ‚úÖ How to control creativity vs accuracy (temperature)<br>
                            ‚úÖ How to make models smaller (quantization)<br>
                            ‚úÖ How to run AI on your own computer (local deployment)<br>
                            ‚úÖ How context windows work (memory management)
                        </p>
                    </div>

                    <h2>üìö Complete Journey Recap (All 22 Sections!)</h2>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 30px 0;">
                        <div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #2196F3;">
                            <h4 style="margin-top: 0; color: #1565C0;">üìù Data Foundations</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 1-3: Binary, Unicode, ASCII</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #4CAF50;">
                            <h4 style="margin-top: 0; color: #2E7D32;">üî§ Tokenization</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 4-5: BPE, Vocabulary</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #9C27B0;">
                            <h4 style="margin-top: 0; color: #6A1B9A;">üìä Vectors & Embeddings</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 6-8: Meaning in numbers</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #FF9800;">
                            <h4 style="margin-top: 0; color: #E65100;">üíæ Parameters</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 9-10: 7B learned numbers</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #ffebee 0%, #ffcdd2 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #F44336;">
                            <h4 style="margin-top: 0; color: #C62828;">‚öôÔ∏è Transformers</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 11-13: Attention, Layers</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #fce4ec 0%, #f8bbd0 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #E91E63;">
                            <h4 style="margin-top: 0; color: #AD1457;">ü§ñ Complete Pipeline</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 14-15: Forward pass, Prediction</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #ede7f6 0%, #d1c4e9 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #673AB7;">
                            <h4 style="margin-top: 0; color: #4527A0;">üîß Efficiency</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 16-18: FP16, Quantization, GGUF</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e0f2f1 0%, #b2dfdb 100%);
                                    padding: 15px;
                                    border-radius: 10px;
                                    border-left: 4px solid #009688;">
                            <h4 style="margin-top: 0; color: #00695C;">üöÄ Deployment</h4>
                            <p style="font-size: 14px; margin-bottom: 0;">Sections 19-21: Local models, LM Studio</p>
                        </div>
                    </div>

                    <h2>üîÑ From Text to AI: The Complete Story</h2>

                    <div style="background: linear-gradient(135deg, #f5f5f5 0%, #e0e0e0 100%);
                                padding: 30px;
                                border-radius: 12px;
                                margin: 30px 0;
                                border: 3px solid #9E9E9E;">
                        <p style="font-size: 16px; line-height: 2.2; margin: 0; font-family: monospace;">
                            <strong style="color: #1976D2; font-size: 18px;">"Tell me about cats"</strong><br>
                            ‚Üì <em style="color: #666;">Tokenization (Sections 4-5)</em><br>
                            ["Tell", " me", " about", " cats"]<br>
                            ‚Üì <em style="color: #666;">Token IDs (Section 5)</em><br>
                            [5779, 757, 546, 10875]<br>
                            ‚Üì <em style="color: #666;">Embedding Lookup (Section 10)</em><br>
                            4096-dimensional vectors for each token<br>
                            ‚Üì <em style="color: #666;">Self-Attention (Section 11)</em><br>
                            "cats" attends to "about" ‚Üí context-aware!<br>
                            ‚Üì <em style="color: #666;">96 Transformer Layers (Sections 12-13)</em><br>
                            Each layer refines understanding<br>
                            ‚Üì <em style="color: #666;">Output Layer (Section 14)</em><br>
                            Probabilities for next token<br>
                            ‚Üì <em style="color: #666;">Sampling (Section 15)</em><br>
                            <strong style="color: #2E7D32; font-size: 18px;">‚ú® "Cats are domesticated carnivorous mammals..."</strong>
                        </p>
                    </div>

                    <h2>üéØ What Makes You Special Now</h2>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0;">
                        <div style="background: linear-gradient(135deg, #fff9c4 0%, #fff59d 100%);
                                    padding: 25px;
                                    border-radius: 12px;
                                    border: 2px solid #FBC02D;">
                            <h3 style="margin-top: 0; color: #F57F17;">üí° Deep Understanding</h3>
                            <p>You don't just use AI - you <strong>understand</strong> how it works internally. You can explain transformers to others!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #e1f5fe 0%, #b3e5fc 100%);
                                    padding: 25px;
                                    border-radius: 12px;
                                    border: 2px solid #03A9F4;">
                            <h3 style="margin-top: 0; color: #01579B;">üõ†Ô∏è Practical Skills</h3>
                            <p>You know how to choose models, optimize for size, run locally, and manage context windows effectively!</p>
                        </div>

                        <div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
                                    padding: 25px;
                                    border-radius: 12px;
                                    border: 2px solid #9C27B0;">
                            <h3 style="margin-top: 0; color: #6A1B9A;">üîÆ Future-Ready</h3>
                            <p>This foundational knowledge applies to GPT-5, GPT-6, and beyond. The core concepts won't change!</p>
                        </div>
                    </div>

                    <h2>üöÄ What's Next? Your Learning Path Forward</h2>

                    <div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
                                padding: 25px;
                                border-radius: 12px;
                                border-left: 5px solid #2196F3;
                                margin: 30px 0;">
                        <h3 style="margin-top: 0; color: #1565C0;">üìñ Deepen Your Knowledge</h3>
                        <ul style="line-height: 2;">
                            <li>Read the original "Attention is All You Need" paper (2017)</li>
                            <li>Study GPT-2 and GPT-3 papers from OpenAI</li>
                            <li>Learn about training techniques (supervised fine-tuning, RLHF)</li>
                            <li>Explore LLaMA 2, Claude, and Gemini architectures</li>
                        </ul>
                    </div>

                    <div style="background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
                                padding: 25px;
                                border-radius: 12px;
                                border-left: 5px solid #4CAF50;
                                margin: 30px 0;">
                        <h3 style="margin-top: 0; color: #2E7D32;">üíª Build Projects</h3>
                        <ul style="line-height: 2;">
                            <li>Fine-tune your own model on custom data</li>
                            <li>Build a RAG (Retrieval-Augmented Generation) system</li>
                            <li>Create a local AI assistant with LM Studio</li>
                            <li>Implement attention mechanism from scratch in Python</li>
                        </ul>
                    </div>

                    <div style="background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                                padding: 25px;
                                border-radius: 12px;
                                border-left: 5px solid #FF9800;
                                margin: 30px 0;">
                        <h3 style="margin-top: 0; color: #E65100;">üéì Advanced Topics</h3>
                        <ul style="line-height: 2;">
                            <li>Model training and optimization (Adam, learning rate schedules)</li>
                            <li>Advanced architectures (mixture of experts, sparse attention)</li>
                            <li>Multimodal models (CLIP, GPT-4 Vision)</li>
                            <li>Prompt engineering and few-shot learning</li>
                        </ul>
                    </div>

                    <h2>üìä Your Final Progress</h2>

                    <div style="margin: 30px 0;">
                        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                            <span style="font-weight: bold; color: #4CAF50; font-size: 22px;">100% COMPLETE! üéâ</span>
                            <span style="color: #666; font-size: 14px;">22 of 22 sections (including checkpoints: 26 total)</span>
                        </div>
                        <div style="background: #e0e0e0;
                                    height: 40px;
                                    border-radius: 20px;
                                    overflow: hidden;
                                    box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);">
                            <div style="background: linear-gradient(90deg, #4CAF50 0%, #8BC34A 50%, #CDDC39 100%);
                                        width: 100%;
                                        height: 100%;
                                        border-radius: 20px;
                                        display: flex;
                                        align-items: center;
                                        justify-content: center;
                                        color: white;
                                        font-weight: bold;
                                        font-size: 18px;
                                        box-shadow: 0 2px 8px rgba(76, 175, 80, 0.6);
                                        animation: progressGrowFinal 2s ease-out, celebration 3s ease-in-out infinite;">
                                ‚ú® 100% ‚ú®
                            </div>
                        </div>
                    </div>

                    <style>
                        @keyframes progressGrowFinal {
                            from { width: 0%; }
                            to { width: 100%; }
                        }
                        @keyframes celebration {
                            0%, 100% { transform: scale(1); }
                            50% { transform: scale(1.02); }
                        }
                    </style>

                    <div style="background: linear-gradient(135deg, #FFD700 0%, #FFA500 100%);
                                color: white;
                                padding: 30px;
                                border-radius: 16px;
                                text-align: center;
                                margin: 40px 0;
                                box-shadow: 0 8px 32px rgba(255, 215, 0, 0.5);">
                        <h2 style="margin-top: 0; color: white; font-size: 28px;">üèÜ You Are Now an LLM Expert! üèÜ</h2>
                        <p style="font-size: 18px; margin-bottom: 20px; opacity: 0.95;">
                            Share this knowledge, build amazing things, and continue learning!
                        </p>
                        <p style="font-size: 16px; margin-bottom: 0; opacity: 0.9;">
                            <em>"The beautiful thing about learning is that nobody can take it away from you."</em><br>
                            - B.B. King
                        </p>
                    </div>

                    <div style="background: linear-gradient(135deg, #f5f5f5 0%, #e0e0e0 100%);
                                padding: 25px;
                                border-radius: 12px;
                                text-align: center;
                                margin: 30px 0;
                                border: 2px dashed #9E9E9E;">
                        <p style="font-size: 16px; color: #666; margin: 0;">
                            <strong>Did this guide help you?</strong><br>
                            Consider sharing it with others who want to understand AI!<br>
                            The best way to solidify your knowledge is to teach it to someone else. üéì
                        </p>
                    </div>

                    <div class="success-box" style="background: linear-gradient(135deg, #c8e6c9 0%, #a5d6a7 100%); border-left: 5px solid #2E7D32; margin-top: 40px;">
                        <p style="margin: 0; font-size: 18px; text-align: center;">
                            <strong>üéâ CONGRATULATIONS AGAIN! üéâ</strong><br><br>
                            You've completed one of the most comprehensive LLM guides available.<br>
                            You now have the knowledge to understand, use, and build with AI confidently.<br><br>
                            <strong style="font-size: 20px;">Welcome to the future! üöÄ</strong>
                        </p>
                    </div>
                `,
                quiz: []
            }

        ];

        // Category definitions
        const categories = [
            {
                id: 0,
                name: "Welcome & Overview",
                icon: "üéì",
                why: "Start here to understand what you'll learn and choose your learning path.",
                sections: [0],
                color: "#667eea",
                colorDark: "#764ba2"
            },
            {
                id: 1,
                name: "Data Foundations",
                icon: "üìù",
                why: "Everything starts with data. You need to understand how computers store and process text before understanding AI.",
                sections: [1, 2, 3],
                color: "#2196F3",
                colorDark: "#1976D2"
            },
            {
                id: 2,
                name: "Tokenization",
                icon: "üî§",
                why: "AI can't process whole sentences at once. Tokenization is the first step in converting text to something AI understands.",
                sections: [4, 5],
                color: "#4CAF50",
                colorDark: "#388E3C"
            },
            {
                id: 3,
                name: "Vectors & Embeddings",
                icon: "üìä",
                why: "Tokens are just IDs. Vectors add meaning - making 'cat' and 'dog' mathematically similar because they're both animals.",
                sections: [6, 7, 8],
                color: "#009688",
                colorDark: "#00796B"
            },
            {
                id: 4,
                name: "Model Parameters",
                icon: "üíæ",
                why: "Parameters are the AI's learned knowledge - like a student's notes after years of study.",
                sections: [9, 10],
                color: "#FF9800",
                colorDark: "#F57C00"
            },
            {
                id: 5,
                name: "Transformer Blocks",
                icon: "‚öôÔ∏è",
                why: "Transformers are the engine - they take vectors and find patterns, relationships, and meaning.",
                sections: [11, 12, 13],
                color: "#F44336",
                colorDark: "#D32F2F"
            },
            {
                id: 6,
                name: "Complete Pipeline",
                icon: "ü§ñ",
                why: "See the complete journey from your typed text to AI's response.",
                sections: [14, 15],
                color: "#E91E63",
                colorDark: "#C2185B"
            },
            {
                id: 7,
                name: "Efficiency",
                icon: "üîß",
                why: "Full models are huge (140GB). Quantization makes them fit on your laptop.",
                sections: [16, 17, 18],
                color: "#9C27B0",
                colorDark: "#7B1FA2"
            },
            {
                id: 8,
                name: "Deployment",
                icon: "üöÄ",
                why: "Now that you understand how it works, learn how to actually use it.",
                sections: [19, 20, 21],
                color: "#607D8B",
                colorDark: "#455A64"
            }
        ];

        // Feed flow steps
        const feedFlowSteps = [
            { icon: "üìù", label: "Text Input", sections: [1, 2, 3] },
            { icon: "üî§", label: "Tokenize", sections: [4, 5] },
            { icon: "üìä", label: "Vectors", sections: [6, 7, 8] },
            { icon: "üîÆ", label: "Embed", sections: [9, 10] },
            { icon: "‚öôÔ∏è", label: "Transform", sections: [11, 12, 13] },
            { icon: "üéØ", label: "Generate", sections: [14, 15] },
            { icon: "üöÄ", label: "Deploy", sections: [16, 17, 18, 19, 20, 21] }
        ];

        // Current section index
        let currentSection = 0;

        // Function to render category navigation
        function renderCategoryNav() {
            const nav = document.getElementById('categoryNav');
            nav.innerHTML = '';

            categories.forEach(category => {
                const categoryDiv = document.createElement('div');
                categoryDiv.className = 'category-item';
                categoryDiv.style.setProperty('--category-color', category.color);
                categoryDiv.style.setProperty('--category-color-dark', category.colorDark);

                const header = document.createElement('div');
                header.className = 'category-header';
                header.innerHTML = `
                    <div class="category-title">
                        <span class="category-icon">${category.icon}</span>
                        <span>${category.name}</span>
                    </div>
                    <span class="category-toggle">‚ñº</span>
                `;

                const why = document.createElement('div');
                why.className = 'category-why';
                why.textContent = category.why;

                const sectionsDiv = document.createElement('div');
                sectionsDiv.className = 'category-sections';

                category.sections.forEach(sectionId => {
                    const sectionIndex = sections.findIndex(s => s.id === sectionId);
                    if (sectionIndex !== -1) {
                        const sectionLink = document.createElement('div');
                        sectionLink.className = 'section-link';
                        sectionLink.textContent = `${sectionId}. ${sections[sectionIndex].title}`;
                        sectionLink.style.setProperty('--category-color', category.color);
                        sectionLink.onclick = () => {
                            showSection(sectionIndex);
                        };
                        sectionsDiv.appendChild(sectionLink);
                    }
                });

                header.onclick = () => {
                    categoryDiv.classList.toggle('expanded');
                    const toggle = header.querySelector('.category-toggle');
                    toggle.classList.toggle('expanded');
                };

                categoryDiv.appendChild(header);
                categoryDiv.appendChild(why);
                categoryDiv.appendChild(sectionsDiv);
                nav.appendChild(categoryDiv);
            });
        }

        // Function to render feed flow
        function renderFeedFlow(currentSectionId) {
            const flow = document.getElementById('feedFlow');
            flow.innerHTML = '';

            feedFlowSteps.forEach((step, index) => {
                const stepDiv = document.createElement('div');
                stepDiv.className = 'feed-step';

                // Check if this step is active
                const isActive = step.sections.includes(currentSectionId);

                // Check if this step is completed
                const isCompleted = step.sections.every(sid => {
                    const sIndex = sections.findIndex(s => s.id === sid);
                    return sIndex !== -1 && sIndex < currentSection;
                });

                if (isActive) {
                    stepDiv.classList.add('active');
                } else if (isCompleted) {
                    stepDiv.classList.add('completed');
                }

                stepDiv.innerHTML = `
                    <div class="feed-step-icon">${step.icon}</div>
                    <div class="feed-step-label">${step.label}</div>
                    <div class="feed-step-sections">Sections ${step.sections[0]}-${step.sections[step.sections.length - 1]}</div>
                `;

                flow.appendChild(stepDiv);
            });
        }

        // Function to show section
        function showSection(index) {
            if (index < 0 || index >= sections.length) return;

            currentSection = index;
            const section = sections[index];

            // Update stage badge
            const badge = document.getElementById('stageBadge');
            badge.textContent = `Section ${section.id} of ${sections.length}`;

            // Update content
            const content = document.getElementById('content');
            content.innerHTML = section.content;

            // Update navigation buttons
            document.getElementById('prevBtn').disabled = index === 0;
            document.getElementById('nextBtn').disabled = index === sections.length - 1;

            // Update progress
            updateProgress();

            // Update feed flow
            renderFeedFlow(section.id);

            // Update category nav active state
            updateCategoryNavActive(section.id);

            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Function to update category nav active state
        function updateCategoryNavActive(sectionId) {
            const allLinks = document.querySelectorAll('.section-link');
            allLinks.forEach(link => {
                link.classList.remove('active');
                const linkSectionId = parseInt(link.textContent.split('.')[0]);
                if (linkSectionId === sectionId) {
                    link.classList.add('active');

                    // Expand parent category
                    const categoryItem = link.closest('.category-item');
                    if (categoryItem && !categoryItem.classList.contains('expanded')) {
                        categoryItem.classList.add('expanded');
                        const toggle = categoryItem.querySelector('.category-toggle');
                        if (toggle) toggle.classList.add('expanded');
                    }
                }
            });
        }

        // Function to navigate sections
        function navigateSection(direction) {
            const newIndex = currentSection + direction;
            if (newIndex >= 0 && newIndex < sections.length) {
                showSection(newIndex);
            }
        }

        // Function to update progress
        function updateProgress() {
            const progress = ((currentSection + 1) / sections.length) * 100;
            document.getElementById('progressFill').style.width = progress + '%';
            document.getElementById('progressText').textContent = `Progress: ${Math.round(progress)}%`;
        }

        // Function to generate quiz HTML
        function generateQuiz(quizzes) {
            if (!quizzes || quizzes.length === 0) return '';

            let html = '<div class="quiz-container"><h4>üéØ Test Your Understanding</h4>';

            quizzes.forEach((quiz, qIndex) => {
                html += `
                    <div class="quiz-question" data-question="${qIndex}">
                        <p>${qIndex + 1}. ${quiz.question}</p>
                        <div class="quiz-options">
                `;

                quiz.options.forEach((option, oIndex) => {
                    html += `
                        <div class="quiz-option" onclick="checkAnswer(${qIndex}, ${oIndex}, ${JSON.stringify(quiz).replace(/"/g, '&quot;')})">
                            ${String.fromCharCode(65 + oIndex)}. ${option}
                        </div>
                    `;
                });

                html += `
                        </div>
                        <div class="quiz-feedback" id="feedback-${qIndex}"></div>
                    </div>
                `;
            });

            html += '</div>';
            return html;
        }

        // Function to check quiz answer
        function checkAnswer(questionIndex, selectedOption, quizData) {
            const questionDiv = document.querySelector(`[data-question="${questionIndex}"]`);
            const options = questionDiv.querySelectorAll('.quiz-option');
            const feedback = document.getElementById(`feedback-${questionIndex}`);

            // Remove previous selections
            options.forEach(opt => {
                opt.classList.remove('selected', 'correct', 'incorrect');
            });

            // Mark selected option
            options[selectedOption].classList.add('selected');

            // Check if correct
            const isCorrect = selectedOption === quizData.correct;

            if (isCorrect) {
                options[selectedOption].classList.add('correct');
                feedback.className = 'quiz-feedback correct show';
                feedback.textContent = '‚úì Correct! ' + quizData.explanation;
            } else {
                options[selectedOption].classList.add('incorrect');
                options[quizData.correct].classList.add('correct');
                feedback.className = 'quiz-feedback incorrect show';
                feedback.textContent = '‚úó Not quite. ' + quizData.explanation;
            }
        }

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            // Only initialize if sections exist
            if (sections.length > 0) {
                renderCategoryNav();
                renderFeedFlow(sections[0].id);
                showSection(0);
            } else {
                // Show placeholder content
                document.getElementById('content').innerHTML = `
                    <h2>Welcome to the LLM Learning Guide V4</h2>
                    <p>This interactive guide is ready to receive content. The framework includes:</p>
                    <ul>
                        <li>Three-column responsive layout</li>
                        <li>Category-based navigation with expandable sections</li>
                        <li>Visual data flow pipeline tracker</li>
                        <li>Progress tracking system</li>
                        <li>Interactive quiz system</li>
                        <li>Glass morphism design with modern styling</li>
                    </ul>
                    <p>Content will be added in phases 2-9.</p>
                `;
                document.getElementById('stageBadge').textContent = 'Framework Ready';
                document.getElementById('prevBtn').disabled = true;
                document.getElementById('nextBtn').disabled = true;
            }
        });

        // Event listeners for navigation buttons
        document.getElementById('prevBtn').addEventListener('click', () => navigateSection(-1));
        document.getElementById('nextBtn').addEventListener('click', () => navigateSection(1));
    </script>
</body>
</html>