<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y03EXMZW8F"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-Y03EXMZW8F');
    </script>
    <!-- End Google Analytics -->

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding LLMs: A Complete Interactive Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            user-select: none;
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        .container {
            display: flex;
            height: 100vh;
        }

        .sidebar {
            width: 300px;
            background: #2c3e50;
            color: white;
            overflow-y: auto;
            padding: 20px;
        }

        .sidebar h1 {
            font-size: 1.5em;
            margin-bottom: 20px;
            color: #3498db;
        }

        .toc-item {
            padding: 10px;
            margin: 5px 0;
            cursor: pointer;
            border-radius: 5px;
            transition: background 0.3s;
            font-size: 0.9em;
        }

        .toc-item:hover {
            background: #34495e;
        }

        .toc-item.active {
            background: #3498db;
            font-weight: bold;
        }

        .main-content {
            flex: 1;
            overflow-y: auto;
            padding: 40px;
            background: white;
        }

        .progress-bar {
            position: sticky;
            top: 0;
            height: 4px;
            background: #e0e0e0;
            margin-bottom: 20px;
            border-radius: 2px;
            overflow: hidden;
            z-index: 100;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #3498db, #2ecc71);
            transition: width 0.3s;
        }


        /* Learning Path Flow */
        .learning-path-flow {
            position: sticky;
            top: 4px;
            background: white;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            z-index: 99;
            overflow-x: auto;
            white-space: nowrap;
        }

        .learning-path-flow h3 {
            font-size: 1.1em;
            margin-bottom: 15px;
            color: #2c3e50;
            text-align: center;
        }

        .flow-stages {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            flex-wrap: wrap;
        }

        .flow-stage {
            display: inline-flex;
            align-items: center;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
            transition: all 0.3s;
            opacity: 0.5;
            border: 2px solid transparent;
        }

        .flow-stage.active {
            opacity: 1;
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }

        .flow-stage.foundations { background: #e1d4f7; color: #6c3fb5; }
        .flow-stage.foundations.active { border-color: #6c3fb5; }

        .flow-stage.tokens { background: #d4e4f7; color: #2563eb; }
        .flow-stage.tokens.active { border-color: #2563eb; }

        .flow-stage.vectors { background: #d4f7e1; color: #059669; }
        .flow-stage.vectors.active { border-color: #059669; }

        .flow-stage.layers { background: #ffe4cc; color: #ea580c; }
        .flow-stage.layers.active { border-color: #ea580c; }

        .flow-stage.embeddings { background: #ccf7f3; color: #0d9488; }
        .flow-stage.embeddings.active { border-color: #0d9488; }

        .flow-stage.dimensions { background: #fce7f3; color: #db2777; }
        .flow-stage.dimensions.active { border-color: #db2777; }

        .flow-stage.summary { background: #e5e7eb; color: #4b5563; }
        .flow-stage.summary.active { border-color: #4b5563; }

        .flow-arrow {
            color: #94a3b8;
            font-size: 1.2em;
            margin: 0 5px;
        }

        /* Breadcrumb Navigation */
        .breadcrumb {
            background: #f8fafc;
            padding: 12px 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            font-size: 0.9em;
            color: #64748b;
        }

        .breadcrumb a {
            color: #3498db;
            text-decoration: none;
            font-weight: 600;
        }

        .breadcrumb a:hover {
            text-decoration: underline;
        }

        .breadcrumb-separator {
            margin: 0 8px;
            color: #cbd5e1;
        }

        /* Stage Indicators in TOC */
        .stage-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            flex-shrink: 0;
            display: inline-block;
            margin-right: 8px;
        }

        .stage-dot.foundations { background: #6c3fb5; }
        .stage-dot.tokens { background: #2563eb; }
        .stage-dot.vectors { background: #059669; }
        .stage-dot.layers { background: #ea580c; }
        .stage-dot.embeddings { background: #0d9488; }
        .stage-dot.dimensions { background: #db2777; }
        .stage-dot.summary { background: #4b5563; }

        /* Quiz Section */
        .quiz-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 30px;
            border-radius: 15px;
            margin: 40px 0;
            color: white;
        }

        .quiz-section h3 {
            color: white;
            margin-bottom: 20px;
            font-size: 1.8em;
            text-align: center;
        }

        .quiz-question {
            background: rgba(255,255,255,0.1);
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            backdrop-filter: blur(10px);
        }

        .quiz-question h4 {
            color: white;
            margin-bottom: 15px;
            font-size: 1.2em;
        }

        .quiz-options {
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        .quiz-option {
            background: rgba(255,255,255,0.2);
            padding: 15px;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
            border: 2px solid transparent;
        }

        .quiz-option:hover {
            background: rgba(255,255,255,0.3);
            transform: translateX(5px);
        }

        .quiz-option.selected {
            border-color: white;
            background: rgba(255,255,255,0.3);
        }

        .quiz-option.correct {
            background: #10b981;
            border-color: #059669;
        }

        .quiz-option.incorrect {
            background: #ef4444;
            border-color: #dc2626;
        }

        .quiz-feedback {
            margin-top: 15px;
            padding: 15px;
            border-radius: 8px;
            font-weight: 600;
            display: none;
        }

        .quiz-feedback.show {
            display: block;
        }

        .quiz-feedback.correct {
            background: #10b981;
        }

        .quiz-feedback.incorrect {
            background: #ef4444;
        }

        .quiz-score {
            text-align: center;
            font-size: 1.2em;
            margin-top: 20px;
            padding: 15px;
            background: rgba(255,255,255,0.2);
            border-radius: 10px;
        }

        .quiz-review-link {
            color: #fbbf24;
            text-decoration: underline;
            cursor: pointer;
            font-weight: bold;
        }


        .section {
            display: none;
            animation: fadeIn 0.5s;
        }

        .section.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h1 {
            color: #2c3e50;
            margin-bottom: 20px;
            font-size: 2.5em;
        }

        h2 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 2em;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }

        h3 {
            color: #34495e;
            margin-top: 25px;
            margin-bottom: 10px;
            font-size: 1.5em;
        }

        h4 {
            color: #555;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        .user-question {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .user-question::before {
            content: "💭 Your Question: ";
            font-weight: bold;
            color: #856404;
        }

        pre {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.5;
        }

        code {
            font-family: 'Courier New', monospace;
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            color: #e74c3c;
        }

        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        th, td {
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }

        th {
            background: #3498db;
            color: white;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .success-box {
            background: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .critical-box {
            background: #ffebee;
            border-left: 4px solid #f44336;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 2px solid #e0e0e0;
        }

        .btn {
            padding: 12px 24px;
            background: #3498db;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 1em;
            transition: background 0.3s;
        }

        .btn:hover {
            background: #2980b9;
        }

        .btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
        }

        ul, ol {
            margin-left: 30px;
            margin-top: 10px;
            margin-bottom: 10px;
        }

        li {
            margin: 8px 0;
        }

        strong {
            color: #2c3e50;
        }

        hr {
            border: none;
            border-top: 2px solid #e0e0e0;
            margin: 30px 0;
        }

        .copyright-footer {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
            margin-top: 50px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .copyright-footer h3 {
            color: white;
            margin-bottom: 10px;
        }

        .copyright-footer p {
            margin: 5px 0;
            opacity: 0.9;
        }

        .no-copy-warning {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: rgba(0,0,0,0.9);
            color: white;
            padding: 30px;
            border-radius: 10px;
            display: none;
            z-index: 10000;
            text-align: center;
        }

        @media (max-width: 768px) {
            .container {
                flex-direction: column;
            }
            .sidebar {
                width: 100%;
                height: auto;
            }
            .main-content {
                padding: 20px;
            }
        }
    
        /* ==================== V3 ENHANCEMENTS ==================== */

        /* AI Sparkle Animation */
        @keyframes sparkle {
            0%, 100% {
                transform: scale(1) rotate(0deg);
                opacity: 1;
                filter: drop-shadow(0 0 8px rgba(52, 152, 219, 0.6));
            }
            25% {
                transform: scale(1.15) rotate(90deg);
                opacity: 0.9;
                filter: drop-shadow(0 0 12px rgba(52, 152, 219, 0.8));
            }
            50% {
                transform: scale(1.25) rotate(180deg);
                opacity: 0.8;
                filter: drop-shadow(0 0 16px rgba(52, 152, 219, 1));
            }
            75% {
                transform: scale(1.15) rotate(270deg);
                opacity: 0.9;
                filter: drop-shadow(0 0 12px rgba(52, 152, 219, 0.8));
            }
        }

        .ai-sparkle {
            display: inline-block;
            animation: sparkle 3s ease-in-out infinite;
        }

        /* Glass Morphism for Visual Demos */
        .visual-demo {
            background: rgba(255, 255, 255, 0.15);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.3);
            border-radius: 20px;
            padding: 30px;
            box-shadow:
                0 8px 32px rgba(0, 0, 0, 0.1),
                inset 0 1px 0 rgba(255, 255, 255, 0.5);
            margin: 30px 0;
            position: relative;
            overflow: hidden;
        }

        .visual-demo::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.2), transparent);
            animation: shimmer 3s infinite;
        }

        @keyframes shimmer {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        /* Tokenization Animation */
        .tokenization-demo {
            background: linear-gradient(135deg, #667eea20 0%, #764ba220 100%);
            text-align: center;
        }

        .token-input {
            font-size: 1.5em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 20px;
            animation: fadeIn 1s;
        }

        .token-arrows {
            font-size: 2em;
            color: #3498db;
            margin: 20px 0;
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.2); opacity: 0.7; }
        }

        .token-output {
            display: flex;
            justify-content: center;
            gap: 10px;
            flex-wrap: wrap;
        }

        .token {
            background: #3498db;
            color: white;
            padding: 10px 20px;
            border-radius: 8px;
            font-weight: 600;
            animation: tokenPop 0.6s ease-out;
            animation-fill-mode: both;
        }

        .token:nth-child(1) { animation-delay: 0.2s; }
        .token:nth-child(2) { animation-delay: 0.4s; }
        .token:nth-child(3) { animation-delay: 0.6s; }

        @keyframes tokenPop {
            0% { transform: scale(0); opacity: 0; }
            70% { transform: scale(1.1); }
            100% { transform: scale(1); opacity: 1; }
        }

        /* Vector Transformation Animation */
        .vector-demo {
            background: linear-gradient(135deg, #11998e20 0%, #38ef7d20 100%);
            display: flex;
            align-items: center;
            justify-content: space-around;
            min-height: 150px;
        }

        .word-input {
            font-size: 2em;
            font-weight: bold;
            color: #11998e;
            animation: float 3s ease-in-out infinite;
        }

        @keyframes float {
            0%, 100% { transform: translateY(0); }
            50% { transform: translateY(-10px); }
        }

        .transform-arrow {
            font-size: 3em;
            color: #38ef7d;
            animation: slideRight 2s ease-in-out infinite;
        }

        @keyframes slideRight {
            0%, 100% { transform: translateX(0); opacity: 1; }
            50% { transform: translateX(20px); opacity: 0.5; }
        }

        .vector-output {
            flex: 1;
            max-width: 400px;
        }

        .vector-numbers {
            font-family: 'Courier New', monospace;
            background: rgba(17, 153, 142, 0.2);
            padding: 15px;
            border-radius: 10px;
            color: #11998e;
            font-weight: 600;
            animation: numberGlow 2s ease-in-out infinite;
        }

        @keyframes numberGlow {
            0%, 100% { box-shadow: 0 0 10px rgba(17, 153, 142, 0.3); }
            50% { box-shadow: 0 0 20px rgba(17, 153, 142, 0.6); }
        }

        /* Embedding Space 3D Visualization */
        .embedding-demo {
            background: linear-gradient(135deg, #667eea20 0%, #764ba220 100%);
            min-height: 300px;
            position: relative;
            perspective: 1000px;
        }

        .embedding-space {
            width: 100%;
            height: 300px;
            position: relative;
            transform-style: preserve-3d;
            animation: rotate3D 20s linear infinite;
        }

        @keyframes rotate3D {
            0% { transform: rotateY(0deg); }
            100% { transform: rotateY(360deg); }
        }

        .vector-point {
            position: absolute;
            padding: 10px 20px;
            background: rgba(102, 126, 234, 0.8);
            color: white;
            border-radius: 20px;
            font-weight: 600;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.5);
            animation: pointPulse 3s ease-in-out infinite;
        }

        .vector-point[data-word="cat"] {
            top: 30%;
            left: 20%;
            animation-delay: 0s;
        }

        .vector-point[data-word="dog"] {
            top: 35%;
            left: 30%;
            animation-delay: 0.5s;
        }

        .vector-point[data-word="king"] {
            top: 60%;
            left: 50%;
            animation-delay: 1s;
        }

        @keyframes pointPulse {
            0%, 100% {
                transform: scale(1) translateZ(0);
                box-shadow: 0 4px 15px rgba(102, 126, 234, 0.5);
            }
            50% {
                transform: scale(1.15) translateZ(50px);
                box-shadow: 0 8px 30px rgba(102, 126, 234, 0.8);
            }
        }

        /* Transformer Layers Data Flow */
        .transformer-demo {
            background: linear-gradient(135deg, #f093fb20 0%, #f5576c20 100%);
            padding: 40px 20px;
        }

        .data-flow {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0;
        }

        .layer {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 20px 60px;
            border-radius: 15px;
            font-weight: 600;
            font-size: 1.1em;
            position: relative;
            animation: layerGlow 2s ease-in-out infinite;
        }

        .layer:nth-child(even) { animation-delay: 0.5s; }

        @keyframes layerGlow {
            0%, 100% {
                box-shadow: 0 4px 20px rgba(240, 147, 251, 0.4);
                transform: scale(1);
            }
            50% {
                box-shadow: 0 8px 40px rgba(240, 147, 251, 0.7);
                transform: scale(1.05);
            }
        }

        .flow-arrow {
            font-size: 2.5em;
            color: #f5576c;
            margin: 10px 0;
            animation: flowDown 1.5s ease-in-out infinite;
        }

        @keyframes flowDown {
            0%, 100% { transform: translateY(0); opacity: 0.7; }
            50% { transform: translateY(10px); opacity: 1; }
        }

        /* Feed Flow Visualization - Right Sidebar */
        .feed-flow-container {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 20px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
            max-width: 200px;
            z-index: 1000;
        }

        .feed-step {
            text-align: center;
            padding: 10px;
            border-radius: 10px;
            margin: 5px 0;
            transition: all 0.3s;
            opacity: 0.5;
        }

        .feed-step.active {
            opacity: 1;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            transform: scale(1.1);
        }

        .feed-icon {
            font-size: 1.5em;
            margin-bottom: 5px;
        }

        .feed-label {
            font-size: 0.75em;
            font-weight: 600;
        }

        .feed-example {
            font-size: 0.65em;
            font-family: 'Courier New', monospace;
            margin-top: 5px;
            opacity: 0.8;
        }

        .feed-connector {
            text-align: center;
            color: #3498db;
            font-size: 1.2em;
            margin: 0;
        }

        /* New Stage Colors */
        .flow-stage.text { background: #e3f2fd; color: #2196F3; }
        .flow-stage.text.active { border-color: #2196F3; }

        .flow-stage.tokenization { background: #e8f5e9; color: #4CAF50; }
        .flow-stage.tokenization.active { border-color: #4CAF50; }

        .flow-stage.transformer { background: #ffebee; color: #F44336; }
        .flow-stage.transformer.active { border-color: #F44336; }

        .flow-stage.complete { background: #fce4ec; color: #E91E63; }
        .flow-stage.complete.active { border-color: #E91E63; }

        .flow-stage.deployment { background: #eceff1; color: #607D8B; }
        .flow-stage.deployment.active { border-color: #607D8B; }

        /* Stage dots for TOC */
        .stage-dot.text { background: #2196F3; }
        .stage-dot.tokenization { background: #4CAF50; }
        .stage-dot.transformer { background: #F44336; }
        .stage-dot.complete { background: #E91E63; }
        .stage-dot.deployment { background: #607D8B; }

        /* Mobile Responsiveness */
        @media (max-width: 1200px) {
            .feed-flow-container {
                display: none;
            }
        }

        @media (max-width: 768px) {
            .visual-demo {
                padding: 20px 15px;
            }

            .vector-demo {
                flex-direction: column;
                gap: 20px;
            }

            .transformer-demo .layer {
                padding: 15px 30px;
                font-size: 0.9em;
            }

            .embedding-space {
                height: 200px;
            }
        }
    </style>
</head>
<body>
    <div class="no-copy-warning" id="copyWarning">
        <h2>⚠️ Copyright Protected</h2>
        <p>This content is copyrighted by Arul.</p>
        <p>Unauthorized copying or distribution is prohibited.</p>
    </div>

    <div class="container">
        <div class="sidebar">
            <h1>📚 LLM Learning Guide</h1>
            <div id="toc"></div>
        </div>

        <div class="main-content">
            <div class="progress-bar">
                <div class="progress-fill" id="progressBar"></div>
            </div>
            
            <!-- Learning Path Flow -->
            <div class="learning-path-flow" id="learningPathFlow"></div>

            <!-- Breadcrumb -->
            <div class="breadcrumb" id="breadcrumb"></div>

            <div id="content"></div>

            <div class="nav-buttons">
                <button class="btn" id="prevBtn" onclick="navigateSection(-1)">← Previous</button>
                <button class="btn" id="nextBtn" onclick="navigateSection(1)">Next →</button>
            </div>

            <div class="copyright-footer">
                <h3>© 2025 Arul - All Rights Reserved</h3>
                <p>📚 Complete Guide to Understanding Large Language Models</p>
                <p>⚖️ This educational material is protected by copyright law.</p>
                <p>Unauthorized reproduction, distribution, or modification is strictly prohibited.</p>
            </div>
        </div>
    </div>

    <script>
        // Copyright protection
        (function() {
            console.clear();
            console.log('%c⚠️ COPYRIGHT NOTICE', 'color: red; font-size: 20px; font-weight: bold;');
            console.log('%c© 2025 Arul - All Rights Reserved', 'color: #3498db; font-size: 14px;');
            console.log('%cThis content is protected by copyright law.', 'color: #333; font-size: 12px;');
            console.log('%cUnauthorized access to this code is prohibited.', 'color: #e74c3c; font-size: 12px;');
        })();

        // Disable right-click
        document.addEventListener('contextmenu', function(e) {
            e.preventDefault();
            showCopyWarning();
            return false;
        });

        // Disable key combinations
        document.addEventListener('keydown', function(e) {
            // F12, Ctrl+Shift+I, Ctrl+Shift+J, Ctrl+U, Ctrl+Shift+C
            if (e.key === 'F12' || 
                (e.ctrlKey && e.shiftKey && (e.key === 'I' || e.key === 'J' || e.key === 'C')) ||
                (e.ctrlKey && e.key === 'u') ||
                (e.ctrlKey && e.key === 's')) {
                e.preventDefault();
                showCopyWarning();
                return false;
            }
        });

        // Disable text selection on specific elements
        document.addEventListener('selectstart', function(e) {
            if (e.target.tagName !== 'INPUT' && e.target.tagName !== 'TEXTAREA') {
                e.preventDefault();
            }
        });

        function showCopyWarning() {
            const warning = document.getElementById('copyWarning');
            warning.style.display = 'block';
            setTimeout(() => {
                warning.style.display = 'none';
            }, 2000);
        }

        let currentSection = 0;
        
        const sections = [
            {
                title: "1. Running LLMs Locally",
                content: `
                    <h1>Running LLMs Locally: Getting Started with GGUF Files</h1>
                    
                    <div class="user-question">
                    If I have a GGUF file, how to run in my machines?
                    </div>

                    <p>To run a GGUF file, you have several options depending on your preference for command-line or GUI tools:</p>

                    <h2>Easiest Options (GUI)</h2>

                    <h3>LM Studio (Recommended for beginners)</h3>
                    <ul>
                        <li>Download from <a href="https://lmstudio.ai" target="_blank">lmstudio.ai</a></li>
                        <li>Install and open the app</li>
                        <li>Click "Load Model" and select your GGUF file</li>
                        <li>Start chatting in the interface</li>
                        <li>Works on Windows, Mac, and Linux</li>
                    </ul>

                    <h3>Ollama (Simple CLI)</h3>
                    <pre><code># Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Create a Modelfile
echo 'FROM /path/to/your/model.gguf' > Modelfile

# Create and run the model
ollama create mymodel -f Modelfile
ollama run mymodel</code></pre>

                    <h2>Command Line (llama.cpp)</h2>

                    <pre><code># Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Run inference
./main -m /path/to/your/model.gguf -p "Your prompt here" -n 512</code></pre>

                    <p>For GPU acceleration, you'll need to compile with appropriate flags (<code>LLAMA_CUDA=1</code> for NVIDIA, <code>LLAMA_METAL=1</code> for Mac M1/M2, etc.)</p>

                    <h2>Which Should You Choose?</h2>
                    <ul>
                        <li><strong>New to local LLMs?</strong> → LM Studio</li>
                        <li><strong>Want simple CLI?</strong> → Ollama</li>
                        <li><strong>Need maximum control?</strong> → llama.cpp</li>
                        <li><strong>Want a web UI?</strong> → Text generation web UI (oobabooga)</li>
                    </ul>

                    <div class="info-box">
                    <strong>💡 Tip:</strong> What type of machine are you running (Windows/Mac/Linux) and do you have a GPU? This will help determine the best option for you.
                    </div>
                `
            },
            {
                title: "2. LLM Tools & Inference Engines",
                content: `
                    <h1>Understanding LLM Tools and Inference Engines</h1>
                    
                    <div class="user-question">
                    What are these tools called like LM Studio, Llama.cpp etc?
                    </div>

                    <p>These tools are called <strong>LLM inference engines</strong> or <strong>local LLM runners</strong>. They're software that lets you run large language models on your own computer rather than through a cloud API.</p>

                    <h2>How They're Categorized</h2>

                    <h3>Core Types:</h3>

                    <h4>Inference Engines (the underlying technology)</h4>
                    <ul>
                        <li><strong>llama.cpp</strong> - The foundational C++ library that most others build on</li>
                        <li><strong>vLLM</strong> - High-performance inference server</li>
                        <li><strong>GGML</strong> - The original library (GGUF is the file format from this)</li>
                    </ul>

                    <h4>User-Friendly Frontends (easier interfaces on top of inference engines)</h4>
                    <ul>
                        <li><strong>LM Studio</strong> - GUI desktop app</li>
                        <li><strong>Ollama</strong> - Simple CLI tool (uses llama.cpp under the hood)</li>
                        <li><strong>GPT4All</strong> - Another desktop GUI</li>
                        <li><strong>Jan</strong> - Open-source desktop app</li>
                    </ul>

                    <h4>Web UIs (browser-based interfaces)</h4>
                    <ul>
                        <li><strong>Text generation web UI</strong> (oobabooga)</li>
                        <li><strong>SillyTavern</strong> - For chatbot/roleplay</li>
                        <li><strong>Open WebUI</strong> - Works with Ollama</li>
                    </ul>

                    <h2>Why So Many Options?</h2>

                    <p>They all do the same basic thing (run LLMs locally) but differ in:</p>
                    <ul>
                        <li><strong>Ease of use</strong> - LM Studio vs raw llama.cpp</li>
                        <li><strong>Performance optimization</strong> - CPU vs GPU acceleration</li>
                        <li><strong>Features</strong> - Chat interface, API servers, multi-model support</li>
                        <li><strong>Platform support</strong> - Windows, Mac, Linux</li>
                    </ul>

                    <div class="info-box">
                    <strong>💡 Simple Terms:</strong> Most people just call them <strong>"local LLM tools"</strong> or <strong>"inference engines"</strong> in casual conversation.
                    </div>
                `
            },
            {
                title: "3. Enterprise vs Personal Deployment",
                content: `
                    <h1>Enterprise-Level LLM Deployment</h1>
                    
                    <div class="user-question">
                    How is it run in enterprise level? What are inference engines or runners?
                    </div>

                    <h2>What are Inference Engines?</h2>

                    <p>An <strong>inference engine</strong> is software that takes a trained AI model and runs it to generate predictions/responses. Think of it like this:</p>

                    <ul>
                        <li><strong>Training</strong> = Teaching the model (requires massive compute, done once)</li>
                        <li><strong>Inference</strong> = Using the trained model to answer questions (what you do repeatedly)</li>
                    </ul>

                    <p>The inference engine:</p>
                    <ol>
                        <li>Loads the model weights into memory</li>
                        <li>Processes your input (prompt)</li>
                        <li>Runs mathematical operations to generate output</li>
                        <li>Manages memory, batching, and optimization</li>
                    </ol>

                    <h2>Enterprise Inference Platforms</h2>

                    <h3>vLLM (Very popular)</h3>
                    <ul>
                        <li>High-throughput serving</li>
                        <li>Handles multiple requests simultaneously</li>
                        <li>Advanced batching and memory management</li>
                        <li>Used by many companies</li>
                    </ul>

                    <h3>NVIDIA Triton Inference Server</h3>
                    <ul>
                        <li>Multi-model serving</li>
                        <li>Supports multiple frameworks</li>
                        <li>GPU optimization</li>
                        <li>Production monitoring</li>
                    </ul>

                    <h3>TensorRT-LLM</h3>
                    <ul>
                        <li>NVIDIA's optimized inference engine</li>
                        <li>Maximum GPU performance</li>
                        <li>Used for large-scale deployments</li>
                    </ul>

                    <h2>Deployment Architecture</h2>

                    <pre><code>Enterprise Setup:
├── Load Balancer
├── API Gateway (authentication, rate limiting)
├── Inference Cluster
│   ├── Multiple GPU servers
│   ├── Model replicas for redundancy
│   └── Auto-scaling based on traffic
├── Caching Layer (for common queries)
├── Monitoring & Logging
└── Model Management System</code></pre>

                    <h2>Key Differences: Personal vs Enterprise</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>Personal (GGUF)</th>
                                <th>Enterprise</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Hardware</strong></td>
                                <td>Single machine</td>
                                <td>Cluster of GPU servers</td>
                            </tr>
                            <tr>
                                <td><strong>Concurrency</strong></td>
                                <td>1-2 users</td>
                                <td>Thousands of simultaneous requests</td>
                            </tr>
                            <tr>
                                <td><strong>Models</strong></td>
                                <td>One at a time</td>
                                <td>Multiple models, A/B testing</td>
                            </tr>
                            <tr>
                                <td><strong>Optimization</strong></td>
                                <td>Quantization (GGUF)</td>
                                <td>Custom kernels, tensor parallelism</td>
                            </tr>
                            <tr>
                                <td><strong>Cost</strong></td>
                                <td>Free/one-time</td>
                                <td>$10K-$1M+/month</td>
                            </tr>
                            <tr>
                                <td><strong>Latency</strong></td>
                                <td>Seconds</td>
                                <td>Milliseconds</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>Enterprise Solutions</h2>

                    <h3>Managed Services:</h3>
                    <ul>
                        <li><strong>AWS SageMaker</strong> - Deploy models on AWS infrastructure</li>
                        <li><strong>Azure ML</strong> - Microsoft's ML platform</li>
                        <li><strong>Google Vertex AI</strong> - Google Cloud's offering</li>
                        <li><strong>Databricks</strong> - For companies using their data platform</li>
                    </ul>

                    <h3>Self-Hosted Platforms:</h3>
                    <ul>
                        <li><strong>Kubernetes + vLLM</strong> - Most common self-hosted setup</li>
                        <li><strong>RunPod, Modal, Replicate</strong> - Serverless GPU platforms</li>
                        <li><strong>BentoML</strong> - ML model serving framework</li>
                    </ul>

                    <h2>Example Enterprise Stack</h2>

                    <pre><code>Large Company Running LLMs:
├── Frontend: React/Next.js app
├── API: FastAPI with rate limiting
├── Inference: vLLM cluster (8x A100 GPUs)
├── Models: Llama 3 70B + specialized fine-tunes
├── Storage: S3 for model weights
├── Monitoring: Prometheus + Grafana
├── Logging: ELK stack
└── Cost: ~$50K-100K/month</code></pre>

                    <div class="critical-box">
                    <strong>⚠️ Key Difference:</strong> Enterprises need 99.9% uptime, sub-second response times, ability to handle traffic spikes, cost optimization, and security/compliance.
                    </div>
                `
            },
            {
                title: "4. Understanding Floating Point",
                content: `
                    <h1>Understanding Floating Point Numbers</h1>
                    
                    <div class="user-question">
                    What is floating point you explained there? And what is FP16, FP32?
                    </div>

                    <h2>The Problem</h2>

                    <p>Computers only understand 0s and 1s (binary). So how do they store decimal numbers like 3.14159?</p>

                    <h2>What is Floating Point?</h2>

                    <p><strong>Floating Point</strong> = A way to represent decimal numbers in binary using limited space</p>

                    <p>Think of <strong>scientific notation</strong>:</p>
                    <ul>
                        <li>123,456,789 = 1.23456789 × 10⁸</li>
                        <li>0.00000123 = 1.23 × 10⁻⁶</li>
                    </ul>

                    <p>Floating point does the same thing in binary!</p>

                    <h2>Structure of a Floating Point Number</h2>

                    <pre><code>FP32 (32-bit floating point):
├── 1 bit: Sign (positive or negative)
├── 8 bits: Exponent (how big/small the number is)
└── 23 bits: Mantissa (the actual digits)

Total: 32 bits = 4 bytes per number</code></pre>

                    <h3>Example: Storing 3.14159</h3>

                    <pre><code>Sign: 0 (positive)
Exponent: (encoded power of 2)
Mantissa: 1.10010010000111111...

Result: Uses 32 bits (4 bytes) to store 3.14159</code></pre>

                    <h2>Common Floating Point Formats</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Format</th>
                                <th>Bits</th>
                                <th>Bytes</th>
                                <th>Range</th>
                                <th>Precision</th>
                                <th>Use Case</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>FP64</strong></td>
                                <td>64</td>
                                <td>8</td>
                                <td>Huge</td>
                                <td>Very precise</td>
                                <td>Scientific computing</td>
                            </tr>
                            <tr>
                                <td><strong>FP32</strong></td>
                                <td>32</td>
                                <td>4</td>
                                <td>Large</td>
                                <td>Good</td>
                                <td>Training AI models</td>
                            </tr>
                            <tr>
                                <td><strong>FP16</strong></td>
                                <td>16</td>
                                <td>2</td>
                                <td>Medium</td>
                                <td>Decent</td>
                                <td>Inference, graphics</td>
                            </tr>
                            <tr>
                                <td><strong>BF16</strong></td>
                                <td>16</td>
                                <td>2</td>
                                <td>Large</td>
                                <td>Less precise</td>
                                <td>Google's AI training</td>
                            </tr>
                            <tr>
                                <td><strong>FP8</strong></td>
                                <td>8</td>
                                <td>1</td>
                                <td>Small</td>
                                <td>Low</td>
                                <td>New GPUs (H100)</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>Visual Comparison</h2>

                    <pre><code>Number: 3.14159265359

FP64: 3.14159265359 (perfect)
FP32: 3.1415927 (tiny error)
FP16: 3.140625 (small error)
FP8:  3.125 (noticeable error)</code></pre>

                    <h2>Why This Matters for AI Models</h2>

                    <p><strong>Llama 3 70B model has 70 billion parameters (numbers/weights)</strong></p>

                    <pre><code>FP32 (original training):
70 billion numbers × 4 bytes = 280 GB

FP16 (half precision):
70 billion numbers × 2 bytes = 140 GB

FP8 (quarter precision):
70 billion numbers × 1 byte = 70 GB</code></pre>

                    <div class="warning-box">
                    <strong>Trade-off:</strong><br>
                    More bits = More accurate but HUGE memory<br>
                    Fewer bits = Less accurate but fits in memory
                    </div>
                `
            },
            {
                title: "5. Understanding Quantization",
                content: `
                    <h1>Quantization: Making Models Smaller</h1>
                    
                    <div class="user-question">
                    What is quantization? How does it work?
                    </div>

                    <h2>What is Quantization?</h2>

                    <p><strong>Quantization</strong> = Converting high-precision numbers to low-precision numbers</p>

                    <p>It's like rounding:</p>
                    <ul>
                        <li>Original: 3.14159265359</li>
                        <li>Rounded: 3.14</li>
                    </ul>

                    <p>You lose some precision, but the number is simpler!</p>

                    <h2>Why Quantize?</h2>

                    <div class="critical-box">
                    <strong>Problem:</strong> Modern AI models are MASSIVE

                    <pre><code>GPT-3 (175B parameters) in FP32:
175 billion × 4 bytes = 700 GB of RAM needed! 💸💸💸

After 4-bit quantization:
175 billion × 0.5 bytes = 87.5 GB
Now runs on a single high-end GPU!</code></pre>
                    </div>

                    <h2>How Quantization Works</h2>

                    <h3>Simple Example:</h3>

                    <pre><code>Original weights (FP16 - 16 bits):
3.1415, 2.7182, 1.4142, 0.5772, 4.6692

Each number uses 16 bits = 80 bits total</code></pre>

                    <h3>After Quantization to 4 bits:</h3>

                    <pre><code>Step 1: Find range
Min: 0.5772, Max: 4.6692

Step 2: Create 16 levels (2^4 = 16 possible values)
Levels: 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, ...

Step 3: Map each weight to nearest level
3.1415 → 3.0
2.7182 → 3.0  
1.4142 → 1.5
0.5772 → 0.5
4.6692 → 4.5

Each number uses 4 bits = 20 bits total
Saved: 75% memory!</code></pre>

                    <h2>Quantization Methods</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Type</th>
                                <th>Bits</th>
                                <th>Levels</th>
                                <th>Quality</th>
                                <th>Memory Saved</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>FP16</strong></td>
                                <td>16</td>
                                <td>65,536</td>
                                <td>Excellent</td>
                                <td>0% (baseline)</td>
                            </tr>
                            <tr>
                                <td><strong>INT8</strong></td>
                                <td>8</td>
                                <td>256</td>
                                <td>Very Good</td>
                                <td>50%</td>
                            </tr>
                            <tr>
                                <td><strong>INT4</strong></td>
                                <td>4</td>
                                <td>16</td>
                                <td>Good</td>
                                <td>75%</td>
                            </tr>
                            <tr>
                                <td><strong>INT3</strong></td>
                                <td>3</td>
                                <td>8</td>
                                <td>Okay</td>
                                <td>81%</td>
                            </tr>
                            <tr>
                                <td><strong>INT2</strong></td>
                                <td>2</td>
                                <td>4</td>
                                <td>Poor</td>
                                <td>87%</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>GGUF Quantization Types</h2>

                    <p>When you download a GGUF file, you'll see names like:</p>

                    <table>
                        <thead>
                            <tr>
                                <th>Code</th>
                                <th>Meaning</th>
                                <th>Quality</th>
                                <th>Use Case</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Q2_K</strong></td>
                                <td>2-bit</td>
                                <td>Lowest</td>
                                <td>Tiny devices</td>
                            </tr>
                            <tr>
                                <td><strong>Q3_K_M</strong></td>
                                <td>3-bit medium</td>
                                <td>Low</td>
                                <td>Mobile</td>
                            </tr>
                            <tr>
                                <td><strong>Q4_K_M</strong></td>
                                <td>4-bit medium</td>
                                <td>Good</td>
                                <td>Most popular</td>
                            </tr>
                            <tr>
                                <td><strong>Q5_K_M</strong></td>
                                <td>5-bit medium</td>
                                <td>Better</td>
                                <td>Balance</td>
                            </tr>
                            <tr>
                                <td><strong>Q6_K</strong></td>
                                <td>6-bit</td>
                                <td>Very good</td>
                                <td>Quality matters</td>
                            </tr>
                            <tr>
                                <td><strong>Q8_0</strong></td>
                                <td>8-bit</td>
                                <td>Excellent</td>
                                <td>Nearly lossless</td>
                            </tr>
                        </tbody>
                    </table>

                    <p><strong>The "K" methods:</strong> More advanced quantization that preserves important weights better</p>

                    <h2>Real Example</h2>

                    <pre><code>Original Model (Llama 3 8B):
Format: FP16
Size: 16 GB
Quality: 100%
Runs on: High-end GPU only

Quantized (Q4_K_M):
Format: 4-bit
Size: 4.7 GB
Quality: ~95-98%
Runs on: M1 Mac, gaming PC, decent laptop

Quantized (Q8_0):
Format: 8-bit  
Size: 8.5 GB
Quality: ~99%
Runs on: Good GPU or Mac</code></pre>

                    <h2>The Trade-off</h2>

                    <pre><code>More Bits → Better Quality → More Memory → Fewer Users Can Run
Fewer Bits → Lower Quality → Less Memory → More Users Can Run</code></pre>

                    <div class="success-box">
                    <strong>Sweet spot for most people:</strong> Q4_K_M or Q5_K_M<br>
                    - 70-80% memory savings<br>
                    - 95-98% quality retention<br>
                    - Runs on consumer hardware
                    </div>
                `
            },
            {
                title: "6. What Gets Quantized",
                content: `
                    <h1>What Exactly Gets Quantized?</h1>
                    
                    <div class="user-question">
                    What do they quantize here? I understand FP now and Memory, but what exactly is quantization happening to?
                    </div>

                    <h2>The Answer: Model Weights (Parameters)</h2>

                    <p>When we quantize, we're quantizing the <strong>model weights</strong> - the billions of numbers that ARE the AI model.</p>

                    <h3>What are Weights?</h3>

                    <p>Think of an AI model like a giant mathematical formula with billions of knobs:</p>

                    <pre><code>Your Input (text) 
    ↓
Goes through billions of math operations
    ↓
Each operation uses stored numbers (weights)
    ↓
Output (response text)</code></pre>

                    <p><strong>These stored numbers are the WEIGHTS</strong> - and that's what we quantize!</p>

                    <h2>Simple Analogy</h2>

                    <div class="info-box">
                    <strong>Recipe analogy:</strong>

                    <pre><code>Original Recipe (High Precision):
- 2.7182818 cups flour
- 1.4142135 cups sugar  
- 0.5772156 teaspoons salt
- 3.1415926 tablespoons butter

Quantized Recipe (Low Precision):
- 2.75 cups flour
- 1.5 cups sugar
- 0.5 teaspoons salt
- 3 tablespoons butter

Still works! Just slightly less precise.</code></pre>
                    </div>

                    <p>The <strong>weights</strong> are like the measurements in a recipe. The AI "learned" these exact numbers during training, and it uses them to generate responses.</p>

                    <h2>What ARE These Weights?</h2>

                    <h3>Neural Network Structure</h3>

                    <pre><code>AI Model = Layers of neurons connected by weights

Input: "What is 2+2?"
    ↓
Layer 1: [neuron1] [neuron2] [neuron3] ... [neuron4096]
    ↓ (connections have weights)
Layer 2: [neuron1] [neuron2] [neuron3] ... [neuron4096]
    ↓ (connections have weights)
Layer 3: [neuron1] [neuron2] [neuron3] ... [neuron4096]
    ↓
... (80 layers in Llama 3 70B)
    ↓
Output: "The answer is 4"</code></pre>

                    <h3>The Weights = Connection Strengths</h3>

                    <pre><code>Example connection:

Neuron A ----[weight: 0.7182]----> Neuron B

This weight (0.7182) means:
"When Neuron A activates, send 71.82% of its signal to Neuron B"

There are BILLIONS of these connections!</code></pre>

                    <h2>Concrete Example with Real Numbers</h2>

                    <h3>Before Quantization (FP16)</h3>

                    <pre><code>Layer 47, Attention Weights (just 5 of billions):

weight[0] = 0.31415926  (16 bits)
weight[1] = -0.27182818 (16 bits)
weight[2] = 0.14142135  (16 bits)
weight[3] = 0.57721566  (16 bits)
weight[4] = -0.69314718 (16 bits)

Memory: 5 weights × 2 bytes = 10 bytes</code></pre>

                    <h3>After 4-bit Quantization</h3>

                    <pre><code>Step 1: Analyze the range
Min: -0.693, Max: 0.577

Step 2: Create 16 levels (4 bits = 2^4 = 16 options)
[-0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 
  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7, 0.8]

Step 3: Round each weight to nearest level
weight[0] = 0.31415926 → 0.3  (level 10)
weight[1] = -0.27182818 → -0.3 (level 4)
weight[2] = 0.14142135 → 0.1  (level 8)
weight[3] = 0.57721566 → 0.6  (level 13)
weight[4] = -0.69314718 → -0.7 (level 0)

Memory: 5 weights × 0.5 bytes = 2.5 bytes
Saved: 75%!</code></pre>

                    <h2>Visual: What's in a Model File</h2>

                    <h3>Inside <code>llama-3-70b.safetensors</code> (Original)</h3>

                    <pre><code>File Structure:
├── Metadata (model info)
└── Weights (the actual model):
    ├── Layer 0 weights:    [0.1234, -0.5678, 0.9012, ...]  (millions)
    ├── Layer 1 weights:    [0.3456, 0.7890, -0.1234, ...]  (millions)
    ├── Layer 2 weights:    [...]
    ├── ...
    └── Layer 79 weights:   [...]

Total: 70 billion numbers (FP16)
Size: 140 GB</code></pre>

                    <h3>Inside <code>llama-3-70b-Q4_K_M.gguf</code> (Quantized)</h3>

                    <pre><code>File Structure:
├── Metadata (model info + quantization info)
└── Weights (compressed):
    ├── Layer 0 weights:    [0.12, -0.56, 0.90, ...]  (quantized to 4-bit)
    ├── Layer 1 weights:    [0.35, 0.79, -0.12, ...]  (quantized to 4-bit)
    ├── Layer 2 weights:    [...]
    ├── ...
    └── Layer 79 weights:   [...]

Total: Same 70 billion numbers (4-bit)
Size: 35 GB</code></pre>

                    <p><strong>Same numbers, just less precise!</strong></p>

                    <h2>Why Quantizing Weights Works</h2>

                    <div class="success-box">
                    <strong>Key Insight:</strong> Most weights don't need extreme precision!

                    <pre><code>Think about it:
If a weight is 0.31415926...
vs
0.3

Does it really matter for "understanding" language?
Usually not!</code></pre>
                    </div>

                    <h3>The Model is Robust</h3>

                    <p>AI models are <strong>over-parameterized</strong> - they have more weights than strictly necessary. So losing a bit of precision in each weight doesn't break the model.</p>

                    <pre><code>Analogy: GPS coordinates

Exact: 37.7749295, -122.4194155 (your house)
Rounded: 37.77, -122.42 (still your neighborhood)

For most purposes, the rounded version works fine!</code></pre>

                    <h2>The Full Picture</h2>

                    <h3>Where Weights Come From:</h3>

                    <pre><code>1. Model Training (costs millions):
   - Start with random weights
   - Feed billions of text examples
   - Adjust weights to predict text better
   - End result: 70 billion "learned" numbers

2. These weights ARE the model:
   Model file = Just a giant list of these numbers

3. Quantization:
   - Take those 70 billion numbers
   - Round them to simpler values
   - Save as GGUF file</code></pre>

                    <h3>Why This Is Huge:</h3>

                    <pre><code>Original weights (FP16):
70,000,000,000 numbers × 2 bytes = 140 GB

Quantized weights (4-bit):
70,000,000,000 numbers × 0.5 bytes = 35 GB

You're literally making 70 billion numbers smaller!</code></pre>

                    <div class="info-box">
                    <strong>Summary:</strong><br>
                    <strong>What gets quantized?</strong> The weights (parameters) - the billions of learned numbers that make up the AI model<br><br>
                    <strong>Where are these weights?</strong> Stored in the model file (.safetensors, .gguf, etc.)<br><br>
                    <strong>Why quantize them?</strong> They take up massive amounts of memory. Most don't need ultra-high precision. Rounding them saves 50-75% memory with minimal quality loss
                    </div>
                `
            },
            {
                title: "7. From Text to Tokens",
                content: `
                    <h1>Tokens: Breaking Text Into Chunks</h1>
                    <div class="visual-demo tokenization-demo">
                        <h3 style="margin-top: 0;">Watch Tokenization in Action</h3>
                        <div class="token-input">"The cat sat"</div>
                        <div class="token-arrows">→</div>
                        <div class="token-output">
                            <span class="token">The</span>
                            <span class="token">cat</span>
                            <span class="token">sat</span>
                        </div>
                    </div>

                    
                    <div class="user-question">
                    Now explain those vector numbers, how did they get there? What are tokens and vectors in this context?
                    </div>

                    <h2>STEP 1: Text → Tokens</h2>

                    <h3>What Are Tokens?</h3>

                    <p><strong>Tokens</strong> = Breaking text into chunks the AI can understand</p>

                    <pre><code>Your text: "The cat sat on the mat"

AI breaks it into tokens:
["The", " cat", " sat", " on", " the", " mat"]

Each token gets a number (token ID):
The    →  464
cat    →  2459
sat    →  7731
on     →  389
the    →  279
mat    →  5634</code></pre>

                    <p><strong>Why tokens?</strong> AI can't understand words directly - it needs numbers!</p>

                    <h3>Token Visual:</h3>

                    <pre><code>"The cat sat on the mat"
  ↓
[464, 2459, 7731, 389, 279, 5634]
  ↓
Now it's numbers the AI can process!</code></pre>

                    <h2>Complete Example</h2>

                    <pre><code>INPUT: "The cat sat on the mat"

STEP 1: TOKENIZE
"The" → ID 464
"cat" → ID 2459
"sat" → ID 7731
"on"  → ID 389
"the" → ID 279
"mat" → ID 5634</code></pre>

                    <div class="info-box">
                    <strong>Important:</strong> Tokenization is just a preprocessing step. It converts text into numerical IDs that can be looked up in the embedding table (which we'll learn about in the next section).
                    </div>
                `
            },
            {
                title: "8. Understanding Vectors",
                content: `
                    <h1>Vectors in AI vs Physics</h1>
                    
                    <div class="user-question">
                    I know vector is magnitude and direction, how is that represented here?
                    </div>

                    <h2>Vectors in Physics vs AI</h2>

                    <h3>Physics Vector (What You Know)</h3>

                    <pre><code>2D Vector: [3, 4]
  ↑
  4 |    /  
    |   / ← Vector (magnitude + direction)
    |  /
    | /
  0 |/________→
    0    3

Magnitude: √(3² + 4²) = 5
Direction: tan⁻¹(4/3) = 53°

This represents a PHYSICAL direction in space!</code></pre>

                    <h3>AI Vector (Different Concept)</h3>

                    <pre><code>AI Vector: [0.23, -0.45, 0.67, 0.12]

This is just a LIST OF NUMBERS!
NOT about physical direction - it's about POSITION in "meaning space"</code></pre>

                    <h2>What Does an AI Vector Actually Mean?</h2>

                    <p>Think of it as <strong>coordinates in a multi-dimensional space where "similar meanings" are close together.</strong></p>

                    <h3>Simple 2D Example</h3>

                    <p>Let's pretend we only have 2 dimensions instead of 4096:</p>

                    <pre><code>Dimension 1: "Animal-ness" (0 = not animal, 1 = very animal)
Dimension 2: "Size" (0 = small, 1 = large)

Words as 2D vectors:

"cat"      = [0.8, 0.3]  ← 80% animal, 30% size
"dog"      = [0.9, 0.5]  ← 90% animal, 50% size
"mouse"    = [0.7, 0.1]  ← 70% animal, 10% size (small!)
"elephant" = [0.9, 0.9]  ← 90% animal, 90% size (huge!)
"car"      = [0.0, 0.6]  ← 0% animal, 60% size
"table"    = [0.0, 0.4]  ← 0% animal, 40% size

Plot this:

Size (Dim 2)
1.0 |           elephant[0.9,0.9]
    |
0.6 |                    car[0.0,0.6]
0.5 |         dog[0.9,0.5]
0.4 |                    table[0.0,0.4]
0.3 |    cat[0.8,0.3]
    |
0.1 | mouse[0.7,0.1]
    |
0.0 |_________________________________
    0   0.3  0.6  0.9             1.0
           Animal-ness (Dim 1)

Notice:
- All animals cluster on the right
- Similar animals are CLOSE to each other
- cat and dog are neighbors!
- car and table are far from animals</code></pre>

                    <p><strong>This is NOT about direction - it's about LOCATION in meaning-space!</strong></p>

                    <h2>What Do the Dimensions Mean?</h2>

                    <h3>In Our 2D Example:</h3>
                    <ul>
                        <li>Dimension 1 = "Animal-ness"</li>
                        <li>Dimension 2 = "Size"</li>
                    </ul>

                    <p><strong>We chose these features manually for the example.</strong></p>

                    <h3>In Real AI (4096 Dimensions):</h3>

                    <p>The dimensions are <strong>learned automatically during training</strong>. We don't know what each dimension means!</p>

                    <pre><code>Dimension 1: ??? (Maybe "living-ness"?)
Dimension 2: ??? (Maybe "abstract-ness"?)  
Dimension 3: ??? (Maybe "positive-emotion"?)
Dimension 4: ??? (Maybe "formality"?)
...
Dimension 4096: ??? (No idea!)

The model figures these out on its own!</code></pre>

                    <h2>Why 4096 Dimensions?</h2>

                    <div class="success-box">
                    <strong>More Dimensions = More Nuanced Meaning</strong>

                    <pre><code>With 10 numbers (10 characteristics):
Too simple! Can only capture 10 things about "cat"

With 4096 numbers (4096 characteristics):
Much better! Can capture:
- Is animal? ✓
- Size? ✓
- Has fur? ✓
- Makes sound? ✓
- Domesticated? ✓
- Eats meat? ✓
- Has tail? ✓
- Can climb? ✓
... 4088 more subtle characteristics!

More nuanced understanding of "cat"!</code></pre>
                    </div>

                    <h2>The Mathematical Definition</h2>

                    <p><strong>Vector = An ordered list of numbers</strong></p>

                    <p>That's it! Nothing inherently about "direction and magnitude"!</p>

                    <table>
                        <thead>
                            <tr>
                                <th>Term</th>
                                <th>What It Means</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Vector</strong></td>
                                <td>The whole ordered list of numbers</td>
                            </tr>
                            <tr>
                                <td><strong>Dimension</strong></td>
                                <td>How many numbers are in the list</td>
                            </tr>
                            <tr>
                                <td><strong>Dimensionality</strong></td>
                                <td>The property of having N dimensions</td>
                            </tr>
                        </tbody>
                    </table>

                    <h3>Example:</h3>

                    <pre><code>v = [0.23, -0.45, 0.67, 0.12]

"v is a vector" ← The whole thing
"v has 4 dimensions" ← Count of numbers
"v's dimensionality is 4" ← The property
"v lives in 4D space" ← Where it exists

All describing the same thing!</code></pre>

                    <div class="info-box">
                    <strong>Key Insight:</strong><br>
                    Mathematics: Vector = ordered numbers (general)<br>
                    Physics: Special case → Arrows in 2D/3D space<br>
                    AI: Back to general → Ordered numbers in high-D space<br><br>
                    Same math, different interpretations!
                    </div>
                `
            },
            {
                title: "9. Tokens to Vectors (Embeddings)",
                content: `
                    <h1>From Tokens to Vectors: The Embedding Process</h1>
                    
                    <div class="user-question">
                    How do tokens become vectors? What's the process?
                    </div>

                    <h2>STEP 2: Tokens → Vectors (Embeddings)</h2>

                    <h3>What Are Vectors?</h3>

                    <p>Each token becomes a <strong>vector</strong> - a list of numbers that represents its "meaning"</p>

                    <pre><code>Token: "cat" (ID: 2459)
  ↓
Gets converted to a vector (4096 dimensions):

cat_vector = [0.23, -0.45, 0.67, 0.12, -0.89, ..., 0.34]
             └─────────── 4096 numbers ────────────┘

This is called an "embedding"</code></pre>

                    <h3>Our Sentence as Vectors:</h3>

                    <pre><code>"The cat sat on the mat"
  ↓
Token IDs: [464, 2459, 7731, 389, 279, 5634]
  ↓
Vectors (embeddings):

"The": [0.12, -0.34, 0.56, 0.78, ..., 0.23]  ← 4096 numbers
"cat": [0.23, -0.45, 0.67, 0.12, ..., 0.34]  ← 4096 numbers
"sat": [0.45, 0.23, -0.12, 0.89, ..., 0.56]  ← 4096 numbers
"on":  [0.67, -0.12, 0.34, 0.45, ..., 0.78]  ← 4096 numbers
"the": [0.89, 0.34, -0.56, 0.23, ..., 0.12]  ← 4096 numbers
"mat": [0.34, -0.67, 0.12, -0.45, ..., 0.89]  ← 4096 numbers

Now we have 6 vectors, each with 4096 numbers!</code></pre>

                    <p><strong>Where do these vectors come from?</strong> From the <strong>embedding weights</strong> in the model!</p>

                    <h2>STEP 3: The Embedding Layer (First Weights!)</h2>

                    <h3>What's an Embedding Table?</h3>

                    <p>The model has a <strong>lookup table</strong> that converts token IDs to vectors:</p>

                    <pre><code>Embedding Table (stored in model file):

Token ID  →  Vector (4096 numbers)
────────────────────────────────────
1        →  [0.11, -0.22, 0.33, ...]
2        →  [0.44, 0.55, -0.66, ...]
3        →  [0.77, -0.88, 0.99, ...]
...
464 (The) → [0.12, -0.34, 0.56, ...]  ← We look this up!
...
2459 (cat) → [0.23, -0.45, 0.67, ...]  ← We look this up!
...
50,000   →  [0.22, 0.11, -0.33, ...]</code></pre>

                    <p><strong>These embedding values are WEIGHTS (parameters)!</strong></p>

                    <pre><code>If vocabulary = 50,000 tokens
Each vector = 4,096 numbers

Embedding weights = 50,000 × 4,096 = 204,800,000 numbers
In FP16: 204.8 million × 2 bytes = 409 MB just for embeddings!</code></pre>

                    <h2>The Model File Contains a Lookup Table</h2>

                    <p>Inside the model file (like <code>llama-3-8b.gguf</code>), there's a huge table:</p>

                    <pre><code>EMBEDDING TABLE (part of the 8 billion weights):

Token ID | Vector (4096 numbers each)
---------|---------------------------
0        | [0.123, -0.456, 0.789, 0.234, ..., 0.567]
1        | [0.234, 0.567, -0.123, 0.890, ..., 0.123]
2        | [0.345, -0.234, 0.567, 0.123, ..., 0.890]
...
2459     | [0.23, -0.45, 0.67, 0.12, ..., 0.34]  ← "cat"
...
5634     | [0.34, -0.67, 0.12, -0.45, ..., 0.89] ← "mat"
...
50000    | [0.567, 0.123, -0.890, 0.234, ..., 0.456]

Total: 50,000 rows (one for each word in vocabulary)
Each row: 4096 numbers
Total numbers: 50,000 × 4096 = 204,800,000 weights just for this table!</code></pre>

                    <p><strong>This table IS part of the model's weights!</strong></p>

                    <h2>You Type "cat"</h2>

                    <pre><code>INPUT: "cat"
  ↓
TOKENIZER: Looks up "cat" → finds ID 2459
  ↓
EMBEDDING LOOKUP: Go to row 2459 in the table
  ↓
GET VECTOR: [0.23, -0.45, 0.67, 0.12, -0.89, 0.34, 0.56, ..., 0.34]
            └──────────────── 4096 numbers ────────────────┘</code></pre>

                    <h2>What IS This Vector?</h2>

                    <pre><code>"cat" = [0.23, -0.45, 0.67, 0.12, -0.89, 0.34, 0.56, ..., 0.34]

Breaking it down:

Position 0:    0.23    ← Some learned feature
Position 1:   -0.45    ← Another learned feature  
Position 2:    0.67    ← Another learned feature
Position 3:    0.12    ← Another learned feature
...
Position 4095: 0.34    ← Last learned feature

TOTAL: 4096 positions = 4096 dimensions</code></pre>

                    <p><strong>Each position holds ONE number</strong> = <strong>one dimension</strong></p>

                    <div class="success-box">
                    <strong>Simple answer:</strong><br>
                    Vector = the list itself: [0.23, -0.45, 0.67, ..., 0.34]<br>
                    Dimensionality = how many numbers: 4096<br><br>
                    They're describing the same thing from different angles. That's it!
                    </div>
                `
            },
            {
                title: "10. From Embeddings to Transformer Layers",
                content: `
                    <h1>🚀 From Embeddings to Transformer Layers</h1>
                    
                    <div class="info-box">
                    <strong>Great! You understand embeddings.</strong><br>
                    Now let's see what happens to those 4,096 numbers after the embedding lookup!
                    </div>

                    <h2>🔄 THE COMPLETE FLOW (Big Picture)</h2>

                    <pre><code>INPUT: "The cat sat on the mat"
   ↓
┌─────────────────────────────────────┐
│ STEP 1: TOKENIZATION               │
│ "The cat sat" → [1, 2543, 3855]    │
└─────────────────────────────────────┘
   ↓
┌─────────────────────────────────────┐
│ STEP 2: EMBEDDING LOOKUP            │ ← YOU UNDERSTAND THIS! ✅
│ Token 1 → [0.234, 0.891, ...]      │
│ Token 2543 → [0.892, 0.234, ...]   │
│ Token 3855 → [0.445, -0.678, ...]  │
└─────────────────────────────────────┘
   ↓
┌─────────────────────────────────────┐
│ STEP 3: TRANSFORMER LAYERS          │ ← WE ARE HERE NOW! 👈
│ (80 layers of processing)           │
│ Each layer transforms the vectors   │
└─────────────────────────────────────┘
   ↓
┌─────────────────────────────────────┐
│ STEP 4: OUTPUT PREDICTION           │
│ 4096 numbers → Next token           │
└─────────────────────────────────────┘</code></pre>

                    <h2>📦 STEP 3 ZOOMED IN: What Are Transformer Layers?</h2>

                    <p>After embedding lookup, we have:</p>

                    <pre><code>CURRENT STATE:

"The cat sat"
   ↓
3 vectors (one per token), each with 4,096 dimensions

Vector for "The":  [0.234, 0.891, -0.445, ..., 0.567]
Vector for "cat":  [0.892, 0.234, -0.556, ..., 0.678]
Vector for "sat":  [0.445, -0.678, 0.123, ..., 0.234]

These vectors now need to be PROCESSED!</code></pre>

                    <h2>🏗️ TRANSFORMER LAYER STRUCTURE</h2>

                    <p>A 70B model has <strong>80 transformer layers</strong> stacked on top of each other.</p>

                    <p>Each layer does the same thing:</p>

                    <pre><code>ONE TRANSFORMER LAYER:

Input: 3 vectors (4096 dimensions each)
   ↓
┌────────────────────────────────────┐
│  PART A: ATTENTION                 │ ← Tokens look at each other
│  "What context is important?"      │
└────────────────────────────────────┘
   ↓
┌────────────────────────────────────┐
│  PART B: FEED-FORWARD              │ ← Process each token
│  "Update my understanding"         │
└────────────────────────────────────┘
   ↓
Output: 3 NEW vectors (still 4096 dimensions each)</code></pre>

                    <p><strong>Then repeat this 80 times!</strong></p>

                    <pre><code>LAYER BY LAYER:

Layer 1:  Understands basic patterns
Layer 10: Understands grammar
Layer 30: Understands meaning
Layer 50: Understands relationships
Layer 80: Final refined understanding

Each layer makes the vectors "smarter"!</code></pre>

                    <h2>🔍 WHAT HAPPENS IN ONE LAYER: Simple View</h2>

                    <p>Let's trace "cat" through ONE layer:</p>

                    <pre><code>INPUT to Layer 1:

"cat" vector: [0.892, 0.234, -0.556, ..., 0.678]
                ↑ 4096 numbers

┌─────────────────────────────────────────────────┐
│              TRANSFORMER LAYER 1                 │
│                                                  │
│  STEP A: ATTENTION                              │
│  - Look at "The" vector                         │
│  - Look at "sat" vector                         │
│  - Understand: "cat" is between "The" and "sat" │
│  - Update "cat" vector with this context        │
│                                                  │
│  STEP B: FEED-FORWARD                           │
│  - Process the updated vector                   │
│  - Apply learned transformations                │
│                                                  │
└─────────────────────────────────────────────────┘

OUTPUT from Layer 1:

"cat" vector: [0.901, 0.245, -0.523, ..., 0.692]
               ↑ 4096 numbers (slightly different!)

The numbers changed! The vector now contains:
- Original "cat" meaning
+ Context from surrounding words</code></pre>

                    <h2>🎯 THE KEY INSIGHT: Vectors Keep Evolving</h2>

                    <pre><code>TRACKING "cat" THROUGH ALL LAYERS:

After Embedding:     [0.892, 0.234, -0.556, ...]  "raw cat"
After Layer 1:       [0.901, 0.245, -0.523, ...]  "cat near The"
After Layer 10:      [0.887, 0.256, -0.534, ...]  "cat as subject"
After Layer 30:      [0.895, 0.267, -0.512, ...]  "cat that sat"
After Layer 80:      [0.903, 0.278, -0.501, ...]  "fully contextualized cat"

Each layer refines the understanding!</code></pre>

                    <h2>📊 WHERE ARE THE WEIGHT MATRICES?</h2>

                    <p>Remember weight matrices? <strong>They live in each transformer layer!</strong></p>

                    <pre><code>ONE TRANSFORMER LAYER HAS MULTIPLE MATRICES:

┌──────────────────────────────────────────┐
│         TRANSFORMER LAYER                │
│                                          │
│  ATTENTION PART:                         │
│  ├─ Q Matrix (Query):    4096 × 4096    │ ← 16.7M weights
│  ├─ K Matrix (Key):      4096 × 4096    │ ← 16.7M weights
│  ├─ V Matrix (Value):    4096 × 4096    │ ← 16.7M weights
│  └─ O Matrix (Output):   4096 × 4096    │ ← 16.7M weights
│                                          │
│  FEED-FORWARD PART:                      │
│  ├─ Up Matrix:          4096 × 11008    │ ← 45M weights
│  └─ Down Matrix:        11008 × 4096    │ ← 45M weights
│                                          │
│  Total per layer: ~155 million weights  │
└──────────────────────────────────────────┘

80 layers × 155M = ~12.4 billion weights just in layers!</code></pre>

                    <h2>🔢 HOW MATRICES PROCESS VECTORS</h2>

                    <p>This is where <strong>matrix multiplication</strong> happens:</p>

                    <h3>Simple Example (tiny numbers for clarity):</h3>

                    <pre><code>Input vector:     [0.892, 0.234, -0.556, 0.123]  (4 dims)
                           ×
Weight Matrix Q:  [0.5   0.2   -0.3   0.1  ]    (4×4)
                  [0.1   0.6    0.2  -0.4  ]
                  [-0.2  0.3    0.5   0.2  ]
                  [0.4  -0.1    0.1   0.7  ]
                           =
Output vector:    [0.345, 0.123, -0.234, 0.456]  (4 dims)

The matrix "transforms" the vector!</code></pre>

                    <h3>In real models:</h3>

                    <pre><code>Input vector:     4096 numbers
                           ×
Weight Matrix:    4096 × 4096 = 16.7 million weights
                           =
Output vector:    4096 numbers (transformed)</code></pre>

                    <h2>🎨 VISUAL: ONE LAYER PROCESSING</h2>

                    <pre><code>                "The"         "cat"        "sat"
                  ↓             ↓            ↓
               [4096]        [4096]       [4096]  ← Input vectors
                  ↓             ↓            ↓
         ┌────────────────────────────────────┐
         │     ATTENTION (Self-Attention)     │
         │                                    │
         │  Each vector looks at ALL vectors  │
         │  and updates itself                │
         │                                    │
         │  Q Matrix: [4096×4096]            │
         │  K Matrix: [4096×4096]            │
         │  V Matrix: [4096×4096]            │
         │  O Matrix: [4096×4096]            │
         └────────────────────────────────────┘
                  ↓             ↓            ↓
               [4096]        [4096]       [4096]  ← Updated vectors
                  ↓             ↓            ↓
         ┌────────────────────────────────────┐
         │      FEED-FORWARD                  │
         │                                    │
         │  Process each vector individually  │
         │                                    │
         │  Up Matrix:   [4096×11008]        │
         │  Down Matrix: [11008×4096]        │
         └────────────────────────────────────┘
                  ↓             ↓            ↓
               [4096]        [4096]       [4096]  ← Layer output
                  ↓             ↓            ↓
              TO NEXT LAYER (repeat 79 more times!)</code></pre>

                    <h2>🧮 WHAT DO THESE MATRICES DO?</h2>

                    <h3>Attention Matrices (Q, K, V, O):</h3>

                    <div class="info-box">
                    <strong>PURPOSE: Let tokens "talk" to each other</strong>

                    <pre><code>Example:
"The cat sat on the mat"

When processing "cat":
- Query (Q): "What am I looking for?"
- Key (K):   "What information do I have?"
- Value (V): "What information should I use?"
- Output (O): "Combine everything together"

Result: "cat" learns it's:
- After "The" (definite article)
- Before "sat" (the cat is doing the sitting)
- Subject of the sentence</code></pre>
                    </div>

                    <h3>Feed-Forward Matrices:</h3>

                    <div class="success-box">
                    <strong>PURPOSE: Process and refine each token individually</strong>

                    <pre><code>Example:
After attention, "cat" knows its context.
Feed-forward then:
- Extracts features
- Applies learned patterns
- Refines the representation

Think: "Now that I know context, update my understanding"</code></pre>
                    </div>

                    <h2>📈 COMPLETE EXAMPLE: "The cat sat"</h2>

                    <pre><code>STEP-BY-STEP THROUGH ONE LAYER:

INPUT:
Token "The":  [0.234, 0.891, -0.445, ..., 0.567]
Token "cat":  [0.892, 0.234, -0.556, ..., 0.678]
Token "sat":  [0.445, -0.678, 0.123, ..., 0.234]

ATTENTION:
- "The" looks at "cat" and "sat"
- "cat" looks at "The" and "sat"  ← notices it's between them
- "sat" looks at "The" and "cat"

Each vector updates based on what it sees!

AFTER ATTENTION:
Token "The":  [0.245, 0.903, -0.432, ..., 0.578]
Token "cat":  [0.901, 0.245, -0.523, ..., 0.692] ← changed!
Token "sat":  [0.456, -0.665, 0.134, ..., 0.245]

FEED-FORWARD:
Each vector goes through Up Matrix → Down Matrix

AFTER FEED-FORWARD (Layer 1 output):
Token "The":  [0.256, 0.915, -0.421, ..., 0.589]
Token "cat":  [0.913, 0.256, -0.512, ..., 0.703]
Token "sat":  [0.467, -0.654, 0.145, ..., 0.256]

These go to Layer 2!</code></pre>

                    <h2>🎯 How All 80 Layers Work Together</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Layer Range</th>
                                <th>What It Learns</th>
                                <th>Example for "cat"</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Layers 1-10</strong></td>
                                <td>Basic patterns, syntax</td>
                                <td>"cat" is a noun, comes after "The"</td>
                            </tr>
                            <tr>
                                <td><strong>Layers 11-30</strong></td>
                                <td>Grammar, structure</td>
                                <td>"cat" is the subject, "sat" is the verb</td>
                            </tr>
                            <tr>
                                <td><strong>Layers 31-50</strong></td>
                                <td>Meaning, semantics</td>
                                <td>"cat" is an animal that can sit</td>
                            </tr>
                            <tr>
                                <td><strong>Layers 51-70</strong></td>
                                <td>Relationships, context</td>
                                <td>"cat" sitting on "mat" is a common scenario</td>
                            </tr>
                            <tr>
                                <td><strong>Layers 71-80</strong></td>
                                <td>Final refinement</td>
                                <td>Complete understanding ready for prediction</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>💡 Key Insights</h2>

                    <div class="critical-box">
                    <strong>Understanding Transformer Layers:</strong><br><br>

                    <strong>1. Vectors are NOT static</strong><br>
                    They change at every layer, accumulating context and understanding<br><br>

                    <strong>2. Each layer has millions of weights</strong><br>
                    ~155 million per layer × 80 layers = 12.4 billion weights<br><br>

                    <strong>3. Attention is the secret sauce</strong><br>
                    It lets tokens understand their relationship to other tokens<br><br>

                    <strong>4. Progressive refinement</strong><br>
                    Early layers: syntax → Middle layers: meaning → Late layers: context<br><br>

                    <strong>5. Same structure, repeated</strong><br>
                    Each of the 80 layers has the same structure, but different learned weights
                    </div>

                    <h2>✅ Check Your Understanding</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Question</th>
                                <th>Answer</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>After embedding lookup, where do vectors go?</td>
                                <td>Through transformer layers (80 of them)</td>
                            </tr>
                            <tr>
                                <td>What does each transformer layer do?</td>
                                <td>Attention (look at context) + Feed-forward (process)</td>
                            </tr>
                            <tr>
                                <td>Do the vectors stay the same through layers?</td>
                                <td>No! They evolve and refine at each layer</td>
                            </tr>
                            <tr>
                                <td>How many weights in one transformer layer?</td>
                                <td>~155 million (Q, K, V, O, Up, Down matrices)</td>
                            </tr>
                            <tr>
                                <td>What is attention for?</td>
                                <td>Letting tokens understand their context from other tokens</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="success-box">
                    <strong>🎉 You now understand the complete flow!</strong><br><br>
                    Embeddings → Transformer Layers → Refined Understanding<br><br>
                    Next, we'll see how these refined vectors become predictions!
                    </div>
                `
            },
            {
                title: "11. How Vectors Are Learned",
                content: `
                    <h1>How Did Those Numbers Get There?</h1>
                    
                    <div class="user-question">
                    Now explain those vector numbers, how did they get there? So cat alone has a number and then correlated vector number? Because cat alone has 4096 bytes?
                    </div>

                    <h2>Answer: TRAINING</h2>

                    <p>The model <strong>learned</strong> those numbers by reading billions of examples.</p>

                    <h2>Step 1: Start Random</h2>

                    <p>When the model is first created, the numbers are <strong>random</strong>:</p>

                    <pre><code>BEFORE TRAINING:

"cat" → [0.91, -0.17, 0.44, 0.82, -0.33, ...]  ← Random garbage!
"dog" → [0.55, 0.23, -0.61, 0.09, 0.74, ...]  ← Random garbage!
"mat" → [0.38, -0.92, 0.15, -0.47, 0.61, ...]  ← Random garbage!

Model is USELESS at this point!</code></pre>

                    <h2>Step 2: Show Examples</h2>

                    <p>Feed the model <strong>billions</strong> of sentences from the internet:</p>

                    <pre><code>Example 1: "The cat sat on the mat"
Example 2: "I saw a cat in the garden"  
Example 3: "My cat loves to sleep"
Example 4: "The dog chased the cat"
Example 5: "Cats are furry animals"
...
(billions more examples)</code></pre>

                    <h2>Step 3: Learn from Patterns</h2>

                    <p>The model notices <strong>patterns</strong>:</p>

                    <pre><code>PATTERN 1: Words that appear together
"cat" often appears near: "mat", "sat", "pet", "furry", "meow"
"dog" often appears near: "bark", "pet", "furry", "tail"
"car" often appears near: "drive", "road", "engine"

PATTERN 2: Similar contexts
"The ___ sat on the mat" → could be "cat" or "dog" (animals)
"The ___ drove fast" → could be "car" or "truck" (vehicles)

PATTERN 3: What comes next
After "The cat", usually comes: "sat", "ran", "jumped", "is"
After "The cat sat", usually comes: "on", "down", "still"</code></pre>

                    <h2>Step 4: Adjust Numbers to Capture Patterns</h2>

                    <p>The training process <strong>adjusts the numbers</strong> to reflect these patterns:</p>

                    <pre><code>TRAINING CYCLE (repeated billions of times):

1. Model sees: "The cat sat on the ___"

2. Model's current guess (using current numbers):
   Next word might be: "floor" (40% confidence)

3. Check the real answer: "mat"

4. Model was WRONG!

5. ADJUST THE NUMBERS:
   - Make "cat" numbers more similar to "mat" numbers
   - Adjust weights in layers to predict "mat" better
   
6. New numbers for "cat": [0.24, -0.46, 0.68, ...]
   (Slightly different from before)

7. Repeat with next example...</code></pre>

                    <h2>Step 5: After Billions of Adjustments</h2>

                    <pre><code>AFTER TRAINING:

"cat" → [0.23, -0.45, 0.67, 0.12, -0.89, ...]  ← Learned values!
"dog" → [0.21, -0.42, 0.65, 0.15, -0.85, ...]  ← Close to "cat"!
"mat" → [0.34, -0.67, 0.12, -0.45, 0.78, ...]  ← Somewhat similar
"car" → [0.01, 0.12, -0.89, 0.45, 0.23, ...]  ← Very different!

Notice:
- "cat" and "dog" have SIMILAR numbers (both animals)
- "cat" and "car" have DIFFERENT numbers (unrelated)
- These similarities were LEARNED from seeing billions of examples!</code></pre>

                    <h2>Why These Specific Numbers?</h2>

                    <pre><code>Each position (dimension) captures a different pattern:

Position 0 (value: 0.23):
  Maybe captures "is it an animal?"
  cat = 0.23 (yes, somewhat)
  car = 0.01 (no, not at all)

Position 1 (value: -0.45):
  Maybe captures "is it small?"
  cat = -0.45 (yes)
  elephant = 0.89 (no, it's big)

Position 2 (value: 0.67):
  Maybe captures "does it have fur?"
  cat = 0.67 (yes)
  fish = -0.23 (no)

... 4093 more patterns captured in the other positions</code></pre>

                    <p><strong>We don't know exactly what each position means!</strong> The model figured it out on its own.</p>

                    <h2>Clarification: Two Different Numbers</h2>

                    <div class="info-box">
                    <strong>"cat" has TWO things:</strong>

                    <h4>1. One ID Number (to identify it)</h4>
                    <pre><code>"cat" → ID: 2459

This is just a label, like a student ID number</code></pre>

                    <h4>2. One Vector (4096 numbers to represent its meaning)</h4>
                    <pre><code>"cat" → Vector: [0.23, -0.45, 0.67, 0.12, ..., (4096 total numbers)]

This captures what "cat" means</code></pre>
                    </div>

                    <h2>About Bytes (Storage)</h2>

                    <div class="warning-box">
                    <strong>Important Clarification:</strong>

                    <pre><code>"cat" vector has:
- 4096 NUMBERS (count)
- Each number takes 2 BYTES (in FP16 format)
- Total storage: 4096 × 2 = 8,192 BYTES per word

So:
4096 = dimensionality (how many numbers)
8,192 bytes = storage size (how much memory)</code></pre>
                    </div>

                    <h2>The Training Process (Simple Version)</h2>

                    <pre><code>1. START:
   All words have random numbers

2. READ:
   Show model: "The cat sat on the mat"

3. PREDICT:
   Model guesses next word using current numbers

4. CHECK:
   Was the guess right?

5. ADJUST:
   If wrong, change the numbers slightly

6. REPEAT:
   Do this billions of times with different sentences

7. RESULT:
   Numbers that are good at predicting language!</code></pre>

                    <div class="success-box">
                    <strong>Final Answer:</strong><br><br>
                    <strong>Where do the numbers come from?</strong><br>
                    1. Start: Random numbers<br>
                    2. Training: Show billions of examples<br>
                    3. Adjustment: Change numbers to predict better<br>
                    4. Result: Numbers that "understand" the word
                    </div>
                `
            },
            {
                title: "11. Why Sentences Matter for Training",
                content: `
                    <h1>Training Needs Context: Why Sentences Matter</h1>
                    
                    <div class="user-question">
                    Not sure I understand. If I fed cat vs sentence, how does that work? Whenever you explain, you explain the correlation with other words in a sentence...?
                    </div>

                    <h2>TWO Different Scenarios</h2>

                    <h3>Scenario 1: TRAINING (Learning the numbers)</h3>
                    <ul>
                        <li>You MUST use sentences</li>
                        <li>Cannot train on just "cat" alone</li>
                    </ul>

                    <h3>Scenario 2: USING the model (After training)</h3>
                    <ul>
                        <li>You CAN input just "cat" alone</li>
                        <li>It already has the learned numbers</li>
                    </ul>

                    <h2>Why You NEED Sentences for Training</h2>

                    <h3>Problem: "cat" alone teaches NOTHING</h3>

                    <pre><code>If you only show the model:

"cat"
"cat"  
"cat"
"cat"
(repeated millions of times)

What can the model learn? NOTHING!
- What does "cat" mean? Unknown
- What comes after "cat"? Unknown
- What is "cat" related to? Unknown</code></pre>

                    <h3>Solution: Need CONTEXT (sentences)</h3>

                    <pre><code>Model learns "cat" by seeing it WITH other words:

"The cat sat on the mat" 
   ↑ Now model learns: "cat" relates to "sat", "mat"

"My cat is furry"
   ↑ Now model learns: "cat" relates to "furry"

"Cats meow loudly"
   ↑ Now model learns: "cat" relates to "meow"

"I have a cat and a dog"
   ↑ Now model learns: "cat" relates to "dog", both are similar</code></pre>

                    <p><strong>The 4096 numbers for "cat" are learned FROM these contexts!</strong></p>

                    <h2>How "cat" Numbers Are Actually Learned</h2>

                    <h3>Process:</h3>

                    <pre><code>STEP 1: Model sees "The cat sat on the mat"

STEP 2: Model tries to predict:
Given: "The cat sat on the"
Predict: "___"

STEP 3: Model uses current numbers:
"The" → [0.12, ...]
"cat" → [0.91, -0.17, ...] ← Current random numbers
"sat" → [0.55, ...]
"on" → [0.38, ...]
"the" → [0.12, ...]

Combines them with math → Predicts: "floor"

STEP 4: Check answer
Correct answer: "mat"
Model guessed: "floor"
WRONG!

STEP 5: Adjust ALL the words' numbers
"cat" numbers: [0.91, -0.17, ...] → [0.90, -0.16, ...]
"sat" numbers: [0.55, ...] → [0.54, ...]
"the" numbers: [0.12, ...] → [0.13, ...]

All words learn together!</code></pre>

                    <p><strong>"cat" learns its meaning by seeing what words appear around it!</strong></p>

                    <h2>Another Example - Learning "cat" and "dog" are similar</h2>

                    <pre><code>Sentence 1: "The cat sat on the mat"
Model learns: After "The ___", "sat" often follows
Adjusts: "cat" numbers to predict "sat"

Sentence 2: "The dog sat on the mat"  
Model learns: After "The ___", "sat" often follows (again!)
Adjusts: "dog" numbers to ALSO predict "sat"

Result after billions of examples:
"cat" → [0.23, -0.45, 0.67, ...]
"dog" → [0.21, -0.42, 0.65, ...] ← Very similar to "cat"!

Why similar? Because they appear in similar contexts!
- "The cat sat..." 
- "The dog sat..."
- "My cat is..."
- "My dog is..."</code></pre>

                    <h2>What If You Input Just "cat" AFTER Training?</h2>

                    <div class="success-box">
                    <strong>AFTER the model is trained, you CAN use just "cat":</strong>

                    <pre><code>INPUT: "cat"

STEP 1: Look up already-learned numbers
"cat" → [0.23, -0.45, 0.67, 0.12, ..., 0.34]
        ↑ These were learned from billions of sentences!

STEP 2: Model processes these numbers

STEP 3: Model predicts what might come next
Predictions:
- "is" (20%)
- "sat" (15%)
- "ran" (10%)
- "meowed" (8%)
...

OUTPUT: "cat is"</code></pre>

                    <p><strong>The numbers were already learned from sentences during training!</strong></p>
                    </div>

                    <h2>Key Difference</h2>

                    <h3>TRAINING TIME (learning the numbers):</h3>
                    <pre><code>❌ CANNOT do: Just "cat" alone
✅ MUST do: "The cat sat on the mat" (sentences with context)

Why? "cat" alone has no context to learn from!</code></pre>

                    <h3>INFERENCE TIME (using the model):</h3>
                    <pre><code>✅ CAN do: Just "cat" alone
The numbers are already learned!

"cat" already has: [0.23, -0.45, 0.67, ..., 0.34]
Model just looks these up and uses them.</code></pre>

                    <h2>Analogy: Learning a Language</h2>

                    <h3>Learning a Language (TRAINING)</h3>
                    <pre><code>❌ BAD WAY:
Teacher shows flashcard: "cat"
That's it.
You learn: Nothing! What IS a cat?

✅ GOOD WAY:
Teacher shows:
- "The cat sat on the mat" (with picture)
- "My cat is furry" (with picture)
- "Cats meow" (with sound)

You learn: "Oh! A cat is a small furry animal that meows and sits on things!"</code></pre>

                    <h3>Using the Language (INFERENCE)</h3>
                    <pre><code>After learning, someone asks: "What's a cat?"
You CAN answer, even though it's just the word "cat" alone!

Because you already learned what "cat" means from all those sentences!</code></pre>

                    <div class="info-box">
                    <strong>Simple Answer:</strong><br><br>
                    <strong>Q: Can you train on just "cat"?</strong><br>
                    <strong>A: NO! You need sentences.</strong><br><br>
                    The 4096 numbers for "cat" capture:<br>
                    - What words appear near "cat"<br>
                    - What "cat" relates to<br>
                    - What comes before/after "cat"<br>
                    - How "cat" is similar to other words<br><br>
                    <strong>You CANNOT learn this from "cat" alone!</strong><br><br>
                    <strong>Q: Can you input just "cat" after training?</strong><br>
                    <strong>A: YES! The numbers are already learned.</strong>
                    </div>
                `
            },
            {
                title: "12. What the 4096 Numbers Represent",
                content: `
                    <h1>The 4096 Numbers = 4096 Characteristics</h1>
                    
                    <div class="user-question">
                    Ah, so these cat 4096 numbers have values representing characteristics?
                    </div>

                    <h2>YES! EXACTLY!</h2>

                    <p>Each number captures a different characteristic/feature about "cat"!</p>

                    <pre><code>"cat" = [0.23, -0.45, 0.67, 0.12, -0.89, ..., 0.34]
        ↑      ↑      ↑     ↑      ↑          ↑
      Feature Feature Feature Feature    Feature
        #1     #2     #3     #4          #4096

4096 different characteristics!</code></pre>

                    <h2>What These Characteristics Might Be</h2>

                    <pre><code>Position 0 (value: 0.23):
  "Is it an animal?" → 0.23 (yes, somewhat)

Position 1 (value: -0.45):
  "Is it large?" → -0.45 (no, it's small)

Position 2 (value: 0.67):
  "Does it have fur?" → 0.67 (yes, very furry)

Position 3 (value: 0.12):
  "Is it a vehicle?" → 0.12 (no, not at all)

Position 4 (value: -0.89):
  "Is it abstract?" → -0.89 (no, very concrete)

...

Position 4096 (value: 0.34):
  "Is it cute?" → 0.34 (somewhat)</code></pre>

                    <p><strong>Each of the 4096 positions learned to capture SOME characteristic!</strong></p>

                    <h2>Comparison: "cat" vs "car"</h2>

                    <pre><code>"cat" = [0.23, -0.45, 0.67, 0.12, -0.89, ...]
        
Position 0: 0.23  → "Is animal?" = YES
Position 1: -0.45 → "Is large?" = NO (small)
Position 2: 0.67  → "Has fur?" = YES
Position 3: 0.12  → "Is vehicle?" = NO


"car" = [0.01, 0.82, -0.91, 0.95, -0.23, ...]

Position 0: 0.01  → "Is animal?" = NO
Position 1: 0.82  → "Is large?" = YES (bigger than cat)
Position 2: -0.91 → "Has fur?" = NO
Position 3: 0.95  → "Is vehicle?" = YES

Completely different characteristics!</code></pre>

                    <h2>How These Characteristics Are Learned</h2>

                    <p>The model learns them from CONTEXT:</p>

                    <pre><code>Sentence: "The furry cat purred"
           ↑     ↑    ↑
Model learns: "cat" appears near "furry" and "purred"

Adjusts numbers:
Position 2 (fur characteristic): 0.50 → 0.67 (increase!)
Position 573 (sound characteristic): 0.20 → 0.35 (increase!)

After seeing "furry cat" millions of times:
"cat" position 2 = 0.67 (strongly associated with "furry")</code></pre>

                    <pre><code>Sentence: "The cat is small"
                  ↑    ↑
Model learns: "cat" appears with "small"

Adjusts numbers:
Position 1 (size characteristic): 0.10 → -0.45 (decrease!)

After seeing "small cat" millions of times:
"cat" position 1 = -0.45 (strongly associated with "small")</code></pre>

                    <h2>Why 4096?</h2>

                    <p><strong>To capture 4096 DIFFERENT characteristics!</strong></p>

                    <pre><code>With 10 numbers (10 characteristics):
Too simple! Can only capture 10 things about "cat"

With 4096 numbers (4096 characteristics):
Much better! Can capture:
- Is animal? ✓
- Size? ✓
- Has fur? ✓
- Makes sound? ✓
- Domesticated? ✓
- Eats meat? ✓
- Has tail? ✓
- Can climb? ✓
... 4088 more subtle characteristics!

More nuanced understanding of "cat"!</code></pre>

                    <h2>Real Example</h2>

                    <pre><code>After training on billions of sentences:

"cat" learned these characteristics:
[0.23, -0.45, 0.67, 0.12, -0.89, 0.34, 0.56, 0.78, ...]
  ↑      ↑      ↑     ↑      ↑     ↑     ↑     ↑
animal  small  furry  not   living  cute  soft  common
                      vehicle

"dog" learned SIMILAR characteristics:
[0.21, -0.42, 0.65, 0.15, -0.85, 0.31, 0.54, 0.75, ...]
  ↑      ↑      ↑     ↑      ↑     ↑     ↑     ↑
animal  small  furry  not   living  cute  soft  common
              (slightly)  vehicle

Both are similar because they share many characteristics!</code></pre>

                    <div class="success-box">
                    <strong>Summary:</strong><br><br>
                    ✅ <strong>YES! The 4096 numbers represent 4096 characteristics</strong><br><br>
                    Each word = 4096 numbers<br>
                    Each number = 1 learned characteristic/feature<br><br>
                    "cat" = [characteristic 1, characteristic 2, ... characteristic 4096]<br><br>
                    These characteristics were learned by seeing:<br>
                    - What words appear near "cat"<br>
                    - What contexts "cat" appears in<br>
                    - What "cat" relates to<br>
                    - How "cat" is used in sentences<br><br>
                    <strong>You got it! 🎉</strong>
                    </div>
                `
            },
            {
                title: "13. How Features Emerge",
                content: `
                    <h1>How Features Emerge: The Transformer Architecture</h1>
                    
                    <div class="user-question">
                    How is it defined that each feature is what? I know QKV... but how is that defined, transformer, LLM or how each feature is what?
                    </div>

                    <h2>WE DON'T DEFINE WHAT EACH FEATURE IS!</h2>

                    <p><strong>The model figures it out on its own during training.</strong></p>

                    <h2>The Reality</h2>

                    <pre><code>Position 0 of "cat" = 0.23

We DON'T KNOW what this represents!
- Maybe "animal-ness"?
- Maybe "size"?
- Maybe something completely abstract?
- We don't know!

The model learned it automatically.</code></pre>

                    <h2>How It Works</h2>

                    <h3>1. We ONLY Define: The Architecture</h3>

                    <pre><code>Humans design:
✓ Use 4096 dimensions ← We choose the NUMBER
✓ Use transformer architecture ← We choose the STRUCTURE
✓ Use attention mechanism ← We choose the MECHANISM
✓ Training data (which sentences to show)
✓ Learning rate (how fast to adjust)

Humans DO NOT design:
✗ What position 0 means ← Model learns this
✗ What position 1 means ← Model learns this
✗ What each dimension represents ← Model learns all of this</code></pre>

                    <h3>2. Model Learns the Features Automatically</h3>

                    <pre><code>TRAINING:

Start: All 4096 positions are random

Model sees: "The cat sat on the mat"
Model adjusts: All 4096 numbers change slightly

Model sees: "My cat is furry"  
Model adjusts: All 4096 numbers change again

After billions of examples:
Position 0 = 0.23 ← Learned to represent SOMETHING useful
Position 1 = -0.45 ← Learned to represent SOMETHING ELSE useful
...

But we don't know what each position learned!</code></pre>

                    <h2>Transformer Architecture Creates Features</h2>

                    <p>Let me explain how transformers make this happen:</p>

                    <h3>The Transformer Process</h3>

                    <pre><code>INPUT: "cat"
↓
EMBEDDING: [0.23, -0.45, 0.67, ..., 0.34] (4096 numbers)
↓
ATTENTION LAYERS (This is where QKV comes in!)
↓
Each layer transforms the 4096 numbers
↓
OUTPUT: New 4096 numbers with learned features</code></pre>

                    <h2>How QKV (Attention) Works</h2>

                    <h3>What is QKV?</h3>

                    <p><strong>Query, Key, Value</strong> - the mechanism that lets words "talk" to each other</p>

                    <pre><code>Sentence: "The cat sat on the mat"

Each word becomes 4096 numbers, then:

Q (Query): "What am I looking for?"
K (Key): "What do I have?"
V (Value): "What information do I give?"

Example:
"sat" (Query): "I'm a verb, looking for my subject"
"cat" (Key): "I'm a noun, I can be a subject"
↓
MATCH! "sat" pays attention to "cat"
↓
"sat" gets information from "cat" (Value)</code></pre>

                    <h3>This Creates the Features!</h3>

                    <pre><code>Layer 1: Attention
"cat" looks at nearby words
Position 0 learns: "Am I near an action word?"
Position 1 learns: "Am I near a location word?"
...

Layer 2: Attention  
"cat" combines information differently
Position 0 now represents: Something more complex
Position 1 now represents: Something else complex
...

After 80 layers:
Position 0 = Learned SOME complex pattern
Position 1 = Learned ANOTHER complex pattern
...

What patterns? We don't fully know!</code></pre>

                    <h2>Example: How ONE Feature Might Emerge</h2>

                    <pre><code>TRAINING EXAMPLES:

"The cat sat" → Model learns: after "cat" often comes action
"The dog ran" → Model learns: after "dog" often comes action
"The bird flew" → Model learns: after "bird" often comes action

PATTERN EMERGES:
Some dimension (let's say position 573) starts to capture:
"Things that can DO actions"

After billions of examples:
Position 573:
cat = 0.87 (high - animals do actions)
dog = 0.85 (high - animals do actions)
table = 0.02 (low - tables don't do actions)
car = 0.15 (medium - cars do actions but differently)

This dimension LEARNED to represent "agency" or "animacy"
But we didn't program it! It emerged automatically!</code></pre>

                    <h2>Why We Don't Know What Each Feature Means</h2>

                    <h3>The Problem:</h3>

                    <pre><code>4096 dimensions × 80 layers = Extremely complex

Each dimension influences all others
Features are "entangled" - mixed together
Not clean like "Position 0 = size"

More like:
Position 0 = 30% size + 20% animal + 15% common + 35% other stuff

Very hard to interpret!</code></pre>

                    <h2>The Transformer Determines HOW Features Form</h2>

                    <h3>Transformer Architecture:</h3>

                    <pre><code>1. EMBEDDING LAYER
   Creates initial 4096 numbers for each word

2. ATTENTION LAYERS (80 layers in Llama 3)
   Each layer:
   - Q: What to look for
   - K: What you have
   - V: What to share
   
   Words "communicate" and transform their features

3. FEED-FORWARD LAYERS
   Further transforms the features
   
4. OUTPUT LAYER
   Predicts next word using final features</code></pre>

                    <h3>Each Layer Transforms Features:</h3>

                    <pre><code>"cat" at Layer 0: [0.23, -0.45, 0.67, ...]
                   ↑ Basic features
↓ Attention + transformation
"cat" at Layer 1: [0.25, -0.43, 0.69, ...]
                   ↑ Slightly more complex features
↓ Attention + transformation  
"cat" at Layer 2: [0.28, -0.40, 0.71, ...]
                   ↑ Even more complex features
...

"cat" at Layer 80: [0.51, -0.12, 0.89, ...]
                    ↑ Very complex, abstract features

What do these represent? We mostly don't know!</code></pre>

                    <h2>What We DO Control vs What Emerges</h2>

                    <h3>WE CONTROL:</h3>
                    <pre><code>✓ Number of dimensions (4096)
✓ Number of layers (80)
✓ Architecture type (Transformer)
✓ Training data (which sentences to show)
✓ Learning rate (how fast to adjust)</code></pre>

                    <h3>MODEL LEARNS (Emerges):</h3>
                    <pre><code>✗ What each dimension represents
✗ Which features are important
✗ How to combine features
✗ What patterns to detect
✗ What each number should be</code></pre>

                    <div class="critical-box">
                    <strong>Key Insight:</strong><br><br>
                    The 4096 numbers are like <strong>4096 empty boxes</strong>. We create the boxes (architecture), but the <strong>model fills them</strong> with whatever helps it predict text better. We often don't know what's in each box!<br><br>
                    <strong>Your Question: "How is it defined that each feature is what?"</strong><br><br>
                    <strong>Answer: It's NOT defined! It emerges from training.</strong><br><br>
                    We give it the STRUCTURE (transformer, 4096 dimensions)<br>
                    It learns the MEANING (what each dimension represents)
                    </div>
                `
            },
            {
                title: "14. Dimension vs Matrix - Clear Explanation",
                content: `
                    <h1>🎯 Understanding Dimension vs Matrix</h1>
                    
                    <div class="critical-box">
                    <strong>Important Clarification!</strong><br>
                    Let's clear up a common confusion about dimensions, vectors, and matrices.
                    </div>

                    <h2>📐 What is "Dimension"?</h2>

                    <p><strong>Dimension = One Number in a List</strong></p>

                    <h3>Simple Example: A 3-dimensional vector</h3>

                    <pre><code>Vector = [5, 8, 2]
         ↑  ↑  ↑
       dim 0, 1, 2

This vector has 3 DIMENSIONS (3 numbers)</code></pre>

                    <h3>For "cat" with 4096 dimensions:</h3>

                    <pre><code>cat_vector = [0.892, 0.234, -0.556, 0.123, ..., 0.678]
              ↑      ↑      ↑      ↑           ↑
            dim 0   dim 1  dim 2  dim 3  ... dim 4095

This vector has 4096 DIMENSIONS (4096 numbers in the list)</code></pre>

                    <div class="success-box">
                    <strong>Think of it as:</strong> Dimension = How many numbers in the list
                    </div>

                    <h2>📊 What is The Embedding Table (Matrix)?</h2>

                    <p><strong>The embedding table is a MATRIX = Table of Vectors</strong></p>

                    <h3>Super Clear Visualization:</h3>

                    <pre><code>EMBEDDING TABLE = MATRIX

            ← 4096 dimensions (columns) →
          ┌──────────────────────────────────┐
   Token  │  dim 0   dim 1   dim 2   ...  4095│
    ID    │                                   │
    ↓     │                                   │
    0     │  0.123  -0.456   0.789  ...  0.234│ ← Row 0 (one vector)
    1     │  0.234   0.891  -0.445  ...  0.567│ ← Row 1 (one vector) "The"
    2     │  0.567  -0.123   0.456  ... -0.123│ ← Row 2 (one vector)
   ...    │  ...     ...     ...    ...   ... │
   2543   │  0.892   0.234  -0.556  ...  0.678│ ← Row 2543 (one vector) "cat"
   ...    │  ...     ...     ...    ...   ... │
   32000  │  0.345  -0.678   0.912  ...  0.456│ ← Row 32000 (one vector)
          └──────────────────────────────────┘
   ↑                                           ↑
32,000 rows                          4096 columns
(vocab size)                    (embedding dimension)</code></pre>

                    <h2>🔍 Breaking It Down</h2>

                    <h3>The Matrix Has Two Measurements:</h3>

                    <pre><code>EMBEDDING TABLE (Matrix):

Vertical (↓):   32,000 rows    = Vocabulary size (how many tokens)
Horizontal (→): 4,096 columns  = Embedding dimension (how many numbers per token)

Shape: [32,000 × 4,096]
       ↑        ↑
       rows     columns</code></pre>

                    <h3>Each Row is ONE Vector:</h3>

                    <pre><code>ZOOMING IN ON ONE ROW:

Row 2543 (the embedding for token "cat"):

[0.892, 0.234, -0.556, 0.123, -0.789, ..., 0.678]
 ↑                                            ↑
 first dimension                    last dimension
 
This single row has 4096 dimensions (4096 numbers)</code></pre>

                    <h2>🎨 Visual Comparison</h2>

                    <h3>VIEW 1: The Whole Matrix (Table View)</h3>

                    <pre><code>EMBEDDING TABLE (all tokens):

        Column 0  Column 1  Column 2  ...  Column 4095
        ┌────────┬─────────┬─────────┬────┬───────────┐
Row 0   │ 0.123  │ -0.456  │  0.789  │... │  0.234    │
Row 1   │ 0.234  │  0.891  │ -0.445  │... │  0.567    │
Row 2   │ 0.567  │ -0.123  │  0.456  │... │ -0.123    │
...     │ ...    │  ...    │  ...    │... │  ...      │
Row 2543│ 0.892  │  0.234  │ -0.556  │... │  0.678    │ ← "cat"
...     │ ...    │  ...    │  ...    │... │  ...      │
Row 32000│0.345  │ -0.678  │  0.912  │... │  0.456    │
        └────────┴─────────┴─────────┴────┴───────────┘

This is a MATRIX: 32,000 rows × 4,096 columns</code></pre>

                    <h3>VIEW 2: One Row (Vector View)</h3>

                    <pre><code>SINGLE TOKEN "cat" (Row 2543):

Position:  0      1      2      3      4    ...  4095
         ┌──────┬──────┬──────┬──────┬──────┬───┬──────┐
Value:   │ 0.892│ 0.234│-0.556│ 0.123│-0.789│...│ 0.678│
         └──────┴──────┴──────┴──────┴──────┴───┴──────┘
         
This is a VECTOR: 4,096 dimensions (4,096 numbers)</code></pre>

                    <h3>VIEW 3: One Column (All Tokens, One Dimension)</h3>

                    <pre><code>DIMENSION 0 ACROSS ALL TOKENS (Column 0):

Token ID    Dimension 0 Value
   0            0.123
   1            0.234
   2            0.567
  ...           ...
  2543          0.892  ← "cat" dimension 0
  ...           ...
  32000         0.345

This is a COLUMN: 32,000 values (one for each token)</code></pre>

                    <h2>🎯 The Key Distinction</h2>

                    <div class="info-box">
                    <strong>UNDERSTANDING THE CONFUSION:</strong><br><br>

                    <strong>MATRIX</strong> = The whole embedding table<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;32,000 rows × 4,096 columns<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;(all tokens, all dimensions)<br><br>

                    <strong>VECTOR</strong> = One row of the matrix<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;1 token × 4,096 dimensions<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;(one token, all dimensions)<br><br>

                    <strong>DIMENSION</strong> = One column position<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Dimension 0, Dimension 1, etc.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;(one number per token)
                    </div>

                    <h2>📦 Real Example - "cat"</h2>

                    <pre><code>STEP 1: Token "cat" has ID 2543

STEP 2: Look up Row 2543 in the embedding table

    EMBEDDING TABLE (Matrix):
    ┌────────────────────────────────────┐
    │ Row 0:    [...]                    │
    │ Row 1:    [...]                    │
    │ ...                                │
    │ Row 2543: [0.892, 0.234, -0.556, ...]│ ← Get this row!
    │ ...                                │
    │ Row 32000: [...]                   │
    └────────────────────────────────────┘

STEP 3: We get ONE vector (one row):
    cat_vector = [0.892, 0.234, -0.556, ..., 0.678]
                  ↑                             ↑
                  4096 numbers in this vector
                  = 4096 dimensions

STEP 4: This vector goes to the transformer layers</code></pre>

                    <h2>🧮 Counting Example</h2>

                    <pre><code>EMBEDDING TABLE:

Total numbers in table: 32,000 × 4,096 = 131,072,000 numbers
                        ↑        ↑
                      tokens   dimensions

When we look up "cat":
- We access Row 2543
- We get 4,096 numbers
- This is ONE vector with 4,096 dimensions

When we look up "dog":  
- We access Row 4821 (different row!)
- We get 4,096 numbers
- This is a DIFFERENT vector with 4,096 dimensions</code></pre>

                    <h2>📝 Simple Analogy: Spreadsheet</h2>

                    <div class="success-box">
                    <strong>Think of it like an Excel spreadsheet:</strong>

                    <pre><code>EXCEL SPREADSHEET:
Rows = Different people
Columns = Different measurements

        Height  Weight  Age  ShoeSize ... (100 columns)
John    180     75      30   42       ...
Mary    165     60      25   38       ...
Bob     175     80      35   43       ...
...
(1000 rows)

John's data = ONE ROW = [180, 75, 30, 42, ...] (100 numbers)

EMBEDDING TABLE:
Rows = Different tokens (32,000 tokens)
Columns = Different dimensions (4,096 dimensions)

        dim0    dim1    dim2   dim3   ... (4096 columns)
"The"   0.234   0.891  -0.445  0.567  ...
"cat"   0.892   0.234  -0.556  0.123  ...
"dog"   0.876   0.221  -0.543  0.115  ...
...
(32,000 rows)

"cat" data = ONE ROW = [0.892, 0.234, -0.556, ...] (4096 numbers)</code></pre>
                    </div>

                    <h2>✅ Check Your Understanding</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Question</th>
                                <th>Answer</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>What is the embedding table?</td>
                                <td>A matrix with 32,000 rows and 4,096 columns</td>
                            </tr>
                            <tr>
                                <td>What is the embedding for "cat"?</td>
                                <td>One row (4,096 numbers)</td>
                            </tr>
                            <tr>
                                <td>If embedding dimension is 4096, how many numbers in ONE token's vector?</td>
                                <td>4,096</td>
                            </tr>
                            <tr>
                                <td>How many vectors are in the embedding table?</td>
                                <td>32,000 (one for each token)</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>🎯 Final Summary</h2>

                    <div class="critical-box">
                    <strong>Remember:</strong><br><br>
                    
                    <strong>1. MATRIX (Embedding Table):</strong><br>
                    &nbsp;&nbsp;&nbsp;• Contains ALL tokens<br>
                    &nbsp;&nbsp;&nbsp;• Size: 32,000 rows × 4,096 columns<br>
                    &nbsp;&nbsp;&nbsp;• Total: 131 million numbers<br><br>

                    <strong>2. VECTOR (One Token):</strong><br>
                    &nbsp;&nbsp;&nbsp;• ONE row from the matrix<br>
                    &nbsp;&nbsp;&nbsp;• Size: 1 × 4,096<br>
                    &nbsp;&nbsp;&nbsp;• Total: 4,096 numbers<br><br>

                    <strong>3. DIMENSION:</strong><br>
                    &nbsp;&nbsp;&nbsp;• One position in the vector<br>
                    &nbsp;&nbsp;&nbsp;• Example: dimension 0, dimension 1, etc.<br>
                    &nbsp;&nbsp;&nbsp;• Each vector has 4,096 dimensions<br><br>

                    <strong>Analogy:</strong><br>
                    Matrix = Entire phonebook<br>
                    Vector = One person's entry<br>
                    Dimension = One field (name, address, phone, etc.)
                    </div>
                `
            },
            {
                title: "15. What is Embedding",
                content: `
                    <h1>Understanding "Embedding"</h1>
                    
                    <div class="user-question">
                    So embedding means assigning vector numbers to the vector table for a word like "cat"?
                    </div>

                    <h2>Yes! But "Embedding" Has 3 Related Meanings</h2>

                    <h3>1. Embedding = The PROCESS</h3>

                    <p><strong>Converting a word into numbers (vector)</strong></p>

                    <pre><code>PROCESS:
Word "cat" → [0.23, -0.45, 0.67, ..., 0.34]
     ↑                    ↓
   text            4096 numbers

This conversion is called "embedding"
We are "embedding" the word into vector space</code></pre>

                    <h3>2. Embedding = The RESULT (the vector itself)</h3>

                    <p><strong>The 4096 numbers ARE called "the embedding"</strong></p>

                    <pre><code>"What's the embedding for 'cat'?"

Answer: [0.23, -0.45, 0.67, ..., 0.34]
        ↑
        This IS the embedding</code></pre>

                    <h3>3. Embedding = The TABLE</h3>

                    <p><strong>The lookup table is called "the embedding layer" or "embeddings"</strong></p>

                    <pre><code>EMBEDDING TABLE (also called "embeddings"):

Word  | Embedding (vector)
------|----------------------------------
"the" | [0.12, -0.34, 0.56, ..., 0.23]
"cat" | [0.23, -0.45, 0.67, ..., 0.34]
"dog" | [0.21, -0.42, 0.65, ..., 0.31]
"mat" | [0.34, -0.67, 0.12, ..., 0.89]

This whole table = "the embeddings"</code></pre>

                    <h2>How They Connect</h2>

                    <pre><code>1. THE TABLE (embeddings):
   Stored in model file
   50,000 words × 4096 numbers each

2. THE PROCESS (embedding):
   Look up "cat" in the table
   Get its vector

3. THE RESULT (embedding):  
   The vector [0.23, -0.45, 0.67, ..., 0.34]

All three are called "embedding"!</code></pre>

                    <h2>Why Called "Embedding"?</h2>

                    <h3>The Word Means: "Placing Inside"</h3>

                    <pre><code>You're taking a word (text)
And PLACING it INSIDE a numerical space

"cat" (text) → embedded into → 4096-dimensional space
                                at position [0.23, -0.45, ...]

Like embedding a photo in a document
You're embedding a word in vector space</code></pre>

                    <h2>Complete Flow</h2>

                    <pre><code>STEP 1: Training creates EMBEDDINGS (the table)
Model learns 4096 numbers for each word
Stores in embedding table

STEP 2: User types "cat"

STEP 3: EMBEDDING process happens
Look up "cat" in embedding table

STEP 4: Get the EMBEDDING (the vector)
Result: [0.23, -0.45, 0.67, ..., 0.34]

STEP 5: Use this embedding
Process it through transformer layers</code></pre>

                    <h2>In Code</h2>

                    <pre><code># The embedding table (stored in model)
embedding_table = {
    "cat": [0.23, -0.45, 0.67, ..., 0.34],  # 4096 numbers
    "dog": [0.21, -0.42, 0.65, ..., 0.31],  # 4096 numbers
    "mat": [0.34, -0.67, 0.12, ..., 0.89],  # 4096 numbers
}

# The embedding process
word = "cat"
embedding = embedding_table[word]  # Look it up

# The embedding result
print(embedding)  
# [0.23, -0.45, 0.67, ..., 0.34]</code></pre>

                    <h2>Common Usage</h2>

                    <h3>"Word embeddings"</h3>
                    <pre><code>Means: Vector representations of words

"GPT uses word embeddings"
= GPT converts words to vectors</code></pre>

                    <h3>"Embedding dimension"</h3>
                    <pre><code>Means: Size of the vector

"This model has 4096 embedding dimensions"
= Each word becomes 4096 numbers</code></pre>

                    <h3>"Embedding layer"</h3>
                    <pre><code>Means: The table/layer that stores vectors

"The embedding layer has 50,000 words"
= The lookup table has 50,000 entries</code></pre>

                    <h2>All Three Meanings Together</h2>

                    <pre><code>"The model uses EMBEDDINGS"
        ↑ (the table)

"to EMBED each word"
   ↑ (the process)

"into an EMBEDDING"
        ↑ (the resulting vector)

Same word, three related meanings!</code></pre>

                    <div class="success-box">
                    <strong>Your understanding is correct!</strong><br><br>
                    Embedding = Assigning vector numbers to words<br><br>
                    More precisely:<br><br>
                    <strong>EMBEDDING TABLE:</strong><br>
                    "cat" → [0.23, -0.45, 0.67, ..., 0.34]<br>
                    ↑ This assignment IS the embedding<br><br>
                    The table stores these assignments<br>
                    The process looks them up<br>
                    The result is the vector<br><br>
                    <strong>Simple answer:</strong> Yes! Embedding = putting words into a vector table, and also the vectors themselves, and also the process of looking them up. All three!
                    </div>
                `
            },
            {
                title: "16. Advanced Concepts",
                content: `
                    <h1>Advanced Concepts: Custom Kernels & Tensor Parallelism</h1>
                    
                    <div class="user-question">
                    What about custom kernels and tensor parallelism you mentioned earlier?
                    </div>

                    <h2>Custom Kernels</h2>

                    <p>A <strong>kernel</strong> is a small, highly optimized piece of code that runs on a GPU/CPU.</p>

                    <h3>Think of it like this:</h3>
                    <ul>
                        <li>Regular code: "Add these million numbers"</li>
                        <li>Custom kernel: "Add these million numbers using GPU's 10,000 cores simultaneously, with optimized memory access patterns"</li>
                    </ul>

                    <h3>Example:</h3>

                    <pre><code># Slow (regular Python)
result = 0
for i in range(1_000_000):
    result += numbers[i]

# Fast (custom CUDA kernel on GPU)
# Adds all numbers in parallel across thousands of GPU cores
result = cuda_kernel_sum(numbers)</code></pre>

                    <h3>In LLM inference:</h3>
                    <ul>
                        <li>Matrix multiplications (core of neural networks)</li>
                        <li>Attention mechanisms</li>
                        <li>Activation functions</li>
                    </ul>

                    <h3>Enterprise optimization:</h3>
                    <pre><code>Standard inference: Uses general PyTorch/TensorFlow operations
Custom kernels: Hand-written CUDA code for 2-5x speedup
                Optimized specifically for your hardware
                Used by companies like OpenAI, Anthropic</code></pre>

                    <h2>Tensor Parallelism</h2>

                    <p><strong>Problem:</strong> Modern LLMs are HUGE and don't fit on a single GPU.</p>

                    <p><strong>Tensor Parallelism</strong> = Splitting a single model across multiple GPUs</p>

                    <h3>How it works:</h3>

                    <p>Imagine you have a matrix multiplication (core operation in LLMs):</p>

                    <pre><code>Regular (1 GPU):
GPU 1: Computes entire operation
Memory needed: 140GB ❌ Won't fit on 80GB GPU!

Tensor Parallelism (2 GPUs):
GPU 1: Computes first half of matrix
GPU 2: Computes second half of matrix
Combine results → Final answer
Memory needed per GPU: 70GB ✓ Fits!</code></pre>

                    <h3>Visual Example:</h3>

                    <pre><code>Single Large Matrix Operation:
[Matrix A] × [Matrix B] = [Result]
   ↓
Split across GPUs:

GPU 1: [Matrix A_top]    × [Matrix B] = [Result_top]
GPU 2: [Matrix A_bottom] × [Matrix B] = [Result_bottom]

Combine: [Result_top + Result_bottom] = Final Result</code></pre>

                    <h3>Types of Parallelism:</h3>

                    <ol>
                        <li><strong>Tensor Parallelism</strong> - Split model layers across GPUs (same model, same time)</li>
                        <li><strong>Pipeline Parallelism</strong> - Different layers on different GPUs (sequential)</li>
                        <li><strong>Data Parallelism</strong> - Multiple copies of model, different data (for training)</li>
                    </ol>

                    <h3>Enterprise Example:</h3>
                    <pre><code>Running Llama 3 405B model:
├── Requires 8x H100 GPUs (80GB each)
├── Model split using tensor parallelism
├── Each GPU holds ~50GB of model weights
├── GPUs communicate via NVLink (fast interconnect)
└── User sees single response, doesn't know it's split</code></pre>

                    <h2>Comparison Table</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>Technique</th>
                                <th>What It Does</th>
                                <th>Use Case</th>
                                <th>Example</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Quantization (GGUF)</strong></td>
                                <td>Reduces number precision</td>
                                <td>Run big models on small hardware</td>
                                <td>70B model on laptop</td>
                            </tr>
                            <tr>
                                <td><strong>Custom Kernels</strong></td>
                                <td>Optimized GPU code</td>
                                <td>Maximum speed</td>
                                <td>OpenAI's inference</td>
                            </tr>
                            <tr>
                                <td><strong>Tensor Parallelism</strong></td>
                                <td>Split model across GPUs</td>
                                <td>Models too big for 1 GPU</td>
                                <td>GPT-4, Claude-sized models</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="info-box">
                    <strong>Why Enterprises Don't Use GGUF:</strong><br>
                    1. Quantization loss - Enterprises often want full precision for best quality<br>
                    2. Optimization ceiling - GGUF is optimized for consumer hardware, not data centers<br>
                    3. Throughput - Can't handle thousands of requests/second efficiently<br>
                    4. Features - Lacks enterprise needs (batching, model sharding, monitoring)
                    </div>
                `
            },
            {
                title: "17. AI Terms - Organized by Category",
                content: `
                    <h1>🎯 AI Terms - Complete Organization</h1>
                    
                    <div class="info-box">
                    <strong>Let's organize everything you've learned into clear categories!</strong><br>
                    This will help you understand how all the terms relate to each other.
                    </div>

                    <h2>📦 CATEGORY 1: DATA STRUCTURES</h2>
                    <p><em>(How data is organized/arranged)</em></p>

                    <div class="success-box">
                    <strong>DATA STRUCTURES = Containers that hold data</strong>

                    <h4>VECTOR (1D - one dimension structure)</h4>
                    <ul>
                        <li>A list/array of numbers in a row</li>
                        <li>Example: <code>[0.892, 0.234, -0.556, 0.678]</code></li>
                        <li>Like: A single row in Excel</li>
                    </ul>

                    <h4>MATRIX / TABLE (2D - two dimension structure)</h4>
                    <ul>
                        <li>A grid of numbers (rows and columns)</li>
                        <li>Like: An Excel spreadsheet</li>
                    </ul>
                    <pre><code>Example:
[0.892  0.234  -0.556]
[0.123  0.456   0.789]
[0.234  0.567  -0.123]</code></pre>

                    <h4>EMBEDDING TABLE (Specific type of Matrix)</h4>
                    <ul>
                        <li>Special matrix that stores all token vectors</li>
                        <li>Size: 32,000 rows × 4,096 columns</li>
                        <li>Like: Big spreadsheet with all word meanings</li>
                    </ul>
                    </div>

                    <div class="critical-box">
                    <strong>RELATIONSHIP:</strong><br>
                    Matrix contains → Multiple Vectors (as rows)<br>
                    Vector contains → Multiple Numbers (weights)
                    </div>

                    <h2>💾 CATEGORY 2: DATA REPRESENTATION</h2>
                    <p><em>(How data is stored in computer memory/files)</em></p>

                    <div class="warning-box">
                    <strong>DATA REPRESENTATION = Format of storage</strong>

                    <table style="margin-top: 15px;">
                        <thead>
                            <tr>
                                <th>Format</th>
                                <th>Precision</th>
                                <th>Size</th>
                                <th>Example</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>FP32</strong></td>
                                <td>High</td>
                                <td>4 bytes/number</td>
                                <td>0.892371649</td>
                            </tr>
                            <tr>
                                <td><strong>FP16</strong></td>
                                <td>Medium</td>
                                <td>2 bytes/number</td>
                                <td>0.8924</td>
                            </tr>
                            <tr>
                                <td><strong>INT8</strong></td>
                                <td>Lower</td>
                                <td>1 byte/number</td>
                                <td>223 (≈0.89)</td>
                            </tr>
                            <tr>
                                <td><strong>INT4</strong></td>
                                <td>Lowest</td>
                                <td>0.5 bytes/number</td>
                                <td>14 (≈0.89)</td>
                            </tr>
                        </tbody>
                    </table>

                    <p style="margin-top: 10px;"><strong>These are STORAGE FORMATS</strong> - same number, different precision</p>
                    </div>

                    <h2>🎯 CATEGORY 3: DATA CONTENT</h2>
                    <p><em>(What the data actually is/represents)</em></p>

                    <div class="info-box">
                    <strong>DATA CONTENT = The actual data and its meaning</strong>

                    <h4>TOKEN</h4>
                    <ul>
                        <li>A piece of text (word/subword)</li>
                        <li>Represented as: Integer ID number</li>
                        <li>Example: "cat" → 2543</li>
                    </ul>

                    <h4>WEIGHT = PARAMETER (same thing!)</h4>
                    <ul>
                        <li>One single number in the model</li>
                        <li>Example: 0.892</li>
                        <li>These are the learned values</li>
                    </ul>

                    <h4>EMBEDDING (for one token)</h4>
                    <ul>
                        <li>The meaning of a token as numbers</li>
                        <li>One vector from the embedding table</li>
                        <li>Example: "cat" → [0.892, 0.234, -0.556, ...]</li>
                    </ul>
                    </div>

                    <h2>📏 CATEGORY 4: PROPERTIES/MEASUREMENTS</h2>
                    <p><em>(Attributes that describe the data)</em></p>

                    <div class="success-box">
                    <strong>PROPERTIES = Characteristics/measurements</strong>

                    <h4>DIMENSION</h4>
                    <ul>
                        <li>Size of a vector (how many numbers)</li>
                        <li>Position in a vector</li>
                        <li>Example: "4096 dimensions" means 4,096 numbers</li>
                    </ul>

                    <p><strong>Also means:</strong> dimension 0, dimension 1, etc. (the index/position of each number)</p>
                    </div>

                    <h2>🗂️ SIMPLE VISUAL ORGANIZATION</h2>

                    <pre><code>┌──────────────────────────────────────────────────────────┐
│                    COMPLETE PICTURE                      │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  DATA CONTENT: Token "cat" (ID: 2543)                   │
│       ↓                                                  │
│  DATA STRUCTURE: Vector [0.892, 0.234, -0.556, ...]    │
│       ↓                                                  │
│  PROPERTY: Has 4,096 dimensions                         │
│       ↓                                                  │
│  DATA REPRESENTATION: Stored as FP16 (2 bytes each)     │
│       ↓                                                  │
│  Each number is a: WEIGHT/PARAMETER                     │
│                                                          │
│  ───────────────────────────────────────────────────    │
│                                                          │
│  All token vectors stored in:                           │
│  DATA STRUCTURE: Embedding Table (Matrix)               │
│       32,000 rows × 4,096 columns                       │
│                                                          │
└──────────────────────────────────────────────────────────┘</code></pre>

                    <h2>📋 QUICK REFERENCE TABLE</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>TERM</th>
                                <th>CATEGORY</th>
                                <th>SIMPLE MEANING</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Vector</strong></td>
                                <td>Data Structure</td>
                                <td>List of numbers</td>
                            </tr>
                            <tr>
                                <td><strong>Matrix</strong></td>
                                <td>Data Structure</td>
                                <td>Grid of numbers</td>
                            </tr>
                            <tr>
                                <td><strong>Table</strong></td>
                                <td>Data Structure</td>
                                <td>Same as matrix</td>
                            </tr>
                            <tr>
                                <td><strong>Embedding Table</strong></td>
                                <td>Data Structure</td>
                                <td>Matrix for tokens</td>
                            </tr>
                            <tr>
                                <td colspan="3" style="background: #f0f0f0;"></td>
                            </tr>
                            <tr>
                                <td><strong>FP32/FP16</strong></td>
                                <td>Data Representation</td>
                                <td>How numbers stored</td>
                            </tr>
                            <tr>
                                <td><strong>INT8/INT4</strong></td>
                                <td>Data Representation</td>
                                <td>in memory</td>
                            </tr>
                            <tr>
                                <td colspan="3" style="background: #f0f0f0;"></td>
                            </tr>
                            <tr>
                                <td><strong>Token</strong></td>
                                <td>Data Content</td>
                                <td>Text as ID number</td>
                            </tr>
                            <tr>
                                <td><strong>Weight</strong></td>
                                <td>Data Content</td>
                                <td>One learned number</td>
                            </tr>
                            <tr>
                                <td><strong>Parameter</strong></td>
                                <td>Data Content</td>
                                <td>Same as weight</td>
                            </tr>
                            <tr>
                                <td><strong>Embedding (singular)</strong></td>
                                <td>Data Content</td>
                                <td>Token's meaning as vector</td>
                            </tr>
                            <tr>
                                <td colspan="3" style="background: #f0f0f0;"></td>
                            </tr>
                            <tr>
                                <td><strong>Dimension</strong></td>
                                <td>Property</td>
                                <td>Size of vector or position</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>🎯 THE HIERARCHY (How They Nest)</h2>

                    <pre><code>MODEL
 │
 ├─ Contains: EMBEDDING TABLE (Data Structure: Matrix)
 │   │
 │   ├─ Has: 32,000 rows × 4,096 columns (Property: Dimensions)
 │   │
 │   ├─ Stored as: FP16 (Data Representation)
 │   │
 │   ├─ Each row is: VECTOR (Data Structure)
 │   │   │
 │   │   ├─ Represents: One TOKEN's EMBEDDING (Data Content)
 │   │   │
 │   │   └─ Contains: 4,096 WEIGHTS/PARAMETERS (Data Content)
 │   │
 │   └─ Total: 131 million PARAMETERS (Data Content)
 │
 └─ Contains: TRANSFORMER LAYERS (more matrices...)</code></pre>

                    <h2>✅ KEY ORGANIZATION</h2>

                    <div class="critical-box">
                    <strong>The Four Categories:</strong><br><br>

                    <strong>1. Data Structure</strong> = Shape/container<br>
                    &nbsp;&nbsp;&nbsp;(Vector, Matrix, Table)<br><br>

                    <strong>2. Data Representation</strong> = Format/encoding<br>
                    &nbsp;&nbsp;&nbsp;(FP32, FP16, INT4)<br><br>

                    <strong>3. Data Content</strong> = Actual values<br>
                    &nbsp;&nbsp;&nbsp;(Token, Weight, Embedding)<br><br>

                    <strong>4. Properties</strong> = Measurements<br>
                    &nbsp;&nbsp;&nbsp;(Dimension)
                    </div>

                    <h2>🎓 Complete Example Using All Categories</h2>

                    <div class="success-box">
                    <strong>Let's describe "cat" using all 4 categories:</strong>

                    <pre><code>"cat" complete description:

DATA CONTENT:
├─ Token: "cat" (the word itself)
├─ Token ID: 2543 (its identifier)
└─ Embedding: The meaning representation

DATA STRUCTURE:
├─ Stored as: Vector (one row from matrix)
└─ Part of: Embedding Table (matrix with all tokens)

PROPERTY:
├─ Dimensions: 4,096 (number of values)
├─ Position in table: Row 2543
└─ Each dimension: 0, 1, 2, ..., 4095

DATA REPRESENTATION:
├─ Original format: FP16 (2 bytes per number)
├─ Quantized format: INT4 (0.5 bytes per number)
└─ Storage size: 4,096 × 2 bytes = 8,192 bytes (FP16)

The actual numbers (WEIGHTS/PARAMETERS):
[0.892, 0.234, -0.556, 0.123, ..., 0.678]
 ↑      ↑      ↑      ↑          ↑
 Each is a weight (learned value)
 Each is stored in FP16 or INT4
 Each occupies one dimension (position)
 All together form one vector (structure)
 Represent one token's embedding (content)</code></pre>
                    </div>

                    <h2>🎯 Final Summary</h2>

                    <p><strong>When you hear these terms, think:</strong></p>

                    <ul>
                        <li><strong>"Vector"</strong> → Category: Structure → Think: "List of numbers"</li>
                        <li><strong>"FP16"</strong> → Category: Representation → Think: "Storage format"</li>
                        <li><strong>"Weight"</strong> → Category: Content → Think: "Actual learned value"</li>
                        <li><strong>"Dimension"</strong> → Category: Property → Think: "Size or position"</li>
                    </ul>

                    <div class="info-box">
                    <strong>💡 Pro Tip:</strong> When confused about a term, ask yourself:<br>
                    "Is this about STRUCTURE (shape), REPRESENTATION (format), CONTENT (what it is), or PROPERTY (measurement)?"
                    </div>
                `
            },
            {
                title: "18. Summary & Key Takeaways",
                content: `
                    <h1>Complete Learning Summary</h1>

                    <h2>The Complete Journey</h2>

                    <p>Congratulations! You've learned the fundamentals of how Large Language Models work, from running them locally to understanding their internal mechanics.</p>

                    <h2>Key Concepts Recap</h2>

                    <h3>1. Running LLMs Locally</h3>
                    <ul>
                        <li><strong>GGUF files</strong> - Quantized model format for local use</li>
                        <li><strong>Tools</strong> - LM Studio, Ollama, llama.cpp</li>
                        <li><strong>Inference engines</strong> - Software that runs the models</li>
                    </ul>

                    <h3>2. Numbers & Precision</h3>
                    <ul>
                        <li><strong>Floating Point (FP16, FP32)</strong> - How computers store decimal numbers</li>
                        <li><strong>Quantization</strong> - Making numbers smaller (16-bit → 4-bit)</li>
                        <li><strong>Trade-off</strong> - Memory savings vs. quality</li>
                    </ul>

                    <h3>3. Text to Numbers</h3>
                    <ul>
                        <li><strong>Tokens</strong> - Text broken into chunks with IDs</li>
                        <li><strong>Vectors</strong> - Lists of numbers (not arrows!)</li>
                        <li><strong>Dimensionality</strong> - How many numbers in the vector</li>
                    </ul>

                    <h3>4. The Magic: Embeddings</h3>
                    <ul>
                        <li><strong>Embedding Table</strong> - Lookup table: word → 4096 numbers</li>
                        <li><strong>Each number</strong> - Represents a learned characteristic</li>
                        <li><strong>Similar words</strong> - Have similar numbers</li>
                    </ul>

                    <h3>5. How It Learns</h3>
                    <ul>
                        <li><strong>Training</strong> - Needs sentences, not single words</li>
                        <li><strong>Context</strong> - Learns from what appears together</li>
                        <li><strong>Features</strong> - Emerge automatically, not pre-defined</li>
                    </ul>

                    <h2>The Big Picture</h2>

                    <pre><code>YOUR INPUT: "cat"
    ↓
1. TOKENIZE: "cat" → ID 2459
    ↓
2. EMBEDDING LOOKUP: ID 2459 → [0.23, -0.45, 0.67, ..., 0.34]
    ↓
3. TRANSFORMER LAYERS (80 layers):
   Each layer transforms the numbers using learned weights
    ↓
4. OUTPUT: Predictions for next word
    ↓
RESPONSE: "cat is a small mammal..."</code></pre>

                    <h2>Important Insights</h2>

                    <div class="success-box">
                    <strong>1. What are weights/parameters?</strong><br>
                    The billions of numbers that make up the model. For "cat", it's 4096 numbers. For the whole model, it's 70 billion numbers.
                    </div>

                    <div class="success-box">
                    <strong>2. What gets quantized?</strong><br>
                    Those billions of weights. We make them less precise (0.31415926 → 0.3) to save memory.
                    </div>

                    <div class="success-box">
                    <strong>3. What's a vector?</strong><br>
                    Just a list of numbers. In AI, it's a position in high-dimensional "meaning space", not a physical arrow.
                    </div>

                    <div class="success-box">
                    <strong>4. What do the 4096 numbers mean?</strong><br>
                    Each number represents some learned characteristic (animal? size? fur? etc.). The model figured these out on its own!
                    </div>

                    <div class="success-box">
                    <strong>5. Why sentences for training?</strong><br>
                    The model learns word meanings from context. "cat" alone teaches nothing - but "The cat sat on the mat" teaches relationships.
                    </div>

                    <h2>Practical Takeaways</h2>

                    <table>
                        <thead>
                            <tr>
                                <th>For Personal Use</th>
                                <th>For Enterprise</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Use GGUF (quantized models)</td>
                                <td>Use full-precision models</td>
                            </tr>
                            <tr>
                                <td>Q4_K_M or Q5_K_M sweet spot</td>
                                <td>FP16 or custom optimizations</td>
                            </tr>
                            <tr>
                                <td>LM Studio or Ollama</td>
                                <td>vLLM, TensorRT-LLM</td>
                            </tr>
                            <tr>
                                <td>Single machine</td>
                                <td>GPU clusters with parallelism</td>
                            </tr>
                            <tr>
                                <td>4-35 GB models</td>
                                <td>70-400 GB models</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>What You Can Do Now</h2>

                    <ol>
                        <li><strong>Download and run</strong> a GGUF model using LM Studio</li>
                        <li><strong>Understand</strong> what Q4_K_M means (4-bit quantization, K method, medium quality)</li>
                        <li><strong>Explain</strong> how "cat" becomes 4096 numbers</li>
                        <li><strong>Know</strong> why models need billions of parameters</li>
                        <li><strong>Appreciate</strong> the training process and why it costs millions</li>
                    </ol>

                    <h2>Going Deeper</h2>

                    <p>If you want to learn more:</p>
                    <ul>
                        <li><strong>Transformers</strong> - The "Attention is All You Need" paper</li>
                        <li><strong>Fine-tuning</strong> - Customizing models for specific tasks</li>
                        <li><strong>Prompt Engineering</strong> - Getting better responses</li>
                        <li><strong>RAG (Retrieval Augmented Generation)</strong> - Adding external knowledge</li>
                        <li><strong>Model Architecture</strong> - Deep dive into layers, attention, etc.</li>
                    </ul>

                    <h2>Final Thought</h2>

                    <div class="info-box" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; padding: 30px; font-size: 1.1em;">
                    <strong>The Beauty of LLMs:</strong><br><br>
                    We design the structure (architecture, dimensions, layers)<br>
                    But the MODEL discovers the meaning (what each number represents)<br><br>
                    70 billion numbers, each learned from seeing billions of sentences,<br>
                    working together to understand and generate human language.<br><br>
                    <strong>That's the magic! 🎉</strong>
                    </div>

                    <h2>Thank You!</h2>

                    <p>You've completed this comprehensive guide to understanding LLMs. You now know more than most people about how these incredible systems work!</p>

                    <p style="text-align: center; margin-top: 40px; font-size: 1.2em; color: #3498db;">
                    <strong>Happy Learning! 🚀</strong>
                    </p>
                `
            }
        ];


        // Quiz data for each section
        const sectionQuizzes = [
            { // Section 0: Running LLMs Locally
                questions: [
                    {
                        question: "What file format is commonly used for running LLMs locally?",
                        options: ["PDF", "GGUF", "MP4", "DOC"],
                        correct: 1,
                        explanation: "GGUF (GPT-Generated Unified Format) is the standard format for quantized models used in local LLM deployment."
                    },
                    {
                        question: "Which tool is recommended for beginners to run LLMs with a GUI?",
                        options: ["Command Prompt", "LM Studio", "Notepad", "PowerPoint"],
                        correct: 1,
                        explanation: "LM Studio provides an easy-to-use graphical interface perfect for beginners."
                    },
                    {
                        question: "What is llama.cpp primarily used for?",
                        options: ["Image editing", "Running LLMs with maximum control", "Creating documents", "Playing games"],
                        correct: 1,
                        explanation: "llama.cpp is a C++ library that provides maximum control for running LLMs."
                    }
                ]
            },
            { // Section 1: LLM Tools
                questions: [
                    {
                        question: "What are tools like LM Studio and Ollama called?",
                        options: ["Video players", "LLM inference engines", "Text editors", "Web browsers"],
                        correct: 1,
                        explanation: "These are LLM inference engines - software that runs language models locally."
                    },
                    {
                        question: "Which inference engine does Ollama use under the hood?",
                        options: ["PowerPoint", "llama.cpp", "Excel", "Adobe Reader"],
                        correct: 1,
                        explanation: "Ollama is built on top of llama.cpp but provides a simpler interface."
                    }
                ]
            },
            { // Section 2: Enterprise vs Personal
                questions: [
                    {
                        question: "What is the main advantage of quantized models for personal use?",
                        options: ["Better quality", "Smaller file sizes", "Faster internet", "More colors"],
                        correct: 1,
                        explanation: "Quantization reduces file size significantly, making models runnable on consumer hardware."
                    },
                    {
                        question: "Which format do enterprises typically use?",
                        options: ["Q4_K_M (4-bit quantized)", "FP16 or FP32 (full precision)", "MP3", "JPEG"],
                        correct: 1,
                        explanation: "Enterprises use full-precision models (FP16/FP32) for maximum quality."
                    }
                ]
            },
            { // Section 3: Floating Point
                questions: [
                    {
                        question: "What does FP32 stand for?",
                        options: ["Fast Processing 32", "32-bit floating point", "File Protocol 32", "Folder Path 32"],
                        correct: 1,
                        explanation: "FP32 means 32-bit floating point - a way to store decimal numbers."
                    },
                    {
                        question: "How many bytes does an FP16 number use?",
                        options: ["1 byte", "2 bytes", "4 bytes", "8 bytes"],
                        correct: 1,
                        explanation: "FP16 uses 16 bits = 2 bytes per number."
                    }
                ]
            },
            { // Section 4: Quantization
                questions: [
                    {
                        question: "What is the primary purpose of quantization?",
                        options: ["Make models slower", "Reduce memory usage", "Add more features", "Change colors"],
                        correct: 1,
                        explanation: "Quantization reduces precision to save memory, making models smaller and faster."
                    },
                    {
                        question: "What does Q4 in Q4_K_M represent?",
                        options: ["4 layers", "4-bit quantization", "4 gigabytes", "4 questions"],
                        correct: 1,
                        explanation: "Q4 means the model uses 4-bit quantization for its weights."
                    }
                ]
            },
            { // Section 5: What Gets Quantized
                questions: [
                    {
                        question: "What parts of the model get quantized?",
                        options: ["The file name", "The weights/parameters", "The user interface", "The monitor"],
                        correct: 1,
                        explanation: "Quantization affects the billions of weight parameters that make up the model."
                    },
                    {
                        question: "In a 70B parameter model, what does the 70B represent?",
                        options: ["70 bytes", "70 billion weights", "70 bits", "70 badges"],
                        correct: 1,
                        explanation: "70B means 70 billion parameters (weights) in the model."
                    }
                ]
            },
            { // Section 6: Tokens
                questions: [
                    {
                        question: "What is a token in the context of LLMs?",
                        options: ["A password", "A piece of text that the model processes", "A coin", "A badge"],
                        correct: 1,
                        explanation: "Tokens are chunks of text (words, subwords, or characters) that LLMs process."
                    },
                    {
                        question: "What happens during tokenization?",
                        options: ["Text is deleted", "Text is broken into tokens and assigned IDs", "Text is printed", "Text is colored"],
                        correct: 1,
                        explanation: "Tokenization converts text into tokens, each with a unique numeric ID."
                    }
                ]
            },
            { // Section 7: Vectors
                questions: [
                    {
                        question: "In AI/ML, what is a vector?",
                        options: ["An arrow", "A list of numbers", "A type of virus", "A direction"],
                        correct: 1,
                        explanation: "In AI, a vector is simply a list of numbers, not a physical arrow."
                    },
                    {
                        question: "What does a 4096-dimensional vector have?",
                        options: ["4096 arrows", "4096 numbers in the list", "4096 colors", "4096 words"],
                        correct: 1,
                        explanation: "A 4096-dimensional vector contains 4096 numbers."
                    }
                ]
            },
            { // Section 8: Tokens to Vectors
                questions: [
                    {
                        question: "How does a token ID become a vector?",
                        options: ["It stays as a number", "It looks up its row in the embedding table", "It gets deleted", "It prints out"],
                        correct: 1,
                        explanation: "The token ID is used to look up the corresponding vector in the embedding table."
                    },
                    {
                        question: "What is an embedding table?",
                        options: ["A dining table", "A matrix mapping token IDs to vectors", "A database", "A chart"],
                        correct: 1,
                        explanation: "The embedding table is a matrix where each row is a vector for a specific token."
                    }
                ]
            },
            { // Section 9: Transformer Layers
                questions: [
                    {
                        question: "What do transformer layers do to embeddings?",
                        options: ["Delete them", "Transform and refine them through neural network layers", "Print them", "Color them"],
                        correct: 1,
                        explanation: "Transformer layers progressively transform embeddings to understand context and relationships."
                    },
                    {
                        question: "How many layers does Llama 2 70B have?",
                        options: ["10 layers", "40 layers", "80 layers", "120 layers"],
                        correct: 2,
                        explanation: "Llama 2 70B has 80 transformer layers."
                    }
                ]
            },
            { // Section 10: How Vectors Are Learned
                questions: [
                    {
                        question: "How are embedding values learned?",
                        options: ["Manually typed in", "Learned from training data", "Generated randomly", "Copied from internet"],
                        correct: 1,
                        explanation: "Embedding values are learned during training by seeing billions of examples."
                    }
                ]
            },
            { // Section 11: Why Sentences Matter
                questions: [
                    {
                        question: "Why can\'t we train on individual words alone?",
                        options: ["Too boring", "Words need context to learn meaning", "Not enough memory", "Too fast"],
                        correct: 1,
                        explanation: "Words get their meaning from context - how they\'re used with other words in sentences."
                    }
                ]
            },
            { // Section 12: What 4096 Numbers Represent
                questions: [
                    {
                        question: "What do the 4096 numbers in an embedding represent?",
                        options: ["Random values", "Learned characteristics and features", "Phone numbers", "Dates"],
                        correct: 1,
                        explanation: "Each number represents a learned characteristic or feature about the word."
                    }
                ]
            },
            { // Section 13: How Features Emerge
                questions: [
                    {
                        question: "How are features in embeddings defined?",
                        options: ["Programmers manually define them", "They emerge automatically during training", "Users select them", "They are random"],
                        correct: 1,
                        explanation: "Features emerge automatically - the model discovers them during training."
                    }
                ]
            },
            { // Section 14: Dimension vs Matrix
                questions: [
                    {
                        question: "What is dimensionality?",
                        options: ["The size of the file", "The number of values in a vector", "The number of words", "The file format"],
                        correct: 1,
                        explanation: "Dimensionality is the count of numbers in a vector (e.g., 4096-dimensional = 4096 numbers)."
                    }
                ]
            },
            { // Section 15: What is Embedding
                questions: [
                    {
                        question: "What is an embedding?",
                        options: ["A type of file", "A numerical representation of text in vector form", "A programming language", "A database"],
                        correct: 1,
                        explanation: "An embedding is the numerical representation of text as a vector of numbers."
                    }
                ]
            },
            { // Section 16: Advanced Concepts
                questions: [
                    {
                        question: "What does attention mechanism in transformers do?",
                        options: ["Makes the model focus", "Determines which tokens to focus on when processing context", "Deletes data", "Adds colors"],
                        correct: 1,
                        explanation: "Attention allows the model to focus on relevant parts of the input when processing."
                    }
                ]
            },
            { // Section 17: AI Terms
                questions: [
                    {
                        question: "Which category does \'Vector\' belong to?",
                        options: ["Data Representation", "Data Structure", "Data Content", "Property"],
                        correct: 1,
                        explanation: "Vector is a data structure - it\'s the shape/organization of the data (list of numbers)."
                    }
                ]
            },
            { // Section 18: Summary
                questions: [
                    {
                        question: "What is the complete flow from text to LLM output?",
                        options: [
                            "Text → Print → Done",
                            "Text → Token → Embedding → Transformer Layers → Output",
                            "Text → Save → Exit",
                            "Text → Delete → Restart"
                        ],
                        correct: 1,
                        explanation: "The flow is: Text → Tokenization → Embedding Lookup → Transformer Processing → Output Generation."
                    },
                    {
                        question: "What\'s the recommended quantization level for personal use?",
                        options: ["Q2", "Q4_K_M or Q5_K_M", "FP32", "Q8"],
                        correct: 1,
                        explanation: "Q4_K_M or Q5_K_M offers the best balance of quality and size for personal use."
                    }
                ]
            }
        ];

        // Track quiz scores
        let totalQuizScore = 0;
        let totalQuizQuestions = 0;


        // Stage mapping - Restructured learning path
        function getSectionStage(index) {
            if (index === 6) return { name: 'Text/Words', class: 'text' };
            if (index === 6) return { name: 'Tokenization', class: 'tokenization' };
            if (index >= 7 && index <= 8) return { name: 'Vectors', class: 'vectors' };
            if (index >= 10 && index <= 12 || index === 14) return { name: 'Embeddings', class: 'embeddings' };
            if (index === 13) return { name: 'Dimensions', class: 'dimensions' };
            if (index === 9) return { name: 'Transformer', class: 'transformer' };
            if (index === 15) return { name: 'Complete LLM', class: 'complete' };
            return { name: 'Deployment', class: 'deployment' };
        }

        // Render learning path flow - New order
        function renderLearningPath() {
            const stages = [
                { name: 'Text', class: 'text', icon: '📝' },
                { name: 'Tokenization', class: 'tokenization', icon: '🔤' },
                { name: 'Vectors', class: 'vectors', icon: '📊' },
                { name: 'Embeddings', class: 'embeddings', icon: '🔮' },
                { name: 'Dimensions', class: 'dimensions', icon: '📐' },
                { name: 'Transformer', class: 'transformer', icon: '⚙️' },
                { name: 'Complete LLM', class: 'complete', icon: '🤖' },
                { name: 'Deployment', class: 'deployment', icon: '🚀' }
            ];

            const currentStage = getSectionStage(currentSection);

            let html = '<h3>Your Learning Journey</h3><div class="flow-stages">';
            stages.forEach((stage, index) => {
                if (index > 0) {
                    html += '<span class="flow-arrow">→</span>';
                }
                const isActive = stage.class === currentStage.class ? 'active' : '';
                html += `<div class="flow-stage ${stage.class} ${isActive}">${stage.name}</div>`;
            });
            html += '</div>';

            document.getElementById('learningPathFlow').innerHTML = html;
        }

        // Render breadcrumb
        function renderBreadcrumb() {
            const stage = getSectionStage(currentSection);
            const sectionTitle = sections[currentSection].title;

            const html = `
                <a href="#" onclick="showSection(0); return false;">Home</a>
                <span class="breadcrumb-separator">›</span>
                <span style="font-weight: 600;">${stage.name}</span>
                <span class="breadcrumb-separator">›</span>
                <span>${sectionTitle}</span>
            `;

            document.getElementById('breadcrumb').innerHTML = html;
        }

        // Generate quiz HTML
        function generateQuiz(sectionIndex) {
            const quiz = sectionQuizzes[sectionIndex];
            if (!quiz || !quiz.questions || quiz.questions.length === 0) return '';

            let html = `
                <div class="quiz-section">
                    <h3>✏️ Test Your Knowledge</h3>
            `;

            quiz.questions.forEach((q, qIndex) => {
                html += `
                    <div class="quiz-question" id="quiz-${sectionIndex}-${qIndex}">
                        <h4>Question ${qIndex + 1}: ${q.question}</h4>
                        <div class="quiz-options">
                `;

                q.options.forEach((option, oIndex) => {
                    html += `
                        <div class="quiz-option" onclick="selectQuizOption(${sectionIndex}, ${qIndex}, ${oIndex})">
                            ${option}
                        </div>
                    `;
                });

                html += `
                        </div>
                        <div class="quiz-feedback" id="feedback-${sectionIndex}-${qIndex}"></div>
                    </div>
                `;
            });

            html += `
                    <div class="quiz-score" id="quiz-score-${sectionIndex}">
                        Score: 0/${quiz.questions.length} | Total Progress: <span id="total-score-display">0/0</span>
                    </div>
                </div>
            `;

            return html;
        }

        // Handle quiz option selection
        function selectQuizOption(sectionIndex, questionIndex, selectedOption) {
            const quiz = sectionQuizzes[sectionIndex];
            const question = quiz.questions[questionIndex];
            const questionDiv = document.getElementById(`quiz-${sectionIndex}-${questionIndex}`);
            const feedback = document.getElementById(`feedback-${sectionIndex}-${questionIndex}`);
            const options = questionDiv.querySelectorAll('.quiz-option');

            // Clear previous selections
            options.forEach(opt => {
                opt.classList.remove('selected', 'correct', 'incorrect');
            });

            // Mark selected option
            options[selectedOption].classList.add('selected');

            // Check if correct
            const isCorrect = selectedOption === question.correct;

            if (isCorrect) {
                options[selectedOption].classList.add('correct');
                feedback.className = 'quiz-feedback correct show';
                feedback.innerHTML = '✓ Correct! Great job! ' + question.explanation;

                // Update score (only count once per question)
                if (!questionDiv.dataset.answered) {
                    totalQuizScore++;
                    questionDiv.dataset.answered = 'true';
                }
            } else {
                options[selectedOption].classList.add('incorrect');
                options[question.correct].classList.add('correct');
                feedback.className = 'quiz-feedback incorrect show';
                feedback.innerHTML = '✗ Not quite. ' + question.explanation + ' <a class="quiz-review-link" href="#" onclick="scrollToTop(); return false;">Review the section above</a>';

                questionDiv.dataset.answered = 'true';
            }

            totalQuizQuestions = Math.max(totalQuizQuestions, getSectionQuestionCount(sectionIndex));
            updateQuizScore(sectionIndex);
        }

        // Get question count for a section
        function getSectionQuestionCount(sectionIndex) {
            let count = 0;
            for (let i = 0; i <= sectionIndex; i++) {
                if (sectionQuizzes[i] && sectionQuizzes[i].questions) {
                    count += sectionQuizzes[i].questions.length;
                }
            }
            return count;
        }

        // Update quiz score display
        function updateQuizScore(sectionIndex) {
            const quiz = sectionQuizzes[sectionIndex];
            let sectionScore = 0;

            quiz.questions.forEach((q, qIndex) => {
                const questionDiv = document.getElementById(`quiz-${sectionIndex}-${qIndex}`);
                if (questionDiv && questionDiv.dataset.answered) {
                    const feedback = document.getElementById(`feedback-${sectionIndex}-${qIndex}`);
                    if (feedback.classList.contains('correct')) {
                        sectionScore++;
                    }
                }
            });

            const scoreDiv = document.getElementById(`quiz-score-${sectionIndex}`);
            if (scoreDiv) {
                scoreDiv.innerHTML = `
                    Score: ${sectionScore}/${quiz.questions.length} |
                    Total Progress: <span id="total-score-display">${totalQuizScore}/${totalQuizQuestions}</span>
                `;
            }
        }

        function scrollToTop() {
            document.querySelector('.main-content').scrollTop = 0;
        }

        function renderTOC() {
            const toc = document.getElementById('toc');
            sections.forEach((section, index) => {
                const item = document.createElement('div');
                item.className = 'toc-item';
                if (index === 0) item.classList.add('active');

                // Add stage dot
                const stage = getSectionStage(index);
                const dot = document.createElement('span');
                dot.className = `stage-dot ${stage.class}`;
                item.appendChild(dot);

                // Add text node
                const text = document.createTextNode(section.title);
                item.appendChild(text);

                item.onclick = () => showSection(index);
                toc.appendChild(item);
            });
        }

        function showSection(index) {
            currentSection = index;
            
            const content = document.getElementById('content');
            content.innerHTML = `
                <div class="section active">
                    ${sections[index].content}
                </div>
            `;

            document.querySelectorAll('.toc-item').forEach((item, i) => {
                item.classList.toggle('active', i === index);
            });

            const progress = ((index + 1) / sections.length) * 100;
            document.getElementById('progressBar').style.width = progress + '%';

            document.getElementById('prevBtn').disabled = index === 0;
            document.getElementById('nextBtn').disabled = index === sections.length - 1;

            document.querySelector('.main-content').scrollTop = 0;
        }

        function navigateSection(direction) {
            const newIndex = currentSection + direction;
            if (newIndex >= 0 && newIndex < sections.length) {
                showSection(newIndex);
            }
        }

        renderTOC();
        showSection(0);

        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowLeft') navigateSection(-1);
            if (e.key === 'ArrowRight') navigateSection(1);
        });
    </script>
</body>
</html>